77499681
ABSTRACT : The purpose of this study is to evaluate the effects of teriparatide administration on fracture healing after intramedullary nailing in atypical femoral fractures. Materials and Methods: We retrospectively reviewed 26 patients (26 cases) with atypical femoral fracture who were treated using intramedullary nailing between January 2009 and December 2013. Teriparatide was not administered to 15 patients (non-injection group) and was administered to 11 patients after surgery (injection group). Clinical results were assessed using the Nakajima score and the visual analogue scale (VAS). Radiographic results were compared for the time of callus formation, callus bridge formation, and bone union between the groups. Results: Time to recover walking ability and to decrease pain in the surgery region (VAS≤2) were significantly shorter in the injection group than in the non-injection group. The time of callus formation, callus bridge formation, and bone union was significantly shorter in the injection group than in the non-injection group. There were 5 cases of delayed bone union (33.3%) and 1 case of none union (6.7%) in the non-injection group and all cases obtained bone union in injection group. Conclusion: The injection group showed better clinical and radiographic results than the non-injection group after intramedullary nailing in atypical femoral fracture. Therefore, we think that teriparatide administration after intramedullary nailing could be a useful treatment option to promote bone union.

94551546
ABSTRACT : Ethanolamine (EA) or ethylenediamine (ED)-functionalized poly(glycidyl methacrylate) (PGMA), namely PGEA or PGED, has recently been used as effective gene carriers because of their low cytotoxicity and high transfection efficiency. In this study, a series of PGMA-based supramolecular polycations (PGED-Gd@PGEAs) with magnetic resonance imaging (MRI) functions were readily constructed by assembling multiple adamantine-headed star PGEA (Ad-PGEA) units with a versatile PGED-CD-Gd backbone, which possessed numerous flanking β-cyclodextrin species and Gd 3+ ions. The properties of different PGED-Gd@PGEA vectors were systematically characterized, including the plasmid DNA condensation ability, cytotoxicity, gene transfection efficiency, cellular uptake and MRI function. Such supramolecular gene vectors had lower toxicity than 'gold standard' polyethylenimine (PEI, 25 kDa). Furthermore, PGED-Gd@PGEAs exhibited significantly higher transfection efficiencies than PEI or the constituent units (PGED-CD-Gd and Ad-PGEA). The chelation of Gd 3+ ions imparted the PGED-Gd@PGEA vectors with a good MRI ability without obvious adverse effects. The present design of PGMA-based supramolecular polycations with Gd 3+ chelation would provide useful information for the development of low-toxicity and high-efficiency multifunctional gene delivery systems.
INTRODUCTION : Gene therapy holds potential for treating many severe diseases, such as cancer and genetic diseases. 1 Successful gene therapy depends on highefficiency gene delivery processes, in which the gene carriers have an essential role. The application of traditional viral vectors has been a challenge because of their toxicity, immunogenicity and low capability for scaling up. 2 There has long been a scientific demand for developing non-viral gene delivery systems that can overcome the drawbacks of viral vectors. 3 Non-viral gene delivery has been advanced by the rapid development of materials science and technology. Numerous novel gene delivery systems have been proposed based on functional cationic polymers, such as polyethylenimine (PEI), [4] [5] [6] [7] [8] [9] poly(2-(dimethylamino) ethyl methacrylate), 4,10 poly(L-lysine), 11 poly(aspartic acid) 12, 13 and polyamidoamine. 14 However, these non-viral gene carriers still have shortcomings, including cytotoxicity, low transfection efficiency and lack of multifunction.
INTRODUCTION : Recently, we found that ethanolamine (EA) or ethylenediamine (ED)-functionalized poly(glycidyl methacrylate) (PGMA), namely PGEA or PGED, could be used as effective gene carriers. 15, 16 They possess good gene transfection properties. To further improve the performance of PGMA-based gene carriers, several strategies have been applied such as polysaccharide introduction and target molecule binding. 16, 17 Owing to the dynamically unable ability of supramolecular polymers, the application of supramolecular chemistry for gene delivery has been a hot research topic in the biomedical field. 18, 19 The construction of supramolecular polycations via host-guest interaction is a popular strategy for high-efficiency gene delivery systems. 20 In particular, cyclodextrins (CDs) and their derivatives have been widely utilized for constructing supramolecular gene delivery systems, mainly because of their superior biocompatibility. [21] [22] [23] With the host-guest interaction strategy, we successfully prepared one PGEA-based supramolecular delivery system by tying multiple β-cyclodextrin (β-CD)cored star PGEA polymers to an adamantine-modified linear PGEA backbone. 24 Such PGEA supramolecules markedly increased transfection efficiencies. Further improvements in functionality and the development of new preparation strategies for PGMA-based supramolecular vectors would benefit the construction of better gene delivery systems.
INTRODUCTION : To construct multifunctional supramolecular vectors, a novel strategy was proposed to flexibly prepare PGMA-based supramolecular delivery systems (PGED-Gd@PGEAs) with magnetic resonance imaging (MRI) functionality, by assembling multiple adamantine-headed star PGEA (Ad-PGEA) units with a versatile PGED-CD-Gd backbone (Scheme 1). This backbone possessed numerous flanking β-CD species and Gd 3+ ions. MRI technology has received considerable attention because of its high spatial resolution and its applications in areas such as deep tissue imaging. 25, 26 In particular, Gd 3+ ions have been used as contrast agents because of their optimal chemical and magnetic properties. 27, 28 In this work, Gd ions were chelated by diethylenetriaminepentacetate acid (DTPA) immobilized on the PGED backbones to introduce the MRI effect into the resultant PGED-Gd@PGEA supramolecular systems. The physicochemical properties of the PGED-Gd@PGEA assemblies, including plasmid DNA (pDNA) condensation ability, cytotoxicity, gene transfection, cellular uptake and MRI functionality, were examined in detail. The present work provides a new strategy to design multifunctional supramolecular delivery systems.
CONCLUSIONS : A series of PGMA-based supramolecular polycations (PGED-Gd@PGEAs) with MRI functionality were successfully constructed by assembling multiple Ad-PGEA guests with a versatile PGED-CD-Gd host, which possessed numerous flanking Gd 3+ ions. Such PGED-Gd@PGEAs had good DNA condensation abilities and low cell cytotoxicity. Moreover, PGED-Gd@PGEAs exhibited significantly higher transfection efficiencies than PEI (25 kDa) and the constituent units (PGED-CD-Gd and Ad-PGEA). The chelation of Gd 3+ ions imparted PGED-Gd@PGEAs with effective MRI functionality without adverse effects on gene transfection processes. These unique features could allow PGED-Gd@PGEA to become a competitive multifunctional gene delivery system. Figure 8 Linear fitting of the inverse T 1 and the T 1 -weighted MR images of (a) PGED-Gd@PGEA1 solutions and (b) C6 and Hep G2 cells treated with PGED-Gd@PGEA1 at different Gd concentrations.

159355456
ABSTRACT : The Government of India has presented an expanded support of AYUSH arrangement of medicine and foundation of Indian frameworks of solution strength focuses. Taking this vision forward, the Rashtrapati Bhavan with help of Ministry of AYUSH, Government of India; set up the primary AYUSH Wellness Clinic (AWC) of the nation at President's Estate in July 2015. The AWC has treatment facilities in the surges of Ayurveda, Yoga and Naturopathy, Unani, Siddha and Homeopathy. AWC takes into account the restorative needs of the President, authorities of President's Secretariat and occupants of the President's Estate. This paper exhibits a report on the achievement and the work done at AWC from its origin in July 2015 to January 2018. A sum of 82137 patients was counselled and almost 46443 gotten different treatments endorsed by the doctors amid this day and age. AYUSH mindfulness workshops are being directed for consistently. New activities in the year 2015 incorporate foundation of home grown garden at the AWC premises, assigned touch screen intuitive stand and beginning Skill-improvement and Internship Program for the occupants. New activities in the year 2016 incorporate Village out-achieve program and support in the Festival of Innovation at Rashtrapati Bhavan. As new activity in the year 2017-new therapies were added to the current administrations in Ayurveda wing, Yoga and Naturopathy wing and Unani wing; the therapists and bolster staffs working at AWC were prepared in First-AID and CPR and 22 research papers were distributed in peer evaluated indexed journals.

18980380
ABSTRACT : This technical note studies Markov decision processes under parameter uncertainty. We adapt the distributionally robust optimization framework, assume that the uncertain parameters are random variables following an unknown distribution, and seek the strategy which maximizes the expected performance under the most adversarial distribution. In particular, we generalize a previous study [1] which concentrates on distribution sets with very special structure to a considerably more generic class of distribution sets, and show that the optimal strategy can be obtained efficiently under mild technical conditions. This significantly extends the applicability of distributionally robust MDPs by incorporating probabilistic information of uncertainty in a more flexible way.
ABSTRACT : Index Terms-Distributional robustness, Markov decision processes, parameter uncertainty.
V. CONCLUSION : In this technical note, we considered Markov decision problems with uncertainty. Specifically, we generalized the distributionally robust approach proposed in [1] to incorporate more general ambiguity sets proposed in [18] to model a-priori probabilistic information of the uncertain parameters. We proposed a way to compute the distributionally robust strategy through a Bellman type backward induction. We showed that the strategy, which achieves maximum expected utility under the worst admissible distributions of uncertain parameters, can be solved in polynomial time under some mild technical conditions. We believe that many important problems that are usually addressed using standard MDP models could be revisited and better resolved using the proposed models when parameter uncertainty exists, as this formulation naturally enables the decision maker to account for more general parameter uncertainty.

18980463
ABSTRACT : Although development of the adult Drosophila compound eye is very well understood, little is known about development of photoreceptors (PRs) in the simple larval eye. We show here that the larval eye is composed of 12 PRs, four of which express blue-sensitive rhodopsin5 (rh5) while the other eight contain green-sensitive rh6. This is similar to the 30:70 ratio of adult blue and green R8 cells. However, the stochastic choice of adult color PRs and the bistable loop of the warts and melted tumor suppressor genes that unambiguously specify rh5 and rh6 in R8 PRs are not involved in specification of larval PRs. Instead, primary PR precursors signal via EGFR to surrounding tissue to develop as secondary precursors, which will become Rh6-expressing PRs. EGFR signaling is required for the survival of the Rh6 subtype. Primary precursors give rise to the Rh5 subtype. Furthermore, the combinatorial action of the transcription factors Spalt, Seven-up, and Orthodenticle specifies the two PR subtypes. Therefore, even though the larval PRs and adult R8 PRs express the same rhodopsins (rh5 and rh6), they use very distinct mechanisms for their specification.
ABSTRACT : [Keywords: Drosophila; visual system development; photoreceptor specification; transcription factor interaction; EGFR signaling] Supplemental material is available at http://www.genesdev.org. In spite of the morphological and developmental differences between vertebrate and invertebrate eyes, their basic function to translate light information from the environment to the brain is maintained. In Drosophila, the adult compound eye has been studied in great detail. It consists of ∼800 individual ommatidia. Each ommatidium contains eight photoreceptor cells (PRs): six outer PRs (R1-R6) and two inner PRs (R7 and R8). Different PRs are sensitive to different wavelengths of light, depending on the rhodopsin gene (rh) they express. Outer PRs are involved in motion detection and contain Rh1, a broad-spectrum photopigment. R7 and R8 each expresses a distinct rh with restricted absorption spectra-rh3, rh4, rh5, and rh6. The type of rh expressed in inner PRs defines two major types of ommatidia: The pale (p) ommatidia have R7 that contain UV-sensitive Rh3 with the corresponding R8 expressing blue Rh5, whereas in yellow (y) ommatidia, R7 expresses UV-sensitive Rh4 and R8 expresses green Rh6.
ABSTRACT : Recently, substantial progress has been achieved in understanding the molecular basis of how different subtypes of PRs are specified (Wernet and Desplan 2004; Mikeladze-Dvali et al. 2005a ). Initially, R7 and R8 express the transcription factor spalt (sal) that is required to specify them as inner PRs and distinguish them from outer PR identity (Mollereau et al. 2001) . Then, the expression in R7 of the gene prospero (pros), which encodes a homeodomain transcription factor, further distinguishes R7 from R8 by repressing R8 rhs, rh5, and rh6 .
ABSTRACT : The generation of the two types of ommatidia, yellow and pale, includes several steps. First, the stochastic expression of the transcription factor Spineless (Ss) in a subset of R7 cells specifies yellow ommatidia. Ss is required cell autonomously in yR7 for rh4 expression and, further, cell nonautonomously for the underlying R8 cell to acquire y fate and turn on rh6 expression (Wernet et al. 2006) . The coordination between R7 and R8 rhodopsins requires a signal from pR7 that induces the pR8 fate. In sevenless mutants that lack R7, rh5 expression is lost while rh6 is expanded to almost all R8 (Papatsenko et al. 1997; Chou et al. 1999) . The y versus p choice in R8 is then reinforced by a bistable loop of regulation between the tumor suppressor gene warts (wts) and the growth regulator melted (melt) (Mikeladze-Dvali et al. 2005b) : wts is required for rh6 expression, whereas melt is essential for rh5 expression. wts and melt repress each other transcriptionally, thereby ensuring that a robust decision to express either rh5 or rh6 is made. The ho- 

18981336
ABSTRACT : The class of +adequate links contains both alternating and positive links. Generalizing results of Tanaka (for the positive case) and Ng (for the alternating case), we construct fronts of an arbitrary +adequate link A so that the diagram has a ruling, therefore its Thurston-Bennequin number is maximal among Legendrian representatives of A. We derive consequences for the Kauffman polynomial and Khovanov homology of +adequate links.
ABSTRACT : Maximum Thurston-Bennequin number, denoted by tb, is a knot invariant that has drawn a lot of recent interest. Its definition is possible because Bennequin's inequality, tb ≤ 2g − 1, bounds from above the ThurstonBennequin number of Legendrian representatives 1 of any knot type by (essentially) the genus g of the knot. Either Bennequin's inequality itself or other bounds, for example the so-called Kauffman bound on tb [9], make the extension to links possible.
ABSTRACT : Recall that the Thurston-Bennequin number is computed from an oriented front diagram by subtracting the number of right cusps from the writhe of the diagram; tb is the maximum of these numbers for all fronts representing a given link type. The Kauffman bound states that tb, and thus tb is strictly 1 To avoid undue repetition and to keep this note short, for the standard definitions of Legendrian knot theory we refer the reader to [2], [5] , or to any number of other publications. Let us only state that we work in R 3 xyz where the contact structure is the kernel of dz − ydx, so that the front projection is the xz-projection.

18981358
ABSTRACT : Background: Studies in animals and humans indicate that the interruption of body-brain connections following spinal cord injury (SCI) leads to plastic cerebral reorganization.
ABSTRACT : Objective: To explore whether inducing the Rubber Hand Illusion (RHI) via synchronous multisensory visuo-tactile bodily stimulation may reveal any perceptual correlates of plastic remapping in SCI.
ABSTRACT : In 16 paraplegic, 16 tetraplegic and 16 healthy participants we explored whether RHI may be induced by tactile stimuli involving not only the left hand but also the left hemi-face. Touching the participants actual hand or face was either synchronous or asynchronous with tactile stimuli seen on a rubber hand. We assessed two components of the illusion, namely perceived changes in the real hand in space (indexed by proprioceptive drift) and ownership of the rubber hand (indexed by subjective responses to an ad-hoc questionnaire).
ABSTRACT : Results: Proprioceptive drift and ownership were found in the healthy group only in the condition where the left real and fake hand were touched simultaneously. In contrast, no drift was found in the SCI patients who, however, showed ownership after both synchronous and asynchronous hand stroking. Importantly, only tetraplegics showed the effect also after synchronous face stroking.
ABSTRACT : Conclusions: RHI may reveal plastic phenomena in SCI. In hand representation-deprived tetraplegics, stimuli on the face (represented contiguously in the somatic and motor systems), drive the sense of hand ownership. This hand-face remapping phenomenon may be useful for restoring a sense of self in massively deprived individuals.
INTRODUCTION : Spinal cord injuries (SCI) cause an irreversible disconnection between the body and the brain. This disconnection implies a deprivation of somatosensory input to and motor output from the brain. The extent of this deprivation depends on the level and completeness of the lesion. While cervical SCI leads to tetraplegia, a clinical condition with impaired sensory-motor functions in both upper and lower limbs, SCI below the seventh cervical spinal cord segment leads to paraplegia, where deficits affect lower but not upper limbs.
INTRODUCTION : Studies indicate that sensorimotor deprivation in SCI may induce alterations in the bodily-self as indexed by the Rubber Hand Illusion (RHI), where the induction of a visuo-tactile conflict allows rapid changes in body-ownership (Botvinick and Cohen, 1998) . In initial studies of the RHI healthy individuals were asked to look at a rubber hand that was stroked by the examiner, synchronously or asynchronously with their hidden from view real hand. It appeared that only during synchronous stimulation was the rubber hand perceived as part of the participants' own body (index of ownership of an artificial hand) and the position of the real hand was perceived as having shifted toward the rubber hand ("proprioceptive drift," index of illusory perception of body in space) (Botvinick and Cohen, 1998; Ehrsson et al., 2005; Tsakiris and Haggard, 2005; Longo et al., 2008; Mohan et al., 2012; Schaefer et al., 2013) .
INTRODUCTION : The first time that the RHI paradigm was applied to SCI participants demonstrated that SCI did not alter subjective indices of the illusory hand ownership (Lenggenhager et al., 2012) . However, proprioceptive drift was found to be deviated only in subjects with defective hand perception, suggesting that plasticity-related cortical changes might influence the dynamics of the bodily-self (Lenggenhager et al., 2012) . The RHI has subsequently been used to induce a restoration of impaired hand somatosensivity in two SCI patients (Lenggenhager et al., 2013) .
INTRODUCTION : In many previous papers the "proprioceptive drift" is defined as an objective measure of the RHI. We contend it has to be considered as a subjective response. We suggest that this measure is at least as subjective as the point of subjective equivalence (PSE) (Gescheider, 1997) , i.e., the index widely used in psychophysics research which is computed starting from the participants' answers about the perception or the lack of perception of a stimulus, or the perceived difference between two sensory stimuli. For these reasons in this study we defined, the "proprioceptive drift" as a subjective index of perception of the body in space. We propose that psychogalvanic response (Armel and Ramachandran, 2003; Ferri et al., 2013) , the change in temperature (Moseley et al., 2008) , etc. are considered objective indexes in rubber-hand or full-body illusion studies.
INTRODUCTION : Brain reorganization induced by a reduction in somatosensory and motor inputs has been demonstrated in studies on animals and humans (Nahum et al., 2013) . Crucial for the present research, this type of reorganization may follow topographic rules. For example, a single cell recording study on monkeys deprived of somatosensory input due to an extended dorsal rhizotomy, demonstrated that the cortical territories formerly mapping the de-afferented skin regions (e.g., the hand) were driven by inputs coming from brain regions with adjacent, intact representation (e.g., the face) (Pons et al., 1991) . Evidence for the perceptual correlates of this topographic remapping process has been provided by studies on individuals with upper-limb (Ramachandran et al., 1992; Aglioti et al., 1997) , lower-limb (Aglioti et al., 1994a) or breast (Aglioti et al., 1994b) amputation and phantom perception of the lost body part. In particular, tactile stimuli on the face ipsilaterally to hand or finger amputations induced in a considerable number of patients the sensation of being touched not only on the face but also on the phantom hand (Ramachandran et al., 1992) or finger (Aglioti et al., 1997) . Consistent and precise, but topographically disorganized, double sensations were evoked by tactile stimuli applied to the contralesional hypoaesthesic hand in a patient with a selective lesion involving hand representation in the primary somatosensory cortex (Aglioti et al., 1999) .
INTRODUCTION : Based on the notion that the somatosensory and motor brain representation of the face and the hand are contiguous, double sensations were interpreted as an index of remapping of the face on the de-afferented hand representation. The inherent link between hand and face representations is also supported by studies on healthy subjects in whom complete temporary anaesthesia of the thumb rapidly induced the sensation that the size of the lips increased by up to 50% (Gandevia and Phegan, 1999) . Neurophysiological evidence for face-hand remapping has been provided by an EEG study documenting that tactile stimulation of the hand activates the cortical representation of the face in people who had undergone cosmetic injections of botulinum toxin to treat wrinkles (Haenzi et al., 2014) . As previously mentioned, deprivation related neuroplasticity may also be at play after spinal cord lesions. What remains unknown is whether the perceptual correlates of the plasticity found in SCI follows erratic rules (Moore et al., 2000) or may also occur according to somewhat topographic organization.
INTRODUCTION : We explored this issue by applying a novel version of the classic RHI paradigm to healthy, paraplegic, and tetraplegic people. Based on the study on monkeys with cervical SCI who showed an expansion of the face representation in the primary and non-primary somatosensory cortex toward nearby areas (Tandon et al., 2009) , we hypothesized that tetraplegics, but not paraplegics and healthy people, would experience the RHI after stimulation of their cheek synchronously with rubber hand stimulation. We modified two main aspects of the classic RHI paradigm (Botvinick and Cohen, 1998) . The first concerns the stimulation which was applied not only to the participants real hand but also to their cheek. The second is that rubber hand and real hand were vertically aligned with the former in a higher position with respect to the latter. We measured any possible vertical drift in the perceived position of the participants real hand (Bekrater-Bodmann et al., 2012) . We expected only the tetraplegics to show indices of RHI in the Face-Synchronous condition, due to possible mechanisms of plasticity in the somatosensory networks. Instead, in the paraplegics and healthy individuals, we expected to replicate RHI effects exclusively following Synchronous Hand-stimulation.
CONCLUSION : This study has demonstrated that indices of ownership of a fake hand can be induced in SCI subjects and that the indication od illusory ownership over the rubber hand is more likely to occur in the presence of upper spinal levels and thus involves greater de-afferentation. In Tetraplegics the phenomenon is also induced by facial stimulation suggesting that deprivation related plasticity may occur according to somatotopic rules. Further studies are needed to understand whether plastic changes following SCI are inherently adaptive or maladaptive (Kokotilo et al., 2009; Nishimura and Isa, 2009 ). The absence of changes in the perceived position of the body in space confirms that these two components of the RHI may be more dissociated in SCI subjects than in healthy individuals (Rohde et al., 2011) .

18982114
ABSTRACT : Background. The population in the UK is growing older and the number of elderly patients cared for on intensive care units (ICU) is increasing. This study was designed to identify risk factors for mortality in critically ill patients of .80 yr of age after surgery.

18982127
ABSTRACT : Abstract. Recently a new graph convexity was introduced, arising from Steiner intervals in graphs that are a natural generalization of geodesic intervals. The Steiner tree of a set W on k vertices in a connected graph G is a tree with the smallest number of edges in G that contains all vertices of W . The Steiner interval I(W ) of W consists of all vertices in G that lie on some Steiner tree with respect to W . Moreover, a set S of vertices in a graph G is k-Steiner convex, denoted g k -convex, if the Steiner interval I(W ) of every set W on k vertices is contained in S. In this paper we consider two types of local convexities. In particular, for every k > 3, we characterize graphs with g k -convex closed neighborhoods around all vertices of the graph. Then we follow with a characterization of graphs with g 4 -convex closed neighborhoods around all g 4 -convex sets of the graph.
INTRODUCTION : The study of abstract convexity began in the early fifties and is extensively studied in van de Vel's monograph [14] , where the interval convexity is used for introducing it. The theory of axiomatic convexity is based on just three natural conditions, imposed on a family of subsets of a given set. Definition 1. [14] . A family C of subsets of a set X is called a convexity on X if (C1) the empty set ∅ and universal set X are in C;

18982460
ABSTRACT : Background: Benign prostatic hyperplasia is a common progressive disease in aging men, which leads to a significant impact on daily lives of patients. Continuous bladder irrigation (CBI) is a supplementary option for preventing the adverse events following transurethral resection of the prostate (TURP). Regulation of the flow rate based on the color of drainage bag is significant to prevent the clot formation and retention, which is controlled manually at present. To achieve a better control of flow rate and reduce inappropriate flow rate-related adverse effects, we designed an automatic flow rate controller for CBI applied with wireless sensor and evaluated its clinical efficacy.
ABSTRACT : Methods: The therapeutic efficacy was evaluated in patients receiving the novel automatic bladder irrigation post-TURP in the experimental group compared with controls receiving traditional bladder irrigation in the control group.
ABSTRACT : Results: A total of 146 patients were randomly divided into 2 groups-the experimental group (n = 76) and the control group (n = 70). The mean irrigation volume of the experimental group (24.2 ± 3.8 L) was significantly lower than that of the controls (54.6 ± 5.4 L) (P < 0.05). Patients treated with automatic irrigation device had significantly decreased incidence of clot retention (8/76) and cystospasm (12/76) compared to controls (21/70; 39/70, P < 0.05). There was no significant difference between the 2 groups with regard to irrigation time (28.6 ± 2.7 vs 29.5 ± 3.4 hours, P = 0.077).
ABSTRACT : The study suggests that the automatic regulating device applied with wireless sensor for CBI is safe and effective for patients after TURP. However, studies with a large population of patients and a long-term follow-up should be conducted to validate our findings.
ABSTRACT : Abbreviations: BPH = benign prostatic hyperplasia, CBI = continuous bladder irrigation, TURP = transurethral resection of the prostate.
Introduction : Benign prostatic hyperplasia (BPH) is a common disease in older men with age of more than 50 years. [1] BPH is characterized by the enlargement of the prostate and clinically associated with lower urinary tract symptoms. Although BPH is not a lifethreatening disease, it has been a health concern and significantly affected the life quality of patients. [2] The current therapies for BPH mainly include transurethral resection of the prostate (TURP) and open prostatectomy. [3] However, the TURP is commonly associated with adverse events in patients following surgery, thus the adjuvant therapy options for preventing the complications are urgently needed.
Introduction : Continuous bladder irrigation (CBI) is a supplementary approach for BPH management after surgery with a view to preventing clot retention, cystospasm, and hemorrhage postoperatively. [4] Besides, CBI has been proposed to inhibit the hemorrhagic cystitis [5] and increase the survival rate following stem cell transplantation. [6] Although CBI is widely used for preventing the complications following TURP, it is not easy for nurses to take CBI for patients. [7] Nurses are responsible for ensuring a continuous flow of prescribed solution during the whole procedure. Thus, it is imperative to assess the blocked catheter by checking the color of drainage bag and controlling the flow rate. [7] Currently, the flow rate of irrigation fluid is controlled manually by nurse according to the color of drainage fluid. Inappropriate flow rate may result in adverse effects such as clot retention, cystospasm, and hemorrhage. However, research references of the manual to control the flow rate are rare. In order to achieve a better control of flow rate of irrigation fluid and reduce inappropriate flow rate-related adverse effects, we designed the automatic flow rate controller for bladder irrigation applied with wireless sensor and evaluated its clinical efficacy for patients after TURP. We expect that our study would provide therapeutic implication for patients undergoing CBI.

18982496
ABSTRACT : In this paper I discuss some constraints and implications in accessing fellow academics as research participants, a topic that has rarely been addressed thus far in the literature. I will point out that a lack of cooperation from fellow academics may defeat our research purposes, and will survey some studies involving U.S., European, and Chinese academics as research participants to illustrate education researchers' efforts to work with fellow academics against the odds. By referencing my personal experience of engaging with Chinese academics, I will then discuss the role of personal contacts in research and reflect upon various constraints in accessing fellow academics as research participants. I will suggest that, when we do participate in a fellow researcher's project, the incentive is a desire to support our peers in the spirit of "academic citizenship."

18982504
ABSTRACT : Abstract-This paper gives a comprehensive study on the modeling and design challenges of Through Silicon Vias (TSVs) in high speed three-dimensional (3D) system integration. To investigate the propagation characteristics incurred by operations within the ultra-broad band frequency range, we propose an equivalent circuit model which accounts for rough sidewall effect and high frequency effect. A closed-form expression for TSV metal oxide semiconductor (MOS) capacitance in both depletion and accumulation regions is proposed. The coupling of TSV arrays and near and far field effect on crosstalk analysis are performed using 3D Electro-Magnetic (EM) field solver. Based on the TSV circuit model, we optimize the TSVs' architecture and manufacturing process parameters and develop effective design guidelines for TSVs, which could be used to resolve the signal integrity issues arising at high frequency data transmission in 3D Integrated Circuits (ICs).
INTRODUCTION : Three-dimensional integrated circuits (3D ICs) have become an inordinately propitious technology. The advantages gained by vertically stacking with multiple dies are: (1) a reduction in form factor, (2) high system speed, (3) high interconnect densities, (4) a decrease in overall wire length, and (5) reduced power consumption [1] [2] [3] . To analyze the electrical performance of 3D ICs, it is crucial to model and design the Through Silicon Vias (TSVs) accurately and efficiently because of their critical roles in the overall communication architecture [4] . TSVs are most commonly fabricated by high aspect ratio deep silicon etchinglined with a dielectric to provide electrical isolation; and super conformed filled with copper. Unlike the conventional interconnects, TSVs are essentially metal insulator semiconductor (MIS) devices [5] wherein the dielectric layer (typically SiO 2 ) is deposited to isolate the conductive metals from the substrate [6] . From the design standpoint, as related to parasitic extraction, TSVs can be incorporated as a separate cell (within conventional design tools) that demands proper analytical modeling and characterization. The ultimate goal is clear and straightforward: low-cost, high-yield process technologies, and successful attainments require concise optimizations of the TSV electrical design process, which demands a comprehensive consideration of complex TSV electrical modeling and analysis [7] .
INTRODUCTION : Metal oxide semiconductor (MOS) effect is an important phenomenon in TSV based 3D ICs which could overcome the misestimation that occurs in determining the cylindrical capacitance. When TSVs are operated in the low frequency range, the MOS effect in TSVs can be quite satisfactorily modeled as MOS capacitances [8] ; whereas, when TSVs are operated in the ultra-broad band frequency range, the electrical behavior is considerably altered and deviated into a domain of scarcely imaginable complexity. High speed switching can dynamically bias the TSV-MIS interface and result in regions of electronic accumulation or depletion; the capacitance becomes a nonlinear function of signal bias. In 3D ICs, the MOS capacitance plays a significant role to decrease the total capacitance due to biased substrate and thus it reduces the leakage current into the silicon material [2] .
INTRODUCTION : Most of the TSV modeling approaches focus on the radio frequency (RF) applications with operation frequency of several GHz or even lower, whereas the TSV technology specifically for millimeter wave range (mmW)/THz applications is seldom reported. The mmW communication has emerged as an enabling technology to resolve the spectrum shortage issue due to significant growth of mobile data traffic [9] [10] [11] . There are some outstanding challenges due to electro-magnetic (EM) effects that are associated with the mmW technology such as impedance mismatching, signal reflections, crosstalk, and radiation [12, 13] . It is therefore a prudent consideration to address the critical issues which influence the properties of the TSVs in ultra-high and extremely-high frequency range [7] . These challenges can be addressed by properly modeling and designing the TSVs considering those effects in emerging and attractive 3D integration approach. A cross sectional view of an mmW transceiver module in 3D silicon interposer technology is shown in Figure 1 . One of the most important issues is the sidewall roughness which induces a significant quantity of leakage current within the TSVs [14] . The effect of sidewall roughness must be considered within the mmW frequency range, since the root mean square (rms) height of the sidewall roughness features become comparable to the skin depth. Heretofore, a considerable amount of research work on TSV modeling and analysis have been performed to address the impact of TSVs on high speed signals, to analyze the TSV crosstalk, to study the TSV resistance, as well as to apply the full-chip extraction and optimization [15] [16] [17] [18] [19] [20] [21] [22] . Reasonably comprehensive TSV models have been constructed which characterized resistance, inductance, capacitance, and conductance. These modeling methodologies can be categorized into three discrete classes; depending on the type of electromagnetic technique used: full-wave modeling, quasi-static modeling, and analytical modeling [15] . These three modes correspond to different silicon resistivity levels; (1) a slow wave, low frequency mode, where the electrical field is as if "screened" by the silicon substrate, and the magnetic field penetrates somewhat into the substrate; (2) a quasi-transverse electromagnetic mode, wherein both electrical and magnetic fields penetrate deeply into the substrate; and: (3) a very high frequency skin effect mode, where the characteristic dimension of the silicon skin depth is smaller than the TSV spacing.
INTRODUCTION : This paper proposes a closed-form analytic expression for TSV capacitance which accounts for both depletion and accumulation effects. Furthermore, we introduce a rough sidewall TSV model to account for the propagation characteristics within the ultra-broad band frequency range. Our endeavor culminates with an equivalent circuit model that accurately captures all the parasitic elements of various TSVs arrangements. The proposed circuit model accounts for high frequency skin effects, eddy currents in the substrate, MOS effects, and sidewall roughness effects. The model's compactness and compatibility with Simulation Program with Integrated Circuit Emphasis (SPICE) simulators allows Signal TSV Ground TSV Figure 2 . TSV pair with sidewall roughness. Figure 3 . SEM image of TSV with sidewall roughness using Bosch etching [14] .
INTRODUCTION : the electrical modeling of various TSVs arrangements without the need for computationally expensive field solvers, which significantly reduces the simulation running time. We also focused on the coupling and cross talk analysis between the TSV interconnections which is critical in signal integrity. Based on the proposed TSV model, we optimize the TSVs' architecture and manufacturing process parameters and develop effective design guidelines for TSVs which could be used to resolve the signal integrity issues arising at high speed data transmission in 3D ICs.
CONCLUSION : Analytical modeling and 3D electromagnetic field simulation for TSVs have been performed, and the challenges associated with electrical design have been analyzed. A rough sidewall TSV is modeled that captured the impact of conductor sidewall roughness in high speed 3D interconnects. The proposed model is analytically calculated and validated by comparing the S-parameters predicted by both the model and the HFSS simulation up to 100 GHz in both magnitude and phase. From this study, it is found that the effect of TSV sidewall roughness becomes a very important factor when modeling TSVs in the extremely high frequency band. MOS effect and high frequency effect are investigated in mmW/THz frequency range. The impact of the voltage dependent and nonlinear capacitance on the performance of high speed differential signals is analyzed with the data of eye diagram approach. The coupling of TSV arrays under high speed operations using 3D EM field solver is performed and the near and far field effect on crosstalk analysis is observed. Parametric study is performed for the assessment of quantitative influence on loss properties of TSV and the impact of the frequency, pitch, dielectric, and TSV length is observed on the overall performance.

18982512
ABSTRACT : It is shown that the local solution of parabolic equation with nonlocal boundary condition representing entropy can be extended to whole time domain for weights with large L 1 norms. When the weight is identically zero on some part of the boundary, it is shown that the boundary values can decrease even when the other weights are some large.
Introduction. : This paper is concerned with the investigation of large time behavior of solutions to parabolic initial value problem subject to nonlocal boundary condition which describes the entropy in a quasi-static theory of thermoelasticity, namely, u t (x, t) = ∆u(x, t) + µu, (x, t) ∈ Ω × (0, T ), u(z, t) = Ω f(z, y)u(y, t) dy, z ∈ ∂Ω, 0 < t < T, u(x, 0) = u 0 (x), x ∈ Ω, (1.1) where u 0 (x) are assumed to be continuous on Ω and µ is a constant. The function f (z, y) is defined for z ∈ ∂Ω and y ∈ Ω and continuous functions in y ∈ Ω for each z ∈ ∂Ω. Since for each z ∈ ∂Ω, f (z, y) plays the weight of integration in (1.1), the function f (z, y) is called weights throughout this paper. Denote D T = Ω × (0, T ) and D T ∪ Γ T = Ω × [0, T ). The variable z stands for a generic point of boundary ∂Ω. The large time behavior of the solution u to Problem (1.1) is studied by taking an upper bound for u in Section 2. It is shown that the solution of Problem (1.1) with large weights f (z, ·) for each z ∈ ∂Ω has an exponential lower bound in Section 3. Moreover, it is shown that the difference between maximum and minimum value on the boundary ∂Ω can decrease if the weights are zero on a nonempty subset of ∂Ω in z and zero on the boundary in y, that is, f (z, y) = 0, z ∈ Γ ⊂ ∂Ω for all y ∈ Ω and f (z, y) = 0, y ∈ ∂Ω.
Introduction. : A supersolution is defined by reversing inequalities in (1.2). Using the notions of super and subsolution, we state the following: 
Introduction. : As a corollary, u 0 (x) > 0(< 0) implies u(x, t) > 0(< 0). Moreover, the local existence and uniqueness can be written as:
Introduction. : For the proofs of the comparison principle and the local existence theorem, see [4] . Here one can see that the results hold without any restriction to constant µ in the parabolic equation in (1.1). In this paper, we are only interested in the positive solutions of Problem (1.1), and hence we assume that u 0 > 0. Throughout this paper, L 1 and L 2 norm of a function on Ω is denoted by · 1 and · 2 , respectively.

18982781
ABSTRACT : Abstract Developing methods that result in targeting of therapeutic molecules in gene therapies to target tissues has importance, as targeting can increase efficacy and decrease off target-side-effects. Work from my laboratory previously showed that the extracellular matrix protein Del1 is organized in the extracellular matrix (ECM) via the Del1 deposition domain (DDD). In this work, a fusion protein with DDD was made to assay the ability to immobilize an enzyme without disrupting enzymatic function. A prostatic cancer-derived cell line LNCap that grows in an androgendependent manner was used with 3a-hydroxysteroid dehydrogenase (3 aHD), which catalyzes dihydrotestosterone (DHT). Plasmids encoding a 3aHD:DDD fusion were generated and transfected into cultured cells. The effects of 3aHD immobilized in the ECM by the DDD were evaluated by monitoring growth of LNCap cells and DHT concentrations. It was demonstrated that the DDD could immobilize an enzyme in the ECM without interfering with function.
Introduction : Mouse Del1 is an ECM protein secreted by embryonic endothelial cells and hypertrophic chondrocytes [1] . Del1 consists of five domains: three epidermal growth factor (EGF) repeat domains (E1, E2, E3) and two Discoidin domains (C1, C2). Work from my laboratory recently showed that the C-termini of the C1 domains are essential for organization of Del1 into the ECM and that both the E3 repeat domain and the N-terminus of the C1 domain play supportive roles in organization into the ECM [2] . We termed this portion of the C1 domain the Del1 deposition domain (DDD). Fusion proteins that include the DDD and an alkaline phosphatase protein as a marker accumulate in the ECM without interfering with enzyme activity of the alkaline phosphatase.
Introduction : In some therapeutic treatments, localization of bioactive molecules specifically to the target tissue(s) increases the concentration of bioactive molecules in the target cells. This results in higher efficiency of treatment and because the concentration of the bioactive molecules in non-target tissues is minimized, this can also result in a lower incidence of treatment side effects. The purpose of this study is to ask if addition of DDD to an enzyme can result in localization of that enzyme to the ECM without affecting activity of the enzyme. Because the space between a cell and the ECM is not a simple vacancy, it is important to ask if a fusion protein derived from an enzyme is functional in the microenvironment of the ECM.
Introduction : To address this, I designed a cell-based system in which it is possible to readily monitor both an enzyme and the enzymatic products it produces. LNCap cells are prostatic cancer-derived cells that grow in an androgendependent manner and these cells were used as a model target cell in this study [3, 4] . DDD was used to immobilize 3a-hydroxysteroid dehydrogenase (3aHD), which intracellularly catalyzes the conversion of aldehydes and ketones to alcohols, and converts DHT to 5a-androstane-3a-, 17b-diol physiologically [5] . In this study, recombinant 3aHD proteins were secreted extracellularly as a result of addition of a signal sequence at the Nterminus of the recombinant protein (Fig. 1a, b) . The addition of a DDD to the 3aHD protein successfully localized the protein to the ECM. Moreover, the 3aHD in ECM was biochemically active and suppressed growth of LNCap cells.

18983391
ABSTRACT : The conventional approaches to routing 
Introduction : Traffic engineering is one of the active research areas in communication networks. The traditional form of routing and resource allocation, as the two major building blocks of traffic engineering cannot address quality of service requirements of flows while optimizing network utilization for complex communication networks. In this paper we consider ant colony algorithms to address this problem. In this approach foraging ants find the shortest path in a synergistic way. While moving back and forth between nest and food, ants mark their paths by secreting pheromone.
Introduction : Step-by-step routing decisions are biased based on the local intensity of pheromone field which is the colony's collective and distributed memory. Ants will follow the most dense route in a maximum likelihood way. The actual algorithm implemented in nature by real ants is slow in convergence.
Introduction : Our studies show that the ant-based routing models are sensitive to initial parameters settings. Only careful adjustments of these initial parameters results in an acceptable convergence behavior. The robust behavior of the real ant compared to the routing algorithms derived from it justifies the investigation of these algorithms in depth to find the reasons behind their shortcomings. We present results from our study of ant behavior in a quest for a robust algorithm. Most of the ant-based algorithms have been studied with limited sourcedestination traffic. In this work we have extended the algorithm to a more realistic environment in which multiple sourcedestination flows compete for the resources. We study the routing and load balancing behavior that emerges and show how the behavior relates to analytical approaches for optimal minimum delay algorithms by Gallager [3] , Mitra [6] , and others [4] , [5] . We show the results using simulations in OPNET and derive recommendations on the improvement of the ant-like algorithms to achieve load balancing.
Introduction : The rest of this paper is organized as follows. In chapter 2 we highlight the problems of traffic engineering. Chapter 3 is dedicated to the ant algorithm to provide the overall view of the ant approach. In chapter 4 we briefly describe the outstanding analytical methods introduced to address the problem of dynamic routing and flow assignment. Our experiments and the results including our view of the ant algorithm is discussed in chapter 5. Finally we conclude the paper in chapter 6.
Conclusions and Future Works : In conclusion, our suggestion regarding the use of ant algorithms is that improved behavior is possible by augmenting it with the analytical computation.
Conclusions and Future Works : Our current work involves applying analytical solutions to ant like algorithms for routing and flow assignment to extract a robust and autonomic routing algorithm capable of tolerating predictable changes in traffic patterns and learn the unpredicted ones. 

18985891
ABSTRACT : Background
Introduction : Leptospirosis is a zoonotic bacterial disease that occurs in diverse epidemiological settings but imparts its greatest burden on resource-poor populations [1] [2] [3] [4] [5] [6] . The disease has a broad geographical distribution due to the large spectrum of mammalian hosts that harbour and excrete the spirochete agent from their renal tubules [1, 3, 7] . Leptospirosis affects risk groups that are exposed to animal reservoirs or contaminated environments, such as abattoir and sewage workers, military personnel, and individuals partaking in water sports and recreation [8] [9] [10] [11] [12] . However, leptospirosis has a broader health impact as a disease of impoverished subsistence farmers [13] [14] [15] , cash croppers, and pastoralists [16] from tropical regions.
Introduction : Furthermore, leptospirosis has emerged as a health threat in new settings due the influence of globalization and climate. Disasters and extreme weather events are now recognized to precipitate epidemics [6] . The emergence of leptospirosis in Thailand [17] and Sri Lanka [18] highlight the potential for the disease to rapidly spread and cause large unexplained nationwide outbreaks. Finally, the expansion of urban slums worldwide has created conditions for ratborne transmission [19] [20] [21] [22] [23] [24] . Urban epidemics are reported in cities throughout the developing world [6, 19, 25] and will likely intensify as the world's slum population doubles to two billion by 2030 [26] .
Introduction : The major burden attributed to leptospirosis has been its severe life-threatening manifestations. Leptospirosis has emerged as an important cause of pulmonary haemorrhage syndrome [27] [28] [29] [30] and acute kidney injury due to Weil's disease [31] in many regions where transmission is endemic. Case fatality for pulmonary haemorrhage syndrome and Weil's disease is more than 10% and 70% respectively [14] . In addition, leptospirosis is increasingly recognized as an important cause of undifferentiated fever [16, [32] [33] [34] [35] [36] [37] [38] . The majority of leptospirosis patients are not recognized or misdiagnosed as malaria [16] , dengue [39] [40] [41] , and other causes of an acute febrile illness. The lack of an adequate diagnostic test [42, 43] has further contributed to underreporting of cases [44, 45] , as well as deaths [39] . Underestimation of the morbidity and mortality due to leptospirosis is therefore common [44] and has directly contributed to its neglected disease status.
Introduction : The lack of reliable estimates of the leptospirosis burden has hampered efforts to formulate the investment case to address key barriers, such as improved diagnostics, and identify effective prevention and control measures. Leptospirosis is amenable to One Health approaches to intervention [46] , since it is an animal health problem and a cause of economic loss in the same impoverished settings where the human disease burden is high. However, current estimates of cases and deaths rely on national surveillance data compiled from selected countries [47] . Pappas et al performed a review of reports and published literature, which identified regions with high endemicity [7] . Attempts have not been made to systematically estimate the global and regional disease burden, as has been done for other neglected diseases in the Global Burden of Disease (GBD) Study 2010 [48] . The World Health Organization (WHO) convened the Leptospirosis Epidemiology Reference Group (LERG) to guide this task [44] . Herein, we present the findings of a study that aimed to perform a systematic literature review of the data on leptospirosis morbidity and mortality, estimate the annual burden of cases and deaths, and identify GBD and WHO regions with the highest burden to inform local decision making and policy.

18986705
ABSTRACT : We describe cloned segments of rDNA that contain short type I insertions of differing lengths. These insertions represent a coterminal subset of sequences from the right hand side of the major 5kb type I insertion. Three of these shorter insertions are flanked on both sides by a short sequence present as a single copy in uninterrupted rDNA units. The duplicated segment is 7, 14 and 15 nucleotides in the different clones. In this respect, the insertions differ from the 5kb type I insertion, where the corresponding sequence is found only at the right hand junction and where at the left hand side there is a deletion of 9 nucleotides of rDNA (Roiha et al. ,1981). One clone is unusual in that it contains two type I insertions, one of which is flanked by a 14 nucleotide repeat. The left hand junction of the second insertion occurs 380 nucleotides downstream in the rDNA unit from the first. It has an identical right hand junction to the other elements and the 380 nucleotide rDNA sequence is repeated on both sides of the insertion. We discuss the variety of sequence rearrangements of the rDNA which flank type I insertions.
ABSTRACT : The rDNA of Drosophila melanogaster can contain two types of non-homologous insertion sequences in the 28S gene (1-8). These insertions are unlike intervening sequences found in other genes and appear to have a repressive effect on the transcription of rDNA. It is possible to detect only a few high molecular weight transcripts containing insertion sequences and yet the nucleotide sequence of the region in which rDNA transcription initiates is identical in units containing insertions and those without (9,10).
ABSTRACT : The type I insertions are found in more than 50% of the rDNA units of the X chromosome and not in the rDNA of the Y chromosome (11,12,13). The most common type I insertion is about 5kb long, but in addition shorter insertions have been described which are mainly comprised of 0.5kb and lkb of DNA from the right hand side of the 5kb sequence. Roughly 50% of the type I sequences are found at other chromosomal sites, where they occur predominantly in
INTRODUCTION : The rDNA of Drosophila melanogaster can contain two types of non-homologous insertion sequences in the 28S gene (1-8). These insertions are unlike intervening sequences found in other genes and appear to have a repressive effect on the transcription of rDNA. It is possible to detect only a few high molecular weight transcripts containing insertion sequences and yet the nucleotide sequence of the region in which rDNA transcription initiates is identical in units containing insertions and those without (9,10).
INTRODUCTION : The type I insertions are found in more than 50% of the rDNA units of the X chromosome and not in the rDNA of the Y chromosome (11,12,13). The most common type I insertion is about 5kb long, but in addition shorter insertions have been described which are mainly comprised of 0.5kb and lkb of DNA from the right hand side of the 5kb sequence. Roughly 50% of the type I sequences are found at other chromosomal sites, where they occur predominantly in tandem arrays (14) . These type I sequences have probably undergone transposition from the nucleolus since the units within the tandem arrays are flanked by very short segments of the 28S gene (15) . Many Dipteran f l i e s have insertion sequences in their rDNA at the same site as D.melanogaster, as determined by low resolution mapping techniques (16) (17) (18) (19) . In the case of D . v i r i l i s the insertions have been shown by sequencing experiments to be at exactly the same site as the type I sequences of D.melanogaster. The D . v i r i l i s insertions, however, are flanked on both sides by a short segment of the gene present only once in uninterrupted units (20) . In our previous sequence analysis of the junctions of several long type I insertions with the D.melanogaster 28S gene, we found only a single copy of this rDNA sequence to the right of the insertion. At the l e f t hand junction nine nucleotides of rDNA are deleted (15) .
INTRODUCTION : In order to determine whether this was the only arrangement of sequences flanking type I insertions in the genome we decided to sequence the junctions between the short type I insertions and rDNA. In the cloned EcoRI fragments of rDNA which we examine in this paper, the short type I insertions are flanked by a duplicated sequence of 7 to 15 nucleotides with no deletions of flanking rDNA.

18986901
ABSTRACT : Abstract The formalism of Quantum Mechanics is based by definition on conserving probabilities and thus there is no room for the description of dissipative systems in Quantum Mechanics. The treatment of time-irreversible evolution (the arrow of time) is therefore ruled out by definition in Quantum Mechanics. In Quantum Field Theory it is, however, possible to describe time-irreversible evolution by resorting to the existence of infinitely many unitarily inequivalent representations of the canonical commutation relations (ccr). In this paper I review such a result by discussing the canonical quantization of the damped harmonic oscillator (dho), a prototype of dissipative systems. The irreversibility of time evolution is expressed as tunneling among the unitarily inequivalent representations. Canonical quantization is shown to lead to time dependent SU(1,1) coherent states. The exact action for the dho is derived in the path integral formalism of the quantum Brownian motion developed by Schwinger and by Feynman and Vernon. The doubling of the phase-space degrees of freedom for dissipative systems is related to quantum noise effects. Finally, the rôle of dissipation in the quantum model of the brain and the occurrence that the cosmological arrow of time, the thermodynamical one and the biological one point into the same direction are shortly mentioned.
Introduction : The formalism of Quantum Mechanics (QM) is based on conserving probabilities. In principle, therefore, there is no room for the description of time-irreversible evolution (the arrow of time) in QM. One has to introduce some sort of generalized quantum formalism in order to describe dissipative systems. The developments of the theory of unstable states going beyond the Breit-Wigner treatment and other phenomenological approaches have been frequently reported in the literature. See for example refs. [1] - [5] .
Introduction : Dissipative systems have been analyzed in the path integral formalism by Schwinger [6] and by Feynman and Vernon [7] from the point of view of the quantum theory for Brownian motion and are of course a major topic in non-equilibrium statistical mechanics and non-equilibrium Quantum Field Theory (QFT) at finite temperature [8] - [11] .
Introduction : In this paper I report on the results [12] - [16] on dissipative systems in quantum theory which show that QFT does allow a correct treatment of the arrow of time provided the full set of unitarily inequivalent (ui) representations of the canonical commutation relations (ccr) is used. I show [17] that the proper algebraic structure of QFT is the deformed Hopf algebra [18, 19] and that the doubling of the phase-space degrees of freedom implied by such a structure is related to quantum noise effects in the case of dissipative systems [15] .
Introduction : The microscopic theory for a dissipative system must include the details of the processes responsible for dissipation, including quantum effects. One may start since the beginning with a Hamiltonian that describes the system, the bath and the system-bath interaction. Subsequently, the description of the original dissipative system is recovered by the reduced density matrix obtained by eliminating the bath variables which originate the damping and the fluctuations. The problem with dissipative systems in QM is indeed that ccr are not preserved by time evolution due to damping terms. The rôle of fluctuating forces is in fact the one of preserving the canonical structure.
Introduction : It is known since long time [20] that, at a classical level, the attempt to derive from a variational principle the equations of motion defining the dissipative system requires the introduction of additional complementary equations.
Introduction : This latter approach has been pursued since several years also in context of quantum theory. In refs. [21] and [12] - [16] the quantization of the damped harmonic oscillator (dho) has been studied by doubling the phase-space degrees of freedom (see also [22] for the study of unstable particle in QFT). The doubled degrees of freedom play the rôle of the bath degrees of freedom. Let me observe that the canonical formalism is devised solely for closed systems, therefore in order to produce the canonical quantization of the damped oscillator, it is necessary and sufficient to close the system, namely to "balance" the energy flux, the momentum exchange, etc.. For that task, and only for that task, we do not really need to know the details of the environment and not even the details of the system-environment coupling: therefore, for such a limited task, we may "simulate" the environment as a collection of oscillators whose k-modes match the k-modes of our damped oscillator. With such a choice the environment, depicted as the system time-reversed double, is treated as the "effective" environment. Of course, in those cases in which such a crude simplification is not enough (we might be really interested in the details of the system-environment interface, for example) much more care is needed and the doubling picture is not enough. In Sec. 2 I present the approach based on the system doubling.
Introduction : I would like to stress that the analysis for dissipative systems and the arrow of time presented in this paper should not be considered to be something just formal. It is a real problem the one of the description of open systems in a mathematically consistent formalism in QFT. QFT is in fact the only available theoretical scheme to describe high energy physics, as well as condensed matter physics, and quantum systems are always open systems interacting with their environment. It is true that in many cases the approximation of treating them as closed systems is very useful and successful for phenomenological computations, nevertheless there are many cases in which dissipative effects and breakdown of time reversal symmetry cannot be neglected. In these latter circumstances we do need a reliable, mathematically consistent QFT formalism.
Introduction : The approach here presented has revealed to be useful in several applications of physical interest, ranging from unstable particles [22] , to coherence in quantum Brownian motion [23] , squeezed states in quantum optics [12, 24, 25] , topologically massive theories in the infrared region in 2+1 dimensions [26] , the Chern-Simons-like dynamics of Bloch electrons in solids [26] , and has features also common to two-dimensional gravity models [27] , to the study of quantization arising from the loss of information [28, 29] , to the quantization of matter in curved background [30] . Moreover, it has been applied [31, 32, 33] to the study of the memory capacity problem in the quantum model for the brain [34] .
Introduction : It has been known [21] that in QM time evolution of the dho leads out of the Hilbert space of states; in other words, the QM treatment of dho does not provide a unitary irreducible representation of SU(1,1) [35] . To cure these pathologies one must move to QFT, where infinitely many unitarily inequivalent representations of the ccr are allowed (in the infinite volume or thermodynamic limit). The reason for this is that the set of the states of the damped oscillator splits into ui representations (i.e. into disjoint folia, in the C*-algebra formalism) each one representing the states of the system at time t: the time irreversible evolution is described as tunneling between ui representations. A remarkable feature of this description thus emerges: at microscopic level the irreversibility of time evolution (the arrow of time) is expressed by the non unitary evolution across the ui representations of the ccr.
Introduction : I remark that the nature of the ground states of the ui representations is the one of the SU(1,1) generalized coherent states. Furthermore, the squeezed coherent states of light entering quantum optics [36, 24, 25] can be identified [12] , up to elements of the group G of automorphisms of su(1, 1), with the states of the quantum dho.
Introduction : It has been also shown [14] that the dho states are time dependent thermal states, as expected due to the statistical nature of dissipation. This is reported in Sec. 3. The formalism for the dho turns out to be similar to the one of real time QFT at finite temperature, also called thermo-field dynamics (TFD) [9, 10, 11] . In refs. [37] and [38] such a connection with TFD has been further analyzed and the master equation has been discussed [37] .
Introduction : In ref. [15] the exact action for the dho in the path integral formalism of Schwinger and Feynman and Vernon has been obtained. The initial values of the doubled variables have been related to the probability of quantum fluctuations in the vacuum, a result which is interesting also in the more general case of thermal field theories. I report such results in Sec. 4.
Introduction : In Sec. 5 I show that the proper algebraic structure of QFT is the Hopf algebra [24] , which includes the usually considered Weyl-Heisenberg algebra (WH). I then show [17] that dissipative systems are properly described in the frame of the q-deformed Hopf algebra [18, 19, 39] . The q-deformation parameter turns out to be related with time parameter in the case of dho and with temperature in the case of thermal field theory. In both cases, the q-parameter acts as a label for the ui representations. Such a conclusion confirms a general analysis [40] which shows that the Weyl representations in QM and the ui representations in QFT are indeed labeled by the deformation parameter.
Introduction : Sec. 6 is devoted to the conclusions. There I mention some recent developments which point to the rôle of dissipation in the quantization procedure [28, 29] and I also shortly recall the rôle of dissipation in the quantum model of the brain [31, 34] and on the occurrence that the cosmological arrow of time, the thermodynamical one and the biological one point into the same direction [32, 33] .

18987532
ABSTRACT : The activity of recombinant human growth hormone (rhGH) in enhancing CD34 ؉ cell mobilization elicited by chemotherapy plus recombinant human granulocyte colony-stimulating factor (rhG-CSF) was evaluated in 16 hard-to-mobilize patients, that is, those achieving a peak of circulating CD34 ؉ cells 10/L or less, or a collection of CD34 ؉ cells equal to or less than 2 ؋ 10 6 /kg. Patients who had failed a first mobilization attempt with chemotherapy plus rhG-CSF (5 g/kg/d) were remobilized with chemotherapy plus rhG-CSF and rhGH (100 g/kg/d). As compared with rhG-CSF, the combined rhGH/rhG-CSF treatment induced significantly higher (P < .05) median peak values for CD34 ؉ cells/L (7 versus 29), colonyforming cells (CFCs)/mL (2154 versus 28 510), and long-term culture-initiating cells (LTC-ICs)/mL (25 versus 511). Following rhG-CSF and rhGH/rhG-CSF, the median yields of CD34 ؉ cells per leukapheresis were 1.1 ؋ 10 6 /kg and 2.3 ؋ 10 6 /kg (P < .008), respectively; the median total collections of CD34 ؉ cells were 1.1 ؋ 10 6 / kg and 6 ؋ 10 6 /kg (P < 
Introduction : Mobilized peripheral blood progenitor cells (PBPCs) have an established role in the management of patients with non-Hodgkin lymphoma (NHL), 1,2 relapsed Hodgkin lymphoma (HL), 3 or multiple myeloma (MM) 4 who are eligible for high-dose sequential chemotherapy and autologous stem cell transplantation (ASCT). Because the number of infused CD34 ϩ cells correlates with the rate of hematopoietic reconstitution, the availability of adequate amounts of PBPCs is a prerequisite for the feasibility of high-dose chemotherapy and ASCT. 5 There is a general consensus that patients receiving PBPC autografts containing less than or equal to 2 ϫ 10 6 CD34 ϩ cells/kg are at risk for delayed hematopoietic recovery, increased procedure-related morbidity and mortality, engraftment failure, and myelodysplasia, whereas those receiving 5 ϫ 10 6 or more CD34 ϩ cells/kg experience prompt and durable hematopoietic engraftment. 6 PBPCs are mobilized efficiently by the administration of short courses of recombinant human (rh) granulocyte colony-stimulating factor (G-CSF) alone or during recovery from cytotoxic chemotherapy. [7] [8] [9] Indeed, due to prior chemoradiotherapy, disease stage, or disease-intrinsic factors, a substantial proportion of cancer patients (10%-30%) mobilize suboptimal amounts of CD34 ϩ cells (ie, Յ 2 ϫ 10 6 CD34 ϩ cells/kg). [10] [11] [12] The lack of autologous stem cells raises important issues for the clinical management of patients for whom ASCT has proved to be clinically beneficial.
Introduction : PBPC mobilization might be improved by molecules capable of interfering with the mechanisms regulating hematopoietic stem cell trafficking. [13] [14] [15] [16] An increase of CD34 ϩ cell mobilization might also be achieved by combinations of cytokines, such as granulocyte-macrophage colony-stimulating factor (rhGM-CSF) plus rhG-CSF, 17 interleukin-3 (rhIL-3) plus rhG-CSF or rhGM-CSF, 18 and PIXY-321. 19 Additionally, PBPC mobilization may be enhanced by incorporating in the standard mobilization regimen early-acting cytokines, such as stem cell factor (rh-SCF) [20] [21] [22] or flt-3 23 ligand. So far, substitutes or adjuncts to rhG-CSF either failed to substantially improve the mobilization of blood progenitors achieved with rhG-CSF alone or resulted in a limited improvement. 24, 25 Growth hormone (GH) is a pleiotropic cytokine targeting a variety of nonhematopoietic and hematopoietic cells by binding to a specific receptor. 26, 27 In vitro, rhGH significantly increases colony formation by human myeloid (granulocyte-macrophage colonyforming unit [CFU-GM]) and erythroid (erythroid burst-forming unit [BFU-E]) progenitors. [28] [29] [30] In vivo, a 7-day course of rhGH induces a significant increase of marrow and spleen CFU-GMs and BFU-Es in both normal and azidothymidine-treated mice. 31 Following syngeneic marrow transplantation, rhGH significantly hastens multilineage hematopoietic recovery in mice. 32 When given for 4 weeks, rhGH restores the age-associated decline of marrow cellularity in rats, 33 as well as the stem cell mobilization capacity in mice. 34 Collectively, these data suggest that bone marrow is an important target for the action of rhGH and allow us to hypothesize that rhGH might enhance rhG-CSF-induced mobilization of CD34 ϩ cells by increasing the numbers of marrow stem cells susceptible to be released on a subsequent or concomitant mobilization stimulus.
Introduction : Based on these findings, we conducted a pilot study aimed at investigating the feasibility and efficacy of rhGH administration as an adjunct to chemotherapy plus rhG-CSF for enhancing stem cell mobilization. Included in this study were 16 consecutive patients with relapsed or refractory hematologic malignancies who had failed a first mobilization attempt with chemotherapy plus rhG-CSF. Patients were then remobilized with chemotherapy plus rhG-CSF and rhGH. Mobilization failure was defined as a peak value of circulating CD34 ϩ cells equal to or less than 10/L, or a collection of CD34 ϩ cells equal to or less than 2 ϫ 10 6 /kg. To eliminate the interpatient variability induced by the considerable heterogeneity in patient characteristics and responses to a given mobilization regimen, we prospectively compared the number of PBPCs mobilized into blood after 2 consecutive cycles of the same chemotherapy regimen administered in the same patient. The objectives of the study were to: (1) assess the activity of rhGH in increasing rhG-CSF-induced mobilization and harvesting of CD34 ϩ cells, committed colony-forming cells (CFCs), as well as the more primitive long-term culture-initiating cells (LTCICs), and (2) assess the safety and tolerability of rhGH, given in combination with rhG-CSF. Our data indicate that in the great majority of poor mobilizers addition of rhGH to rhG-CSF allows efficient mobilization and collection of CD34 ϩ cells with maintained functional properties.

18987535
ABSTRACT : Abstract. This study analyzed the spatio-temporal patterns of 4,587 (94% of the total) confirmed dengue cases in Kaohsiung and Fengshan Cities (a two-city area) that occurred in Taiwan from 2001 to 2003. The epidemic had two simultaneous distinct diffusion patterns. One was a contiguous pattern, mostly limited to 1 km from an initial cluster, reflecting that there was a rapid dispersal of infected Aedes aegypti and viremic persons. The second followed a relocation pattern, involving clusters of cases that diffused over 10 weeks starting from the southern and moving to the northern parts of the two-city area. The virus from one clustering site jumped to several distant areas where it rapidly dispersed through a series of human-mosquito transmission cycles to several localities. In both patterns, transmission of disease quickly enlarged the epidemic areas. Future dengue control efforts would benefit from a timely syndromic surveillance system plus extensive public education on how to avoid further transmission.
INTRODUCTION : In the last few decades, there have been an increasing number of dengue epidemics in tropical and subtropical countries. 1 The transmission of dengue occurs primarily through infected female mosquitoes, Aedes aegypti or Aedes albopictus, which acquire the virus when taking blood meals from infected humans. 2 Ae. aegypti, which has a multi-meal feeding behavior on several people, is most often found among humans in urban dwellings. 3, 4 Because dengue virus infection can be mildly/atypically symptomatic, even asymptomatic, it is likely that disease can spread silently and can remain in a community without being noticed. 5, 6 As a result, wherever there were clustering dengue cases confirmed for consecutive weeks, there might have been the possibility that infected mosquitoes are present but undetected. 7 Therefore, if the spatial and temporal factors of the clustering of dengue cases were better understood, we could more efficiently prevent and control the transmission of dengue virus.
INTRODUCTION : Between 1987 and 2001, there were epidemics of dengue in Taiwan almost every 3-4 years. They started from imported cases, and most of the epidemics were small-scale involving zero or a small number of cases of dengue hemorrhagic fever (DHF). 8 From 2001 to 2003, however, epidemics of dengue/ DHF predominantly caused by dengue virus serotype 2 (DENV-2) occurred in the two-city area of Kaohsiung and Fengshan, where Taiwan experienced its largest and most severe epidemic in 60 years ( Figure 1 ). Previous studies, using geographical point pattern analysis to study outbreaks in other countries, have shown that dengue cases tend to be clustered either within the same household or in nearby neighborhoods. 3, 4, 7, 9 The aim of this study was to characterize in detail the spatio-temporal patterns of the spread of dengue cases in this two-city area during Taiwan's 2001-2003 dengue epidemic. We showed that dengue case clustering occurred in a contiguous pattern at the community level and in a relocated pattern after the virus had rapidly dispersed on large geographical areas.

18987990
ABSTRACT : The Λ baryons with a single heavy flavor which transfer the quark polarization, have been studied both theoretically and experimentally. The Ξ's with two heavy constituents are well treated in quark-diquark model. In this work we study the production of triply heavy baryons in the perturbative QCD regime and calculate the fragmentation functions for Ω ccc and Ω bbb in the c and b quark fragmentation, respectively. We then obtain the total fragmentation probability and the average fragmentation parameter for each case.
Introduction : The quark model of hadrons has proved to be successful in describing hadrons and their properties. In the heavy quark sector it predicts hadrons having c, b and t quarks as constituents. However, the discovery of the top quark [1] and the determination of its lifetime [2] made it clear that it cannot participate in strong interactions and therefore only the c and b flavors are left to take part in the hadron production interplay.
Introduction : Baryon states with heavy flavor fall into three categories. States containing one heavy flavor such as Λ c and Λ b are interesting states due to the fact that they carry the original heavy flavor polarization. They are presently being studied experimentally [5] . The second category involves baryons with two heavy flavor like the states Ξ cc , Ξ bb and Ξ bc [6] . They are treated within the approximate quark-diquark model [7] . The model treats the production of the so called diquark perturbatively similar to the states such as B c . Then, it can be proved that the formation of a baryon out of the diquark is almost the same as the fragmentation of an antiquark into a meson. In this way one obtains the fragmentation functions, the total production probabilities and other relevant parameters which specify their properties. In the third category, we have baryons with three heavy constituents. If we follow the scheme used in the case of heavy mesons and assume that their fragmentation functions are calculable in the perturbative regime, then we can calculate Feynman diagrams like the one in figure 1 to obtain the fragmentation functions. There are eight such diagrams in the lowest order contributing triply heavy baryons, i.e. Ω ccc , Ω ccb , Ω cbb and Ω bbb production [8] .
Introduction : In this paper our aim is to calculate the fragmentation of the Ω ccc and Ω bbb baryons in the lowest order perturbative regime and obtain their fragmentation functions in an exact analytical form.

18989350
ABSTRACT : Construction of genetic linkage map is essential for genetic and genomic studies. Recent advances in sequencing and genotyping technologies made it possible to generate high-density and high-resolution genetic linkage maps, especially for the organisms lacking extensive genomic resources. In the present work, we constructed a high-density and high-resolution genetic map for channel catfish with three large resource families genotyped using the catfish 250K single-nucleotide polymorphism (SNP) array. A total of 54,342 SNPs were placed on the linkage map, which to our knowledge had the highest marker density among aquaculture species. The estimated genetic size was 3,505.4 cM with a resolution of 0.22 cM for sex-averaged genetic map. The sex-specific linkage maps spanned a total of 4,495.1 cM in females and 2,593.7 cM in males, presenting a ratio of 1.7 : 1 between female and male in recombination fraction. After integration with the previously established physical map, over 87% of physical map contigs were anchored to the linkage groups that covered a physical length of 867 Mb, accounting for ∼90% of the catfish genome. The integrated map provides a valuable tool for validating and improving the catfish whole-genome assembly and facilitates fine-scale QTL mapping and positional cloning of genes responsible for economically important traits.
Introduction : Genetic linkage maps are essential for the understanding of genomic levels of organization of inheritance of traits. 1 Construction of highdensity and high-resolution genetic maps is a key step for fine mapping of quantitative trait loci (QTL) and marker-assisted selection. In addition, genetic linkage maps are valuable resources for the generation of chromosome-level assembly of whole-genome sequences and for comparative genome analysis. 2 In most of the recent whole-genome sequencing cases, whole-genome sequences are generated using nextgeneration sequencing (NGS). Short sequence reads are assembled into contigs. Such contigs are generally still relatively short although they vary in sizes from several kilobases to tens of kilobases. Increases in genome sequencing coverage and sequencing libraries can increase the quality of the assembly, allowing the sizes of contigs to be increased. However, NGS methods alone cannot provide the resources to assemble complex genomes at the chromosomal level. Genome sequence assemblies at this level require the assembly of tens of thousands to hundreds of thousands of contigs. Highly segmented genome assemblies prohibit efficient genome analysis. Therefore, various genome resources have been created to reduce the segmentation of the genome assemblies. One of these resources is the large-insert-based physical maps. Historically, several types of large insert libraries have been used. These include yeast artificial chromosomes (YACs), 3 bacterial artificial chromosomes (BACs), 4 and cosmid-based libraries. 5 YACs have the largest capacity for cloning the large inserts, but they are relatively unstable; therefore, their use in genome studies has been limited. Cosmid libraries have the smallest capacity for cloning the large inserts; therefore, their use in large insert libraries has also been limited. BACs are the most popular large insert libraries as they are stable and can hold inserts of up to 200 kb. BAC-based physical maps can organize the entire genome into restriction fingerprint-based contigs. Such contigs are similar to the whole-genome sequencing contigs, but they are constructed using overlapping restriction enzyme fingerprints rather than overlapping sequences themselves for the whole-genome sequence assemblies. By analysis of restriction fingerprints of overlapping genomic clones of BAC inserts, the whole genome can be organized into a limited number of contigs, most often in thousands. For instance, the catfish physical maps had 3,307 contigs and 1,891 contigs. 6, 7 The integrated catfish physical map included over 2,500 contigs (unpublished). Integration of physical maps and whole-genome sequence contigs allows the relationship to be established between the sequence-based contigs and the restriction fingerprint-based contigs, thereby reducing the levels of segmentation of the genome. One of the major applications of genetic linkage map is to integrate physical maps and wholegenome assemblies. Integration of genetic map with physical map is useful for understanding genomes from different dimensions and is essential for comparative genome analysis, 8 fine-scale QTL mapping and positional cloning of genes responsible for performance and production traits. 9, 10 In aquaculture fish species, genetic maps have been constructed in a few species, such as Asian seabass, 11 Atlantic salmon, 12 half-smooth tongue sole, 13 rainbow trout, 14 common carp, 15 and catfish. 2, 9, 16, 17 These maps harbour several hundred to a couple of thousands of markers, with which QTL for agriculturally important performance and production traits can only be mapped in large genomic regions. Integrated maps have been developed in several aquaculture fish species using low-density genetic maps. In Atlantic salmon, in addition to 579 BAC contigs that were integrated into the linkage map using microsatellite markers, identification and mapping of new BAC-anchored single-nucleotide polymorphism (SNP) markers from BAC-end sequences placed 73 additional BAC contigs to Atlantic salmon linkage groups. 18 The second generation of rainbow trout integrated map anchored up to 265 contigs to the genetic map, covering ∼11% of the genome. 10 In common carp, a total of 463 physical map contigs and 88 single BACs were integrated into the genetic linkage map, which covered 30% of the common carp genome. 15 In catfish, 2,030 BAC-end sequence (BES)-derived microsatellites from 1,481 physical map contigs were used for linkage map construction, which anchored 44.8% of the catfish BAC physical map contigs, covering ∼52.8% of the genome. 9 Apparently, the level of integration is dependent on the density and resolution of the genetic maps. One objective of this study was to construct a high-density and high-resolution genetic linkage map by using a large number of molecular markers, covering the entire genome and a large resource panel for linkage mapping analysis. In addition to this primary goal, a secondary goal was to increase integration of the linkage and physical maps, using markers derived from BECs for genetic linkage analysis. Our previous genetic linkage analysis used mostly microsatellite markers. In spite of their high polymorphism, and low cost of single marker genotyping, an analysis of tens of thousands of microsatellites with a large number of mapping fish is extremely labourious and costprohibitive. SNPs overcome these difficulties by providing high efficiency and low-cost, large-scale genotyping. SNPs have become markers of choice because of their abundance, even genomic distribution, and easy adaptation to automation. 19 Recent advances in NGS have allowed rapid discovery of genome-wide SNPs in any organism in a cost-effective manner. 20 With the availability of large numbers of SNPs, high-density SNP array platform can be developed for high-throughput and efficient genotyping. Alternatively, an NGS-based genotyping-by-sequencing (GBS) [19] [20] [21] is also highly efficient. SNP arrays and GBS have both been used to genotype large-scale SNPs for genetic mapping and association analyses. 12, [22] [23] [24] [25] [26] [27] [28] Compared with GBS, SNP arrays are more costeffective. In addition, they provide a greater level of genome coverage. For instance, in most cases, the total number of commonly analysed SNPs is limited to several thousands while millions of SNPs can be analysed by using very high-density SNP arrays. In addition, GBS of large resource panels for high-resolution maps is cost-prohibitive. Channel catfish, Ictalurus punctatus, is the primary aquaculture species in the United States. Since the initiation of genome research over two decades ago, several genetic maps have been constructed with different types of molecular markers and various resource families. 2, 9, 16, 17 Up to date, the highest density map was developed to contain 2,030 microsatellites and 100 SNPs. 9 Although this genetic map has been very useful for genetic and genomic analysis, 29, 30 marker density in this map was still fairly low and only facilitated integration to the physical map for ∼52% of catfish genome. Recently, following efforts to expand catfish genomic resources, we have identified millions of SNPs in channel catfish 31, 32 and developed high-density SNP arrays, 33 which provided the opportunity to develop a high-density and high-resolution SNP-based genetic map. Here we report the construction of genetic linkage map with over 50,000 SNPs in channel catfish, which is, to the best of our knowledge, the highest density genetic map for any aquaculture species. Along with the high density of markers, the utilization of BAC-associated SNPs also allowed significant increase of the integration of the genetic linkage map with the BAC-based physical map in catfish, allowing over 90% of the catfish genome physical map contigs to be mapped to linkage groups. This genetic map should serve as a valuable framework for validating the reference whole-genome sequences, extensive comparative and functional genomic studies, and fine-scale QTL mapping and association studies in catfish.

18989842
ABSTRACT : Background. Since Aerococcus sanguinicola was designated as a species in 2001, only a few cases of bacteremia have been reported. The aim with this study was to describe the clinical presentation of A sanguinicola bacteremia and to determine the antibiotic susceptibility and the capacity of the bacteria to form biofilm and to induce platelet aggregation.
ABSTRACT : Methods. Isolates of A sanguinicola from blood cultures were retrospectively identified from 2 clinical microbiology laboratories for 2006 to 2012. Species identity was confirmed through sequencing of the 16S rRNA gene. The medical charts of patients were reviewed. The minimum inhibitory concentration (MIC) for relevant antibiotics was determined. Biofilm formation was measured as the amount of crystal violet absorbed. Platelet aggregation was determined by aggregometry.
ABSTRACT : Results. Eleven cases of A sanguinicola bacteremia were identified. All patients were male and the median age was 82 years (range 67-93). Nine patients fulfilled criteria for severe sepsis, and 2 patients died at hospital. Two patients were diagnosed with infective endocarditis. Most patients had underlying urinary tract diseases or an indwelling urinary tract catheter. Five patients suffered from dementia. None of the patients was treated with immunosuppressive medications. The MIC values of the isolates were in line with previous reports, with low MICs for penicillin, cefotaxime, and vancomycin. All 11 isolates produced biofilms but not all could induce platelet aggregation.
ABSTRACT : Conclusions. A sanguinicola can cause severe infections in elderly men with urinary tract abnormalities and the bacteria possess potential virulence mechanisms.

199661892
ABSTRACT : Background: Trust is a critical component of competency committees given their high-stakes decisions. Research from outside of medicine on group trust has not focused on trust in group decisions, and "group trust" has not been clearly defined. The purpose was twofold: to examine the definition of trust in the context of group decisions and to explore what factors may influence trust from the perspective of those who rely on competency committees through a proposed group trust model. Methods: The authors conducted a literature search of four online databases, seeking articles published on trust in group settings. Reviewers extracted, coded, and analyzed key data including definitions of trust and factors pertaining to group trust. Results: The authors selected 42 articles for full text review. Although reviewers found multiple general definitions of trust, they were unable to find a clear definition of group trust and propose the following: a group-directed willingness to accept vulnerability to actions of the members based on the expectation that members will perform a particular action important to the group, encompassing social exchange, collective perceptions, and interpersonal trust. Additionally, the authors propose a model encompassing individual level factors (trustor and trustee), interpersonal interactions, group level factors (structure and processes), and environmental factors.
Conclusions : This scoping review should allow educators and leaders to better appreciate what factors may contribute to trust in group settings (e.g. competency committees). As programs establish and refine policies, procedures, and membership of these committees, our proposed definition and model may help to improve the translation of evaluation data and individual opinions into competency decisions. Educators who understand these factors may also help create a collective environment of trust not only on a competency committee, but potentially within their organization as well. Trainees, patients, and other stakeholders trust competency committees to make carefully weighted decisions, and it is important to determine how these committees can demonstrate that this trust is not misplaced.

199662866
ABSTRACT : Abstract: Background and objectives: The Studies have suggested hypercholesterolemia is a risk factor for cerebrovascular disease. However, few of the studies with a small number of patients had tested the effect of hypercholesterolemia on the outcomes and complications among acute ischemic stroke (AIS) patients. We hypothesized that lipid disorders (LDs), though risk factors for AIS, were associated with better outcomes and fewer post-stroke complications. Materials and Method: We performed a retrospective analysis of the Nationwide Inpatient Sample (years 2003-2014) in adult hospitalizations for AIS to determine the outcomes and complications associated with LDs, using ICD-9-CM codes. In 2014, we also aimed to estimate adjusted odds of AIS in patients with LDs compared to patients without LDs. The multivariable survey logistic regression models, weighted to account for sampling strategy, were fitted to evaluate relationship of LDs with AIS among 2014 hospitalizations, and outcomes and complications amongst AIS patients from 2003 -2014 . Results and Conclusions: In 2014, there were 28,212,820 (2.02% AIS and 5.50% LDs) hospitalizations. LDs patients had higher prevalence and odds of having AIS compared with non-LDs. Between 2003-2014, of the total 4,224,924 AIS hospitalizations, 451,645 (10.69%) had LDs. Patients with LDs had lower percentages and odds of mortality, risk of death, major/extreme disability, discharge to nursing facility, and complications including epilepsy, stroke-associated pneumonia, GI-bleeding and hemorrhagic-transformation compared to non-LDs. Although LDs are risk factors for AIS, concurrent LDs in AIS is not only associated with lower mortality and disability but also lower post-stroke complications and higher chance of discharge to home.
Introduction : Hypercholesterolemia is a well-documented risk factor for cardiovascular morbidity and mortality [1] [2] [3] [4] . However, the relationship between ischemic stroke and cholesterol is complex and appears to contain several paradoxes [5] [6] [7] [8] [9] [10] [11] [12] . Many large-scale studies on stroke and cholesterol have not differentiated between ischemic and hemorrhagic stroke, nor among various subtypes of ischemic stroke [6, [8] [9] [10] . It has been demonstrated that cholesterol may increase the risk of only certain types of stroke [13] [14] [15] , and low cholesterol levels predispose to hemorrhagic stroke [10, 16] , thus weakening an association between cholesterol and all stroke. Regardless of increased cardiovascular disease risk with high cholesterol levels and decreased stroke risk with statin use, a higher cholesterol value has been associated with a better stroke outcome in several studies [17] [18] [19] [20] . Further research has shown "reverse epidemiology" between cholesterol levels at admission, statin treatment and stroke morbidity. Earlier studies have shown a positive association between elevated admission cholesterol at ischemic stroke onset and improved short-term functional outcome [21] and 10-year survival [22] .
Introduction : These paradoxical observations may be related to differential associations with different stroke subtypes. Several recent studies reported that cholesterol was lowest in cardioembolic strokes [15, 23, 24] . The largest prospective cohort of 4128 adults aged >70 years reported that high or normal/borderline total cholesterol values were associated with good survival compared to low values [25] .
Introduction : We aimed to estimate odds of having AIS with LDs, and whether or not LDs in AIS patients were associated with better outcomes.
Conclusions : LDs have been shown to have an increased prevalence in patients with AIS hospitalizations as compared to those without LDs. However, we have also confirmed that LDs have paradoxically been shown to improve post-AIS outcomes and reduce post-AIS complications. This complicated relationship between stroke and LDs requires further work and research to determine the reasoning as to the associated benefit between higher lipids and better outcomes, and whether the theory in that LDs have a stronger association in causing small vessel and thus less severe strokes with better prognoses can be confirmed. Clinical trials should be undertaken to further determine the relationship of statin uses and outcomes to refine treatment strategies before and after stroke onset, and further work should be initiated to determine the association between LDs and AIS subtypes such as small-vessel strokes, AIS-subtype outcomes, and AIS-subtype complications.
Conclusions : Supplementary Materials: The following are available online at http://www.mdpi.com/1010-660X/55/8/475/s1. Table S1 : ICD-9-CM codes used in this analysis, Table S2 

199665186
Conclusion: : Corporations ought to prevent some absolute poverty (Singer, p.230) .
Conclusion: : The second premise is grounded on the virtue of compassion. Absolute poverty is bad and any decent human being or corporation cannot tolerate such situations. The motivations behind such corporate actions are based on the virtue of compassion regardless of profitability. It is simply the right thing to do. However, compassion is not generally part of the strategic mission of organisations, profit is. So, we now ask the question, "how can organisations assist the poor of the world profitably?" In the next section we will consider some evidence from the work of Prahalad and Hamond (2002) that indicates that corporations can assist the poor of the world profitably.

199665542
ABSTRACT : Abstract. We prove existence and uniform bounds for critical static Klein-GordonMaxwell-Proca systems in the case of 4-dimensional closed Riemannian manifolds.
ABSTRACT : Static Klein-Gordon-Maxwell-Proca systems are massive versions of the electrostatic Klein-Gordon-Maxwell systems. The vector field in these systems inherits a mass and is governed by the Proca action which generalizes that of Maxwell. Klein-GordonMaxwell systems are intended to provide a dualistic model for the description of the interaction between a charged relativistic matter scalar field and the electromagnetic field that it generates. The electromagnetic field is both generated by and drives the particle field. In the electrostatic form of the Klein-Gordon-Maxwell systems, looking for standing waves ue iot , the matter field is characterized by the property that u, together with a gauge potential v, solve the electrostatic Klein-Gordon-Maxwell systems (0.3) with m 1 ¼ 0. In the case of a closed manifold we discuss here the two equations in (0.3) are independent one of another when m 1 ¼ 0 and the system reduces to the sole Schrö dinger equation. The Proca formalism, for m 1 > 0, leads to a deeper phenomenon and is more appropriate to the closed case. The particle in this model interacts via the minimum coupling rule
ABSTRACT : with an external massive vector field ðj; AÞ which is governed by the Maxwell-Proca Lagrangian. The Proca action is a gauge-fixed version of the Stueckelberg action in the Higgs mechanism (see Goldhaber and Nieto [26], and Ruegg and Ruiz-Altaba [43] ). In the Proca formalism, developped under the influence of de Broglie, the photon inherits a nonzero mass. This issue is of considerable importance and intensively studied in modern physics (see for instance Adelberger, Dvali Gillies and Tu [37] and the references in these papers). When n ¼ 3, the KGMP equations consist in the nonlinear Klein-Gordon matter equation, the charge continuity equation and the massive modified Maxwell equations in SI units, which are hereafter explicitly written down:
Introduction : Static Klein-Gordon-Maxwell-Proca systems are massive versions of the electrostatic Klein-Gordon-Maxwell systems. The vector field in these systems inherits a mass and is governed by the Proca action which generalizes that of Maxwell. Klein-GordonMaxwell systems are intended to provide a dualistic model for the description of the interaction between a charged relativistic matter scalar field and the electromagnetic field that it generates. The electromagnetic field is both generated by and drives the particle field. In the electrostatic form of the Klein-Gordon-Maxwell systems, looking for standing waves ue iot , the matter field is characterized by the property that u, together with a gauge potential v, solve the electrostatic Klein-Gordon-Maxwell systems (0.3) with m 1 ¼ 0. In the case of a closed manifold we discuss here the two equations in (0.3) are independent one of another when m 1 ¼ 0 and the system reduces to the sole Schrö dinger equation. The Proca formalism, for m 1 > 0, leads to a deeper phenomenon and is more appropriate to the closed case. The particle in this model interacts via the minimum coupling rule q t ! q t þ iqj and ' ! ' À iqA ð0:1Þ
Introduction : with an external massive vector field ðj; AÞ which is governed by the Maxwell-Proca Lagrangian. The Proca action is a gauge-fixed version of the Stueckelberg action in the Higgs mechanism (see Goldhaber and Nieto [26] , and Ruegg and Ruiz-Altaba [43] ). In the Proca formalism, developped under the influence of de Broglie, the photon inherits a nonzero mass. This issue is of considerable importance and intensively studied in modern physics (see for instance Adelberger, Dvali and Gruzinov [1] , Byrne [13] , Goldhaber and Nieto [25] , [26] , Luo and Tu [38] , Luo, Gillies and Tu [37] and the references in these papers). When n ¼ 3, the KGMP equations consist in the nonlinear Klein-Gordon matter equation, the charge continuity equation and the massive modified Maxwell equations in SI units, which are hereafter explicitly written down:
Introduction : ' Â E þ qH qt ¼ 0 and ':H ¼ 0:

199665735
ABSTRACT : Type II arabinogalactan (AG) is a polysaccharide found in Maytenus ilicifolia (Celastraceae), a plant reputed as gastroprotective. Oral and intraperitoneal administration of the AG protected rats from gastric ulcers induced by ethanol. No alteration of mechanisms related to acid gastric secretion and gastrointestinal motility were observed. In vitro, the AG showed a potent scavenging activity against the radical of DPPH (2,2-diphenyl-1-picrylhydrazyl) with an IC 50 value of 9.3 µM. However, the mechanism of the gastroprotective action remains to be identifi ed.
Introduction : Maytenus ilicifolia, a plant popularly known in Brazil as "espinheira santa", is extensively used to treat stomach disorders (Cruz, 1982; Macaubas et al., 1988) . The gastroprotective properties of various extracts of M. ilicifolia have been shown in experimental ulcer models using rodents (Baggio et al., 2007; Ferreira et al., 2004; Jorge et al., 2004; Souza-Formigoni et al., 1991; Tabach and Oliveira, 2003) . Our laboratory identifi ed inhibition of gastric acid secretion and modulation of nitric oxide in the mechanism of activity of a fl avonoid-rich extract containing galactitol (25%), epicatechin (3.1%), and catechin (2%) as the major components (Baggio et al., 2007) .
Introduction : The polysaccharide arabinogalactan is found as an essential structural polymer of the cell wall of plants and as a major component of many gums and exudates (Delgobo et al., 1998; Fincher et al., 1983) . Several plants have been reported to contain polysaccharides of this type, and its presence has been correlated with a variety of biological activities such as antiviral, antitumour, immune-stimulating, anti-infl ammatory, anticoagulant, hypoglycemic, and antiulcer (Capek et al., 2003; Nergard et al., 2005; Srivastava and Kulshreshtha, 1989; Yamada, 1994) . Furthermore, our laboratory showed a potent antiulcer activity of this compound in the ethanol-induced gastric injury model (Cipriani et al., 2006) .
Introduction : In this study, we screened the effects of the arabinogalactan against the experimental models of gastric hypersecretion, ulcer, and gastrointestinal motility in which Maytenus ilicifolia, from which this compound has been isolated, showed potent gastroprotective activity. Isolation and identifi cation of the AG has been described in detail elsewhere (Cipriani et al., 2006) . The content of this purifi ed AG in M. ilicifolia was 0.38% w/w and its average molar mass (M) was 11400 g/mol.

199668877
Introduction : Open clusters (OCs) are valuable objects for revealing stellar evolution and the structure, chemical and dynamical evolution of the Galactic disk. Be 8 has been studied by Hasegawa et al. (2004) [1] from CCD BVI photometry and by Bukowiecki et al. (2011) [2] from 2MASS JHK s photometry. Be 8 is located close to a portion of the Perseus spiral arm in the second quadrant of the Galaxy (Figure 1 ), according to its equatorial and Galactic coordinates (WEBDA) [3] (rows 1-4 of Table 1 ). The main aim of this paper is to present astrophysical parameters such as reddening, distance and age of Be 8 from four colour indices, (B − V ) , (V − I) , (R − I) and (G BP -G RP ) obtained from deep CCD U BV RI and Gaia photometries. This kind of data is also valuable for classifying early-type stars, Blue Stragglers (BS) and Red Giant/Red Clump (RG/RC) candidates in the colour magnitude diagrams (CMDs), and thus probable candidates are proposed for future spectroscopic observations. We used Gaia DR2 astrometric data (proper motion components and parallaxes) [4, 5] and Gaia DR2 photometry (G -G BP G RP ) for determining the probable members of Be 8. With the Gaia DR2 astrometric data, a membership method was done in the literature [6] [7] [8] . The membership determinations of previous works have been based on the proper motions of Roeser et al. (2010) [9] in combination with the 2MASS JHK s photometry of Skrutskie et al. (2006) [10] . Cantat-Gaudin et al. (2018) [11] state that the proper motion uncertainties of UCAC4 fall in the range of 1-10 mas yr −1 [9, 12] . According to Lindegren et al. (2018) [4] and Brown et al.
Introduction : (2018) [5] , the mean parallax errors of Gaia DR2 catalogue fall in the range 0.02 -0.04 mas for G < 15 and 0.1 The sun is at R = 8.2 ± 0.1 kpc [34] . The image is adapted from the image 1 credit by Robert Hurt, IPAC; Bill Saxton, NRAO/AUI/NSF. This paper is organised as follows: Section 2 describes the observation and reduction techniques. Its dimensions are given in Section 3. Section 4 is devoted for the classification of cluster members. Section 5 describes the derivation of the astrophysical parameters. The classification of BS and RG/RC candidates and its morphological age determination are presented in Section 6. Section 7 focuses into its kinematics and orbital parameters. Discussions and Conclusions are given in Section 8.
Discussions and Conclusions : Two BS candidates reside in core radius of 1.8 of Be 8 ( Figure 10 ). BS stars potentially locate in the inner regions of stellar clusters [43] . According to Ferraro (2016) [44] , their formation ways are explained: mass transfer in binary systems [45] due to the merging of the two stars and stellar collisions [46] .
Discussions and Conclusions : It is surprising to find Be 8 with [M/H] = −0.27 (close to solar metallicity) at such large galactic radius (R = 10.57 kpc). However, the orbits in Fig.11 (a) and (b) show that the cluster passed a part of its time at galactocentric radius, R = 6 − 7 kpc, and then possibly it was born at that radius, which would explain the metallicity. According to Figure .57 kpc reside in a region of R > 9 kpc (co-rotation gap at 9 kpc). In the sense it may have been originating from different galactic radius or different star formation region [47] .

199668887
ABSTRACT : Pervasive computing systems employ distributed and embedded devices in order to raise, communicate, and process data in an anytime-anywhere fashion. Certainly, its most prominent device is the smartphone due to its wide proliferation, growing computation power, and wireless networking capabilities. In this context, we revisit the implementation of digitalized word-of-mouth that suggests exchanging item preferences between smartphones ofine and directly in immediate proximity. Collaboratively and decentrally collecting data in this way has two benefts. First, it allows to attach for instance location-sensitive context information in order to enrich collected item preferences. Second, model building does not require network connectivity. Despite the benefts, the approach naturally raises data privacy and data scarcity issues. In order to address both, we propose Propagate and Filter, a method that translates the traditional approach of fnding similar peers and exchanging item preferences among each other from the feld of decentralized to that of pervasive recommender systems. Additionally, we present preliminary results on a prototype mobile application that implements the proposed device-to-device information exchange. Average ad-hoc connection delays of 25.9 seconds and reliable connection success rates within 6 meters underpin the approach's technical feasibility.
INTRODUCTION : Nowadays, the possibility to collect, store, and process very large amounts of data in combination with powerful data transformation and analysis techniques have raised privacy concerns such as when in early 2018, Cambridge Analytica had been granted access to millions of Facebook user profles for political campaigning without users being aware of it. Statutory counter measures to curb data misuse have in particular been undertaken by the European Union in May 2018 by adopting the General Data Protection Regulation (GDPR). The GDPR imposes strict rules on data processing, ownership, information obligation (including raising consent), transparency, and collection of user-related data. Since data protection laws penalize misuse of personal data reactively, they do not actively prohibit misuse on a technical level. In the literature, three major recommender system architectures have been proposed to address privacy issues arising from collecting large authoritative data pools.
INTRODUCTION : Federated learning produces personalized recommendation models by communicating model building between a central server and mutually disconnected peers holding personal information [10, 22] . Thus, federated learning enacts model building centrally on data that is distributed across personal data owners.
INTRODUCTION : In contrast, decentralized recommender systems feature direct interactions between distributed peers without a central server. They are commonly built on top of fle-sharing peer-to-peer networks [3, 21, 28, 31] that use gossip mechanisms [19] in order to establish a logic overlay network for fast network search and network resilience in view of peers joining or churning the network. In short, dissimilar peers are dropped from and similar peers are added to views (lists of visible peers in the network) iteratively. In so doing, the network establishes homogenous interest groups, which share recommendations explicitly among each other.
INTRODUCTION : Pervasive (or ubiquitous) recommender systems [23, 27] are systems that often revolve around location-aware recommendations of for instance items in a nearby shop, restaurants, or events in proximity. The location-based approach naturally circumvents transmitting personal data to a central remote authority for recommendation by instead requesting closeby profle as well as context information.
INTRODUCTION : Pervasive recommender systems are naturally confronted with limited profle data, for usually only a small subpopulation is available in proximity. Fortunately, there are indications that integrating context data into the recommendation process allows to outweigh profle data scarcity [1] .
INTRODUCTION : Among these three approaches we believe that pervasive recommender systems yield the highest potential for data privacy for two reasons. First, the availability of context data as well as mobile compute power on smartphones increases rapidly rendering more and more complex recommendation algorithms feasible on smartphones holding personal data. Second, the model building process happens on-device and does not require connectivity to the network. Consequently, peers become invisible in the network when they do not interact with other peers thus adding privacy on the level of model building. In spite of their potential for recommendation, we see two main burdens. First, pervasive recommender systems are susceptible to data privacy issues since the recommendation mechanism usually builds on the exchange of raw profle data with nearby peers. Second, local scarcity of profle data renders item recommendations on location-independent items taxing. We believe that introducing gossip-based mechanisms and data sampling strategies to the feld of pervasive recommender systems can alleviate both issues. We present the following preliminary results:
INTRODUCTION : • The design of Propagate and Filter, a gossip-based method that addresses the problem of data privacy and data scarcity in pervasive recommender systems. • An implementation of the propagation part in the form of an Android mobile application utilizing Google's Nearby Connections API and its evaluation.
Conclusion : The experimental fndings indicate that the Propagate and Filter's propagation step works reliably with larger amounts of ratings, in multi-device scenarios, and in areas without internet connectivity such as underground trains. Yet, the implementation has certain limitations. First of all, information can only be disseminated reliably within a radius of 6 meters, which on the one hand strengthens privacy, and on the other limits the number of potential peers to share information with. Furthermore, the average initial connection delay of 25.9 seconds is considerable and does not allow to share information between for instance passing pedestrians, yet includes scenarios such as waiting at the trafc lights, sitting next to each other in a café or restaurant, or taking public transportation. Last but not least, battery drainage is relatively high at at least 5% per hour, which thus prohibits continuous advertising and discovery. Apart from ever improving transmission standards and hardware, we believe that it is possible to limit activity and inactivity of the sharing process efciently on the logical level leveraging sensor information on smartphones.

199668943
ABSTRACT : Estimating 3D human pose from monocular images demands large amounts of 3D pose and in-the-wild 2D pose annotated datasets which are costly and require sophisticated systems to acquire. In this regard, we propose a metric learning based approach to jointly learn a rich embedding and 3D pose regression from the embedding using multi-view synchronised videos of human motions and very limited 3D pose annotations. The inclusion of metric learning to the baseline pose estimation framework improves the performance by 21% when 3D supervision is limited. In addition, we make use of a person-identity based adversarial loss as additional weak supervision to outperform state-of-the-art whilst using a much smaller network. Lastly, but importantly, we demonstrate the advantages of the learned embedding and establish view-invariant pose retrieval benchmarks on two popular, publicly available multi-view human pose datasets, Human 3.6M and MPI-INF-3DHP, to facilitate future research.
Introduction : Over the years, the performance of monocular 3D Human pose estimation has improved significantly by leveraging complex CNN models. [37, 18, 35] . However, these methods rely heavily on large-scale 3D pose annotated training data, which is difficult and costly to obtain, especially under in-the-wild setting for articulated poses. The two most popular 3D ground-truth annotated datasets, Human3.6M [7] and MPI-INF-3DHP [13] , have 3.6M and 1.3M annotated poses, respectively. Unfortunately, these datasets are biased towards typical indoor setting like uniform background and illumination and lack real-world environment variations [37] . However, it is relatively easier to obtain time-synchronized video streams of human poses from multiple different viewpoints. Therefore, techniques that can employ un-annotated multi-view human-pose data to learn the 3D structure and geometry could prove benefi- * -equal contribution cial for human-pose estimation with small amount of annotated data. To this end, we propose a metric learning based approach to jointly learn a 3D human pose embedding and pose regression using the embedding from synchronized videos of human motion with very limited pose annotations. Our approach doesn't require camera extrinsics or prior background extraction. Therefore, it can be easily extended to train with further un-annotated in-the-wild data. We seek motivation from a recent work in [22] , where image generation in different views via a geometry-aware latent space is used to improve pose-estimation under limited 3D supervision. This method, however, requires camera extrinsics and static background during training, which limits its application to indoor datasets. Our proposed approach is free from these constraints and, therefore, can potentially be used for in-the-wild setting. Moreover, we also show superior performance with faster inference.
Introduction : We utilize our framework to improve pose estimation accuracy under limited 3D supervision. We show that weak supervision in learning the embedding ensures that our model's performance degrades gracefully when 3D supervision is progressively reduced. Additionally, we eliminate the subject-specific appearance information from our latent embedding with the help of an adversarial mechanism which leads to further improvements and outperforms the current state-of-the-art [22] . Lastly, we use smaller network architecture that affords 3X faster inference time. A simplified overview of our approach and its utilization is shown in Fig. 1 . The formulation of our loss function leads to a view-invariant embedding, and in Sec. 5, we demonstrate the richness of our learned embedding to capture human pose structure invariant to viewpoint by way of carefully designed pose retrieval experiments and establish novel benchmarks on Human3.6M and MPI-INF-3DHP to facilitate future research. A summary of our contributions is,
Conclusion And Future Work : In this paper, we demonstrated a metric learning approach to capture 3D human structure and its effectiveness in both pose estimation and pose retrieval tasks. More specifically, the information from our embedding reduces the need for 3D supervision when regressing human pose, enabling our method to outperform contemporary weaklysupervised approaches even while using a smaller network. Further, we provided strong benchmarks for view-invariant pose retrieval on publicly available datasets.
Conclusion And Future Work : In future, we plan to use multi-view synchronised videos captured in-the-wild and synthetically generated, consisting of images taken from a large no. of viewpoints with diverse appearances, to improve the quality of the embedding and in the wild generalisation. Also, we plan to apply our approach to recognize actions from unseen viewpoints. Figure 8 : Shows top 5 pose retrievals using our embedding on Human3.6M [7] and MPI-INF-3DHP [13] datasets. The top row marked in Red is the query image, the next five rows shows retrieved images taken from different viewpoint having similar poses.

199669503
ABSTRACT : It has been shown that the two texture zero neutrino mass matrices in the charged lepton basis predict non-zero 1-3 mixing and are necessarily CP violating with one possible exception in class C for maximal mixing. * dev5703@yahoo.com †

2870075
ABSTRACT : System calibration is fundamental to the overall accuracy of the ultrasonic temperature measurement, and it is basically involved in accurately measuring the path length and the system latency of the ultrasonic system. This paper proposes a method of high accuracy system calibration. By estimating the time delay between the transmitted signal and the received signal at several different temperatures, the calibration equations are constructed, and the calibrated results are determined with the use of the least squares algorithm. The formulas are deduced for calculating the calibration uncertainties, and the possible influential factors are analyzed. The experimental results in distilled water show that the calibrated path length and system latency can achieve uncertainties of 0.058 mm and 0.038 μs, respectively, and the temperature accuracy is significantly improved by using the calibrated results. The temperature error remains within ±0.04˚C consistently, and the percentage error is less than 0.15%.
Introduction : Ultrasonic temperature measurement, as a non-intrusive technique, plays a crucial role in plenty of industrial processes due to its strong environmental adaptability [1, 2] , high reliability [3] , low cost [4] and wide measuring range [5] . The basic principle is that the ultrasound velocity in any medium is generally a function of temperature. In most liquids, the function is linear; in a gaseous environment, the velocity is directly proportional to the square root of the temperature; and in solid mediums, the velocity generally decreases with the increasing of the temperature [6] . Thus, if the ultrasound velocity is measured, the temperature can be determined. In our previous study [7] , we proposed a promising method for highly accurate ultrasonic temperature measurement. Two transducers, mounted face to face, act as the transmitter and the receiver of the ultrasound signals, respectively, and the distance between them is fixed. By estimating the time delay (TD) between the transmitted signal and the received signal, the ultrasound velocity is given, and the temperature can be determined.
Introduction : However, the estimated TD contains not only the desired time of flight of sound, but also a system latency which includes the time taken by the transmitter to produce the sound, the time taken by the receiver to produce the electrical signal and the time taken by the conditioning circuit [8] . The system latency is an intrinsic hardware constant that need to be estimated in advance. Moreover, the path length of sound is generally not the transducers' geometrical distance, but the travelling sound path length which serves as the acoustic center-to-center distance [5] . The exact path length is not available from direct distance measurement. Therefore, to accurately estimate the path length and the system latency, calibration work of the ultrasonic system should be performed, which is the basis of high accuracy ultrasonic temperature measurement.
Introduction : The calibration methods of path length usually depend on measurement of the acoustic centers of the transducers. Based on the definition that the acoustic center is the position of the point from which the sound pressure varies inversely as distance [9, 10] , most results presented in the literature have been determined from deviations of the amplitude of the sound pressure [11] . Cox attempted to measure the acoustic centers of various transducers using a dismantled lathe bed for positioning the scanning microphone [12] . The comparisons of acoustic centers among several European laboratories were summarized by Rasmussen and Olsen [13] . Juhl [14] measured the acoustic centers by using the boundary value technique and assuming a parabolic movement of the diaphragms. Barrera-Figueroa et al. [15] presented an experimental procedure to determine the position of the acoustic centers of the microphones. Shaw and Hodnett [16] described some typical issues related to calibration and measurement of therapeutic medical ultrasonic equipment. A theoretical model for the transfer characteristics of a hydrophone has been developed, which can help to calculate the system latency [17] . However, the methods mentioned above mainly focused on calibrating the characteristics of the transducers, rather than the ultrasonic system directly. The calibration procedure is very complex, and it is difficult to achieve high calibration performance. This paper proposes a promising calibration method to determine the path length and the system latency of an ultrasonic system for temperature measurement. Calibration equations on the path length and the system latency are presented by estimating the TD between the transmitted signal and received signal at several different temperatures, and the calibrated results are determined simultaneously based on the least squares algorithm. Distilled water is employed as the medium to perform the experiment keeping in a stable temperature environment with an accuracy of 0.01°C. To achieve highly accurate TD estimation, a continuous wave modulated by maximum length sequence is adopted, and a hybrid method is employed by incorporating both cross-correlation and phase shift. According to the basic calibration principle, the given TD uncertainty and the thermometer uncertainty, the formulas for calculating the path length uncertainty and the system latency uncertainty are presented, and the uncertainty propagation coefficients are also deduced. To validate the effectiveness of the proposed method, the calibrated results are used in ultrasonic temperature measurement, which shows the temperature accuracy is significantly improved.
Conclusions : This paper presents a promising calibration method of an ultrasonic system for temperature measurement. And the following conclusions can be made:
Conclusions : • By estimating TD between the transmitted signal and received signal at several different temperatures, the calibration equations are presented, and the path length and the system latency are determined simultaneously based on the least squares algorithm; • The formulas for calculating the calibration uncertainties are given, and the uncertainty propagation coefficients are also deduced, which plays a crucial role in finding out which parameter would have the biggest effect and which need only to be roughly considered;
Conclusions : • Both calibration experiment and validation experiment were performed in the distilled water in a stable temperature environment with an accuracy of 0.01°C;
Conclusions : • The proposed method calibrates the path length and the system latency with uncertainties of 0.058 mm and 0.038 μs, respectively. With the use of the calibrated results, the performance of temperature measurement is significantly improved. The temperature error consistently remains within ±0.04°C, and the percentage error is less than 0.15%.

2870145
ABSTRACT : Excessive daytime sleepiness (EDS) is a ubiquitous problem that affects public health and safety. A test that can reliably identify individuals that suffer from EDS is needed. In contrast to other methods, salivary biomarkers are an objective, inexpensive, and noninvasive method to identify individuals with inadequate sleep. Although we have previously shown that inflammatory genes are elevated in saliva samples taken from sleep deprived individuals, it is unclear if inflammatory genes will be elevated in clinical populations with EDS. In this study, salivary samples from individuals with sleep apnea were evaluated using the Taqman low density inflammation array. Transcript levels for 3 genes, including prostaglandin-endoperoxide synthase 2 (PTGS2), were elevated in patients with sleep apnea. Interestingly, PTGS2 was also elevated in patients with EDS but who did not have sleep apnea. These data demonstrate the feasibility of using salivary transcript levels to identify individuals that self-report excessive daytime sleepiness.
Introduction : Inadequate sleep is a pervasive problem in today's society. Insufficient sleep leads to decreased cognitive performance [1] , increased sleepiness [2] [3] [4] , reduced productivity [5] , and increased traffic accidents [6] . Moreover, inadequate sleep increases the susceptibility of individuals to adverse health outcomes, including cardiovascular deficits [7] , increased immune challenges [8, 9] , longer recovery times after injury [10] , increases in sympathetic tone [11, 12] , and reduced lifespan [7] . Given the number and severity of consequences that accompany inadequate sleep, it would be helpful to have a simple and reliable test to identify vulnerable individuals before they experience adverse consequences.
Introduction : One approach to accomplish this objective has been to test candidate biomarkers to determine whether they are consistently altered in subjects with inadequate or insufficient sleep. Biomarkers are objective, often endogenous factors that report changes in body chemistry and correlate with either disease state or the severity of the disease. Several biomarkers, including eyelid closures [13] and balance [14] , as well as biochemical markers from blood [15, 16] , cerebral spinal fluid [17] [18] [19] , and breath analytes [20] have been evaluated as candidate biomarkers of disrupted sleep. Although each of these approaches has had limited success, assessment tools that can be used in real-world settings are not yet widely available. With that in mind, we have hypothesized that saliva, as a rich source of analytes, can be mined to identify biomarkers of sleepiness or inadequate sleep. Saliva is particularly well suited for monitoring sleepiness since it is a readily accessible biological fluid that can be easily collected using noninvasive procedures. Indeed, we have shown that transcripts for -amylase, Filamin-A, maleic enzyme, integrin, M, and integrin, 5, are all elevated in saliva samples following sleep deprivation [21] [22] [23] [24] . Increased levels of salivary -amylase activity have also been shown to correlate with increased sleepiness and decreased cognitive performance in an independent study [25] . However, since endogenous factors are frequently modulated by a variety of physiological conditions (stress, circadian time, etc.), test of sleepiness should be comprised of a panel of independent analytes. Several studies have found that serum markers of inflammation are elevated in populations of individuals with sleep disorders [9] . For example, serum levels of interleukin 6 (IL-6) [26] , interleukin 8 (IL-8) [27, 28] , tumor necrosis factor-(TNF-) [29] , C-reactive protein (CRP) [30] , intracellular adhesion molecule (ICAM) [31] , selectins [32] , and vascular cell adhesion molecule (VCAM) [31] have all been shown to increase in multiple populations of patients with sleep apnea. Therefore, we wanted to determine if levels of salivary inflammation transcripts could be used to identify sleepiness in a clinical population, individuals diagnosed with sleep apnea.

2870252
ABSTRACT : Background-The signaling cascades responsible for the activation of transcription factors in the hypertrophic growth of cardiac myocytes during hemodynamic overload are largely unknown. Several of the genes upregulated in the hypertrophied heart, including B-type natriuretic peptide (BNP) gene, are controlled by the cardiac-restricted zinc finger transcription factor GATA4. Methods and Results-An in vivo model of intravenous administration of arginine 8 -vasopressin (AVP) for up to 4 hours in conscious normotensive rats was used to study the signaling mechanisms for GATA activation in response to pressure overload. Gel mobility shift assays were used to analyze the trans-acting factors that interact with the GATA motifs of the BNP promoter. AVP-induced increase in mean arterial pressure was followed by a significant increase in the BNP and c-fos mRNA levels in both the endocardial and epicardial layers of the left ventricle, whereas GATA4 and GATA6 mRNA levels remained unchanged. Pressure overload within 15 to 60 minutes produced an increase in left ventricular BNP GATA4 but not GATA5 and GATA6 binding activity, and at 30 minutes a 2.2-fold increase (PϽ0.001) in GATA4 binding was noted. The mixed endothelin-1 ET A /ET B receptor antagonist bosentan but not the angiotensin II type 1 receptor antagonist losartan completely inhibited the pressure overload-induced increase in left ventricular BNP GATA4 binding activity. Bosentan alone had no statistically significant effect on GATA4 binding activity of the left ventricle in conscious animals. Conclusions-ET-1 is a signaling molecule that rapidly upregulates GATA4 DNA binding activity in response to pressure overload in vivo. (Circulation. 2001;103:730-735.) 

2870542
ABSTRACT : . (2016) Understanding patient safety performance and educational needs using the 'Safety-II' approach for complex systems. Education for Primary Care, 27(6), pp. 443-450. (doi:10.1080Care, 27(6), pp. 443-450. (doi:10. /14739879.2016 This is the author's final accepted version.
ABSTRACT : There may be differences between this version and the published version. You are advised to consult the publisher's version if you wish to cite from it.
ABSTRACT : http://eprints.gla.ac.uk/131620/ 
ABSTRACT : Patient safety education and participation are key components of general practice (GP) specialty training, appraisal and revalidation. Priorities for GP education at all career stages are described in the Royal College of General Practitioners curriculum. Current methods that are taught and employed to improve safety often use a 'find-and-fix' approach to identify 'malfunctioning' components of a system (including humans) and introduce change to improve performance -often by attempting to increase conformity with protocols and guidelines. The complex interactions and inter-dependence between components found in healthcare systems mean that 'cause and effect' are not always linked in a predictable manner, meaning this approach does not always improve performance.
ABSTRACT : The Safety-II approach is considered a new way to understand how safety is achieved in complex systems. Understanding and applying this approach may improve quality and safety initiatives and enhance GP and trainee curriculum coverage. Safety-II aims to maximise the number of events with a successful outcome by exploring everyday work.
ABSTRACT : ways to achieve success, dependent on work conditions, may be possible. Understanding and managing variability, rather than constraining it, may be a more beneficial approach.
ABSTRACT : The application of a Safety-II approach to incident investigation, quality improvement projects, prospective analysis of risk in systems and performance indicators may offer improved insight into system performance leading to more effective change. The way forward may be to combine the Safety-II approach with 'traditional' methods to enhance patient safety training, outcomes and curriculum coverage.
ABSTRACT : 3
Introduction : As the patient safety agenda has evolved in primary care over the past decade, completion and application of learning on safety and quality methods has become an important component of the general practice (GP) specialty training curriculum and of appraisal and revalidation. [1, 2] The RCGP patient safety curriculum describes the expertise required to practice as a GP in the United Kingdom (UK) and can act as a guide to learning at any career Practice, is now embedded within most GP practices in the UK. [3] [4] [5] Despite this focus on reflecting on care and improving patient safety, firm evidence that patients are now safer is lacking. [6, 7] Berwick's influential report "A promise to learn -a commitment to act" stated that, in the vast majority of cases, NHS staff were not to blame for patient safety problems. [8] He emphasised the effect of systems and work conditions on staff performance and that, in certain high profile cases, indicators of impending problems had been ignored. Further, it is argued that a new way of thinking about safety is needed which moves beyond viewing safety through the lens of problems, error and failure. [9] This suggests we should be attempting to understand and teach how safety is achieved in the complex conditions and systems found in healthcare.
Introduction : In our previous article we described key concepts for those involved in teaching or performing safety and improvement work. [10] In this article we explore some of these concepts in more depth to aid their application and teaching. The field of Resilience Engineering has given rise to a new way of thinking about patient safety now commonly referred to as 'Safety-II'. [11, 12] This approach attempts to explain and potentially resolve some of the intractable problems associated with complex systems such as those found in primary care, which traditional safety management thinking and responses (termed Safety-I) have struggled to adequately understand and improve upon. If successfully conceptualised, taught and implemented, this approach may lead to better reflection by trainees and qualified GPs (and wider primary care teams) on how everyday success is achieved in the challenging working conditions found in general practice thus allowing more effective change to increase the safety of healthcare systems.

2870859
ABSTRACT : Abstract-Bosch has developed and demonstrated a novel direct current (DC) microgrid system that maximizes the efficiency of locally generated photovoltaic energy while offering high reliability, safety, redundancy, and reduced cost compared to equivalent alternating current (AC) systems. Several demonstration projects validating the system feasibility and expected efficiency gains have been completed and additional ones are in progress. This paper gives an overview of the Bosch DC microgrid system and presents key results from a large simulation study done to estimate the energy savings of the Bosch DC microgrid over conventional AC systems. The study examined the system performance in locations across the United States for several commercial building types and operating profiles. It found that the Bosch DC microgrid uses generated PV energy 6%-8% more efficiently than traditional AC systems.
I. INTRODUCTION : Zero net energy policy goals for buildings in the United States and across the world imply a high penetration of distributed renewable energy resources and a substantial increase in energy efficiency. According to the National Science and Technology Council, aggressive adoption of energy efficiency technologies will reduce building energy consumption by 60%-70% [1] . The remaining 30%-40% of energy must come from onsite generation to achieve zero net site energy. Distributed renewable energy will be deployed on a large scale only when its assets provide attractive returns to owners and enable utilities and grid operators to safely and reliably mitigate the impact of renewables' intermittency on the electricity distribution infrastructure.
I. INTRODUCTION : The direct current (DC) microgrid presented in this paper offers significant energy efficiency, cost, reliability, and safety benefits compared to conventional alternating current (AC) systems. In the Bosch DC microgrid (DCMG) architecture, onsite DC distributed generation such as solar PV is directly connected to energy-efficient DC lighting, DC ventilation, and other DC loads via a 380 V nominal DC bus. A central AC/DC gateway converter provides supplemental grid power whenever local generation cannot fully supply the load. Thus, the DCMG eliminates the use of AC/DC rectifiers at the loads and reduces the need for DC/AC inverters that are currently required to interconnect solar photovoltaics (PV) to the electric utility.
I. INTRODUCTION : The reduction in conversion equipment makes the overall system more efficient and reliable and reduces maintenance costs. The use of a separate DC bus provides a built-in mechanism for operating critical DC loads during grid outages (to the extent that energy is available from local DC generation or storage) without requiring a mechanical transfer switch. From the utility perspective, the DC architecture reduces the size of inverters required to export excess PV energy, thereby mitigating the potential impact of PV variability on the grid. Furthermore, DC-based battery storage can be much more efficiently connected to a DCMG, enabling a more costeffective way to smooth solar power intermittency.
I. INTRODUCTION : By transitioning most of the major hard-wired loads in a building to the DC distribution system, customers can expect up to 30% lower total cost of ownership over the life of the system, higher reliability, and optimized use of renewable generation compared to a conventional AC microgrid. At scale, the capital cost is anticipated at 15%-20% lower than a comparable AC system; the operating costs will also be significantly lower over the 25-year life of the system. The DC system architecture is applicable to a wide variety of commercial buildings, including big-box retail stores, warehouses, distribution centers, and manufacturing facilities.
I. INTRODUCTION : In addition to several systems installed at Bosch facilities, two high-profile DCMG projects are currently underway at external sites. The U.S. Department of Defense has awarded Bosch a demonstration project to be completed in 2015 that involves the retrofit of a building at Fort Bragg in Fayetteville, North Carolina. In addition, the California Energy Commission has awarded Bosch and its partners a demonstration project in southern California that will include 300 kW of installed PV.
I. INTRODUCTION : Section II of this paper provides an overview of the Bosch DCMG system and its components. Sections III and IV summarize the methodology and key results, respectively, for a DCMG simulation study and system energy analysis conducted by the National Renewable Energy Laboratory (NREL). The study estimated the performance of the DCMG for several key metrics, including annual grid energy, system energy efficiency, and PV utilization fraction, for several commercial building types under a variety of operating schedules.
V. CONCLUSION : Bosch has developed a novel DC microgrid design that connects PV generation to DC loads with a minimal number of energy conversions, significantly increasing energy efficiency compared to a traditional AC system. NREL's simulation study of the DC microgrid concluded that it improves the percentage of PV energy that performs useful work to approximately 97% from a baseline value of 90%, with small variations in response to design parameters, operating conditions, and location.

2871295
ABSTRACT : A photosynthetic organism is subjected to photooxidative stress when more light energy is absorbed than is used in photosynthesis. In the light, highly reactive singlet oxygen can be produced via triplet chlorophyll formation in the reaction centre of photosystem II and in the antenna system. In the antenna, triplet chlorophyll is produced directly by excited singlet chlorophyll, while in the reaction centre it is formed via charge recombination of the light-induced charge pair. Changes of the mid-point potential of the primary quinone acceptor in photosystem II modulate the pathway of charge recombination in photosystem II and influence the yield of singlet oxygen production. Singlet oxygen can be quenched by b-carotene, a-tocopherol or can react with the D1 protein of photosystem II as target. If not completely quenched, it can specifically trigger the up-regulation of the expression of genes which are involved in the molecular defence response of plants against photo-oxidative stress.

2871678
ABSTRACT : The tumor necrosis factor-related apoptosis-inducing ligand (TRAIL) is regarded as a potential anticancer agent. However, considerable numbers of cancer cells, especially some highly malignant tumors, are resistant to apoptosis induction by TRAIL, and some cancer cells that were originally sensitive to TRAIL-induced apoptosis can become resistant after repeated exposure (acquired resistance). Understanding the mechanisms underlying such resistance and developing strategies to overcome it are important for the successful use of TRAIL for cancer therapy. Resistance to TRAIL can occur at different points in the signaling pathways of TRAILinduced apoptosis. Dysfunctions of the death receptors DR4 and DR5 due to mutations can lead to resistance. The adaptor protein Fas-associated death domain (FADD) and caspase-8 are essential for assembly of the death-inducing signaling complex, and defects in either of these molecules can lead to TRAIL resistance. Overexpression of cellular FADD-like interleukin-1b-converting enzymeinhibitory protein (cFLIP) correlates with TRAIL resistance in several types of cancer. Overexpression of Bcl-2 or Bcl-X L , loss of Bax or Bak function, high expression of inhibitor of apoptosis proteins, and reduced release of second mitochondria-derived activator of caspases (Smac/Diablo) from the mitochondria to the cytosol have all been reported to result in TRAIL resistance in mitochondriadependent type II cancer cells. Finally, activation of different subunits of mitogen-activated protein kinases or nuclear factor-kappa B can lead to development of either TRAIL resistance or apoptosis in certain types of cancer cells.

2871844
ABSTRACT : The association between cardiorespiratory fitness (fitness) and mortality is well described. However, the association between midlife fitness and the development of nonfatal chronic conditions in older age has not been studied.
Conclusions: : In this cohort of healthy middle-aged adults, fitness was significantly associated with a lower risk of developing chronic disease outcomes during 26 years of follow-up. These findings suggest that higher midlife fitness may be associated with the compression of morbidity in older age.

2872613
ABSTRACT : Poor balance in older persons contributes to a rise in fall risk and serious injury, yet no consensus has developed on which measures of postural sway can identify those at greatest risk of falling. Postural sway was measured in 161 elderly individuals (81.8y67.4), 24 of which had at least one self-reported fall in the prior six months, and compared to sway measured in 37 young adults (34.9y67.1). Center of pressure (COP) was measured during 4 minutes of quiet stance with eyes opened. In the elderly with fall history, all measures but one were worse than those taken from young adults (e.g., maximal COP velocity was 2.76 greater in fallers than young adults; p,0.05), while three measures of balance were significantly worse in fallers as compared to older persons with no recent fall history (COP Displacement, Short Term Diffusion Coefficient, and Critical Displacement). Variance of elderly subjects' COP measures from the young adult cohort were weighted to establish a balance score (''B-score'') algorithm designed to distinguish subjects with a fall history from those more sure on their feet. Relative to a young adult B-score of zero, elderly ''non-fallers'' had a B-score of 0.334, compared to 0.645 for those with a fall history (p,0.001). A weighted amalgam of postural sway elements may identify individuals at greatest risk of falling, allowing interventions to target those with greatest need of attention.
Introduction : Incidence of falls escalates with age, events exacerbated by declines in muscle mass, strength, coordination and balance [1, 2] . When considered in concert with age-related decline in bone quality and quantity [3] , this elevated risk of falling portends an increase in the incidence of injury, including fracture [4] . The increase in falls and concomitant injuries not only elevates rates of mortality, it poses significant economic and societal burdens to health care systems worldwide [5] .
Introduction : Poor postural control is recognized as a major contributor to fall risk, and individuals may rely upon a range of balance strategies to remain upright [6, 7] . Stable posture is maintained by a complex, integrated feedback from the visual, proprioceptive, and vestibular systems, as well as coordinated control by the neuromuscular system [8] [9] [10] . A commonly available -and relatively simplemethod of assessing balance and instability is stabilogram analysis, which entails recording the ground reaction vector, known as center of pressure (COP) [11] [12] [13] . Measures of stabilogram magnitude, peak and average sway velocities, and root-meansquare (RMS) amplitude, are used as predictors of fall-risk [14, 15] , while frequency domain characteristics have been proposed as an alternative way to express the results of COP measures to best capture postural control [16] .
Introduction : With age, diseases such as glaucoma, diabetic neuropathy, and sarcopenia, as well as age-related declines in hearing, diminish the quality of the integrated sensory input systems critical to stability. Inevitably, fall risk is ultimately a complex amalgam of many different system inputs. Nevertheless, regardless of cause, identifying specific components of balance that contribute to elevated risk may help to target interventional strategies or environmental modifications to reduce the occurrence of falls.
Introduction : The principal objective of this work was to determine if there were differences in parameters of balance between a young and elderly cohort -both with and without a self-reported history of falls -and use these data to develop an algorithm based on these retrospective data to potentially -and isolate those specific components of the posture measurements which help identify individuals at increased risk of falls. We hypothesized that 'elderly' subjects would be less stable than younger subjects, and that those elderly subjects with a history of falls would have a COP signature distinct from age-matched non-fallers. The comparison of COP measures in elderly fallers vs. non-fallers, and their relationship to the young healthy cohort was then used to iteratively develop a fall predictor algorithm. It is hoped that data such as these may ultimately provide simple, relatively accessible diagnostic infor-W mation from balance to prospectively identify those individuals at greatest risk of falls.

2872653
ABSTRACT : Background: The yeast Saccharomyces cerevisiae relies on the high-osmolarity glycerol (HOG) signaling pathway to respond to increases in external osmolarity. The HOG pathway is rapidly activated under conditions of elevated osmolarity and regulates transcriptional and metabolic changes within the cell. Under normal growth conditions, however, a three-component phospho-relay consisting of the histidine kinase Sln1, the transfer protein Ypd1, and the response regulator Ssk1 represses HOG pathway activity by phosphorylation of Ssk1. This inhibition of the HOG pathway is essential for cellular fitness in normal osmolarity. Nevertheless, the extent to and mechanisms by which inhibition is robust to fluctuations in the concentrations of the phospho-relay components has received little attention. Results: We established that the Sln1-Ypd1-Ssk1 phospho-relay is robust-it is able to maintain inhibition of the HOG pathway even after significant changes in the levels of its three components. We then developed a biochemically realistic mathematical model of the phospho-relay, which suggested that robustness is due to buffering by a large excess pool of Ypd1. We confirmed experimentally that depletion of the Ypd1 pool results in inappropriate activation of the HOG pathway. Conclusions: We identified buffering by an intermediate component in excess as a novel mechanism through which a phospho-relay can achieve robustness. This buffering requires multiple components and is therefore unavailable to two-component systems, suggesting one important advantage of multi-component relays.
ABSTRACT : The high-osmolarity glycerol (HOG) pathway (Figure 1 ) of the budding yeast Saccharomyces cerevisiae mediates cellular response to increased external osmolarity [1, 2] . A key component of the HOG pathway is a mitogenactivated protein (MAP) kinase cascade. Within the kinase cascade, the MAP3Ks Ssk2 and Ssk22 phosphorylate the MAP2K Pbs2, which in turn phosphorylates the MAP kinase Hog1. Phospho-Hog1 then regulates
Conclusions : Robustness of the Sln1-Ypd1-Ssk1 phospho-relay is essential to prevent spurious activation of the HOG pathway, which severely compromises yeast fitness. We established that the phospho-relay is robust to perturbations in the concentrations of the three relay components. A theoretical analysis suggested that a large pool of the intermediate component Ypd1 can buffer fluctuations in other pathway components to maintain robustness. This suggestion was consistent with earlier published measurements indicating that Ypd1 is at least 5 times more abundant than Ssk1 at normal expression levels [7, 29] . Although Ypd1 may also bind to the protein Skn7, combined levels of Ssk1 and Skn7 have been measured to be below total Ypd1 levels [29] . Our subsequent experiments confirmed that depletion of this buffering pool of Ypd1 leads to inappropriate activation of the HOG pathway.
Conclusions : The differential expression of Ypd1 and Ssk1 enables phosphorylation of excess Ssk1 and stabilization of the new phospho-Ssk1, buffering HOG pathway activation to fluctuations in Ssk1 levels. This novel mechanism of robustness suggests an advantage of a three-component Figure 5 Massive overexpression of phospho-relay components leads to growth defects. A Haploid GEV strains carrying a high-copy plasmid with an inducible HOG pathway gene were grown in different concentrations of β-estradiol. The OD 600 after 36 hours of growth is plotted as a function of β-estradiol concentration. Each point represents the mean and standard deviation of four replicates. Overexpression of Sln1 and Ssk1 (but not Ypd1) caused a growth defect. B The same strains were grown over a finer titration of β-estradiol concentrations. The OD 600 after 36 hours is plotted. At this resolution, it is clear that the growth defect from Ssk1 overexpression is more severe than the growth defect from Sln1 overexpression at low β-estradiol concentrations. C The same strains were frogged onto plates containing different concentrations of β-estradiol. Massive overexpression of Sln1 and Ssk1 again caused a growth defect comparable to that from overexpression of Pbs2. In all experiments, the parent strain carrying the empty vector plasmid [2μ P GAL1 scURA3] was used as a negative control.
Conclusions : architecture over a two-component one. In particular, the implementation of an analogous buffering strategy in a two-component system would be difficult because it would require expressing the sensor histidine kinase at very high levels. This situation might lead to imprecise sensing and various other off-target effects. In contrast, Figure 6 Growth defects following massive overexpression of phospho-relay components are due to activation of the HOG pathway. A We assayed for Hog1 phosphorylation after overexpression of relay components (Sln1, Ypd1, Ssk1) and positive controls (Pbs2, Ssk22). The parental strain carrying the empty plasmid vector was used as a negative control. B We quantified the amount of phosphorylated Hog1 (relative to Hog1) in five biological replicates of this experiment. Error bars represent the standard error. Pbs2, Ssk22, and Ssk1 caused a significant ( * ) change in Hog1 phosphorylation levels after overexpression for 30 minutes (p = 0.0395, 0.0096, and 0.0224, respectively; paired t-test). Hog1 phosphorylation levels were also significantly lower in the Ypd1 overexpression strain (p = 0.0246, two-way ANOVA).
Conclusions : the use of an intermediate transfer protein enables robust buffering with both the sensor and response regulator expressed at comparable levels. Our work has thus identified a potential mechanism for circumventing a tradeoff between efficient sensing and robust control. There are other possible advantages for a three-component architecture, including combinatorial control of response regulators by sensor proteins through a common phosphotransfer protein or segregation of sensing and activation functions between the nucleus and cytoplasm. Intriguingly, deletion of YPD1 has recently been shown to cause constitutive activation of the HOG pathway in Candida albicans, suggesting that its buffering capacity might also be important in this organism [30] .
Conclusions : Robustness in real biological systems is necessarily approximate and apt to be compromised at extreme expression levels of cellular components. In many systems, however, it has proven difficult to characterize where robustness breaks down and to reconcile such results with mathematical models, which often predict exact robustness [31] . Our combined theoretical and experimental results specify a single condition (Ypd1 in large excess) for robust regulation of the HOG pathway.
Conclusions : The link between bifunctionality and robustness is well-established [31] [32] [33] [34] [35] , and it is known that bifunctionality of EnvZ is essential to robustness in Escherichia coli osmoregulation [36, 37] . As such, it is intriguing that our model suggests that robustness in S. cerevisiae osmoregulation is not dependent on bifunctionality of Sln1. It is important to emphasize, however, that bifunctionality would not compromise robustness. Rather, the model indicates that any upstream process that produces non-zero levels of Y and Y P should enable the same fundamental behavior predicted by Eq. 1. The possibility that Sln1 exhibits phosphatase activity warrants further experimental investigation.

2873021
ABSTRACT : With the advances in e-Sciences and the growing complexity of scientific analyses, more and more scientists and researchers are relying on workflow systems for process coordination, derivation automation, provenance tracking, and bookkeeping.
Introduction : Scientific workflow has become increasingly popular in modern scientific computation as more and more scientists and researchers are relying on workflow systems to conduct their daily science analysis and discovery. With technology advances in both scientific instrumentation and simulation, the amount of scientific datasets is growing exponentially each year, such large data size combined with growing complexity of data analysis procedures and algorithms have rendered traditional manual processing and exploration unfavorable as compared with modern in silico processes automated by scientific workflow systems (SWFS). While the term workflow speaks of different things in different context, we find in general SWFS are engaged and applied to the following aspects of scientific computations: 1) describing complex scientific procedures, 2) automating data derivation processes, 3) high performance computing (HPC) to improve throughput and performance, and 4) provenance management and query.
Introduction : Workflows are not a new concept and have been around for decades. There were a number of coordination languages and systems developed in the 80s and 90s [1, 7] , which share many common characteristic with workflow systems (i.e. they describe individual computation components and their ports and channels, and the data and event flow between them). They also coordinate the execution of the components, often on parallel computing resources. Furthermore, business process management systems have been developed and invested in for years; there are many mature commercial products and industry standards such as BPEL [2] . In the scientific community there are also many emerging systems for scientific programming and computation [5, 22] . Before we jump on developing yet another workflow system, a fundamental question to ask is whether we can use existing technologies, or we should invent new languages and systems in order to achieve the four aspects mentioned earlier that are essential to scientific workflow systems. This paper identifies the challenges to workflow development in the context of scientific computation; we present an overview of some of the existing technologies and emerging systems, and discuss opportunities in addressing these challenges.

2874048
ABSTRACT : The study of the spread of influence through a social network has a long history in the social sciences. The first studies focused on the adoption of medical and agricultural innovations, later marketing researchers investigated the "word-of-mouth" diffusion process as an important mechanism by which information can reach large populations, possibly influencing public opinion, driving new product market share and brand awareness. Recently, thanks to the success of on-line social networks and microblogging platforms such as Facebook and Twitter, the phenomenon of influence exerted by users of an online social network on other users and in how it propagates in the network, has attracted the interest of computer scientists and IT specialists. One of the key problems in this area is the identification of influential users, by targeting whom certain desirable outcomes can be achieved. Here, targeting could mean giving free (or price discounted) samples of a product and the desired outcome may be to get as many customers to buy the product as possible. In this talk we take a data mining perspective and we discuss what (and how) can be learned from the available traces of past propagations. While doing this we provide a brief survey of some recent progresses in this area, as well as discuss the open problems.

2874113
ABSTRACT : Abstract-Orthogonal frequency division multiplexing (OFDM) is a technique that will prevail in the next generation wireless communication. Channel estimation is one of the key challenges in an OFDM system. In this paper, we formulate OFDM channel estimation as a compressive sensing problem, which takes advantage of the sparsity of the channel impulse response and reduces the number of probing measurements, which in turn reduces the ADC speed needed for channel estimation. Specifically, we propose sending out pilots with random phases in order to "spread out" the sparse taps in the impulse response over the uniformly downsampled measurements at the low speed receiver ADC, so that the impulse response can still be recovered by sparse optimization. This contribution leads to high resolution channel estimation with low speed ADCs, distinguishing this paper from the existing attempts of OFDM channel estimation. We also propose a novel estimator that performs better than the commonly used 1 minimization. Specifically, it significantly reduces estimation error by combing 1 minimization with iterative support detection and limited-support least-squares. While letting the receiver ADC running at a speed as low as 1/16 of the speed of the transmitter DAC, we simulated various numbers of multipaths and different measurement SNRs. The proposed system has channel estimation resolution as high as the system equipped with the high speed ADCs, and the proposed algorithm provides additional 6 dB gain for signal to noise ratio.
I. INTRODUCTION : Orthogonal frequency division multiplexing (OFDM) has been widely applied in wireless communication systems, because it transmits at a high rate, achieves high bandwidth efficiency, and is robust to multipath fading and delay [1] . OFDM applications can be found in digital television and audio broadcasting, wireless networking, and broadband internet access. Current OFDM based WLAN standards (such as IEEE802.11a/g) use variations of QAM schemes for subcarrier modulations which require a coherent detection at the OFDM receiver and consequently requires an accurate (or near accurate) estimation of Channel State Information (CSI). The structure of OFDM signal makes it difficult to balance complexity and performance in channel estimation. The design principles for channel estimators are to reduce the computational complexity and bandwidth overhead while maintaining sufficient estimation accuracy.
I. INTRODUCTION : Some channel estimation schemes proposed in literature are based on pilots, which form the reference signal used by both the transmitter and the receiver. This approach has two main challenges: (i) the design of pilots; and (ii) the design of an efficient estimation algorithm (i.e., the estimator).
I. INTRODUCTION : There is a tradeoff between the spectrum efficiency and the channel estimation accuracy. Most of the existing pilotassisted OFDM channel estimation schemes rely on the use of a large number of pilots to increase to estimation accuracy; the spectral efficiency is therefore reduced. For example, there are approaches based on time-multiplexed pilot, frequencymultiplexed pilot, and scattered pilot [2] , all achieving higher estimation accuracy at the price of using more pilots. There have been attempts to reduce the number of pilots, i.e. J. Byun et al. in [3] . The solutions generally require extra "test signal" for channel pre-estimation. By sending out "test signal", they try to find out how many pilots are needed by firstly inserting a relatively small number of pilots and then, based on the results of the "test", the number of pilots are decided. Therefore, there is no guaranteed overall reduction of pilots insertion.
I. INTRODUCTION : As a sensing problem, OFDM channel estimation can benefit from the emerging technique of compressive sensing (CS), which acquires and reconstructs a signal from fewer samples than what is dictated by the Nyquist-Shannon sampling theorem, mainly by utilizing the signal's sparse or compressible property. The field has exploded since the pioneering work by Donoho [4] and Candes, Romberg and Tao [5] . The main idea is to encode a sparse signal by taking its "incoherent" linear projections and recover the signal through algorithms such as 1 minimization. To maximize the benefits of CS for OFDM channel estimation, one shall skillfully perform the CS encoding and decoding steps, which are precisely the two focuses of this paper: the designs of pilots and estimator, respectively.
I. INTRODUCTION : Contributions: CS has been applied to channel estimation in [13] [14] [15] [16] , which are reviewed in subsection III-E below. For OFDM channel estimation, there are papers [6] [7] [8] [9] , to which our work differs in various ways as follows. We skillfully design CS encoding and decoding strategies for OFDM channel estimation. Compared to existing work, we are able to obtain channel response in much higher resolutions and from much fewer pilots (thus taking much shorter times). This is achieved by designing pilots with uniform random phases and using a novel estimator. The pilot design preserves the information of high-resolution channel response during aggressive uniform down-sampling, which means that receiver ADC can run at a much lower speed. The estimator is tailored for OFDM channel response; in particular, instead of the generic 1 minimization, iterative support detection (ISD) [17] and limited-support least-squares are adopted in order to take advantage of the characteristics of channel response. The resulting algorithm is very simple and performs better.
I. INTRODUCTION : The rest of this paper is organized as follows: Section II reviews the general OFDM system model and sets up the channel estimation formulation. Section III relates channel estimation to CS and present the proposed pilot design. In Section IV, the estimator based on iterative support detection and limited-support least-squares are introduced. Section V give the simulation results. Finally, Section VI concludes this work.
I. INTRODUCTION : II. OFDM SYSTEM MODEL A baseband OFDM system is shown in Figure 1 . In this system, the modulated signal in the frequency domain, represented by X(k), k ∈ [1, N] , is inserted with pilot signal and guard band, and then an N -point IDFT transforms the signal into the time domain, denoted by x(n), n ∈ [1, N] , where a cyclic extension of time length T G is added to avoid inter-symbol and inter-subcarrier interferences. The resulting time series data is converted by a digital-to-analog converter (DAC) with a clock speed of 1/T S Hz into an analog signal for transmission. We assume that the channel response comprises P propagation paths, which can be modeled by a time-domain complex-baseband vector with P taps:
I. INTRODUCTION : where α p is a complex multipath component and τ p is the multipath delay (0 
I. INTRODUCTION : where ⊗ denotes convolution and ξ(n), n ∈ 
I. INTRODUCTION : , where the guard band and pilot signal will be removed. For pilot assisted OFDM channel estimation, we shall design the pilots X (and thus x) and recover h from the measurements Y (or, equivalently y).

2874360
ABSTRACT : Objective. To evaluate the effectiveness of radiation protective curtains in reducing the occupational radiation exposure of medical personnel. Methods. We studied medical staff members who had assisted in 80 consecutive therapeutic endoscopic retrograde cholangiopancreatography (ERCP) procedures. Use of radiation protective curtains mounted to the X-ray tube was determined randomly for each procedure, and radiation doses were measured with electronic pocket dosimeters placed outside the protective apron. Results. When protective curtains were not used, the mean radiation doses to endoscopists, first assistants, second assistants, and nurses were 340.9, 27.5, 45.3, and 33.1 Sv, respectively; doses decreased to 42.6, 4.2, 13.1, and 10.6 Sv, respectively, when protective curtains were used ( < 0.01). When the patient had to be restrained during ERCP ( = 8), the radiation dose to second assistants without protective curtains increased by a factor of 9.95 ( < 0.01) relative to cases in which restraint was not required. Conclusions. During ERCP, not only endoscopists, but also assistants and nurses were exposed to high doses of radiation. Radiation exposure to staff members during ERCP was reduced with the use of protective curtains.
Introduction : Techniques related to endoscopic retrograde cholangiopancreatography (ERCP) and endoscopic ultrasonography (EUS) are becoming more widely used in the field of gastrointestinal endoscopy [1] . Although such advances have allowed patients to receive minimally invasive treatments, they take longer to perform and, consequently, expose both patients and medical staff members (occupational exposure) to higher doses of radiation. There is no established radiation exposure threshold for patients, because exposure is permitted as long as the benefits of the examination or treatment outweigh the risks of exposure. In contrast, there are strict annual permissible doses for medical personnel, because they receive no benefit from radiation exposure. Reducing the radiation exposure of patients and medical personnel is an important issue [2] [3] [4] [5] [6] .
Introduction : It is already known that scatter radiation with overheadtube fluoroscopic equipment is higher than that with undercouch-tube equipment. Lens injuries induced in nonoptimized interventional radiology laboratories have been reported [7] . However, overhead-tube equipment is still often used in endoscopic procedures. Equipment such as radiation protective clothing and eyewear is used to prevent exposure to scattered radiation from patients, which is the main source of occupational radiation exposure for health care providers. However, these types of protective equipment do not cover some parts of the body, and it is very important to protect the entire body. In an experimental and clinical study, Kurihara et al. found that using a protective lead shield mounted to the X-ray tube was effective in reducing the radiation dose to staff members [8] . However, it is not yet known how effective a protective lead shield would be for protection in clinical practice. In this study, we assessed the level of occupational radiation exposure during ERCP and measured the radiation doses to staff members when a protective lead shield was used and was not used during ERCP to assess the effectiveness of the shield.
Conclusion : Not only endoscopists, but also assistants and nurses are exposed to a high dose of radiation during ERCP, which is why appropriate measures must be taken. Use of protective curtains reduced the radiation dose to all medical staff members by shielding the entire operating room from scattered radiation and thus was very effective in reducing occupational radiation exposure.

2874456
ABSTRACT : Background: The Personal Responsibility and Work Opportunity Reconciliation Act (PRWORA) of 1996 gave states the option to withdraw Medicaid coverage of nonemergency care from most legal immigrants. Our goal was to assess the effect of PRWORA on hospital uncompensated care in the United States.

2874531
ABSTRACT : Die Dokumente auf EconStor dürfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden.
ABSTRACT : Sie dürfen die Dokumente nicht für öffentliche oder kommerzielle Zwecke vervielfältigen, öffentlich ausstellen, öffentlich zugänglich machen, vertreiben oder anderweitig nutzen.
ABSTRACT : Sofern die Verfasser die Dokumente unter Open-Content-Lizenzen (insbesondere CC-Lizenzen) zur Verfügung gestellt haben sollten, gelten abweichend von diesen Nutzungsbedingungen die in der dort genannten Lizenz gewährten Nutzungsrechte.
ABSTRACT : This paper analyzes the impact of shortening the duration of secondary schooling on the accumulation of human capital. In 2003, an educational policy reform was enacted in Saxony-Anhalt, a German state, providing a natural experimental setting. The thirteenth year of schooling was eliminated for those students currently attending the ninth grade. Tenth grade students were unaffected. The academic curriculum remained almost unaltered. Primary data collected from the double cohort of 2007 Abitur graduates reveals significantly negative effects for both genders in mathematics. Only females were negatively effected in English and the results obtained in German literature were statistically insignificant.
Introduction : The enactment of educational policies designed to foster scholastic achievement must be a national priority. The schooling opportunities available to the nation's young people are essential ingredients for the cognitive skill formation process. Given today's accelerating technological change, together with an increasingly competitive global economic environment, the importance of cognitive skills has become recognized as essential for increases in individual earnings and aggregate economic outcomes. 1 Previously, public educational policy has been concerned principally with issues relating to the quantity of schooling. The implementation of compulsory education, raising the minimum school drop out age, and lengthening the time allotted for the completion of the necessary university entrance qualifications were enacted to enhance educational outcomes. 2 The opportunity costs associated with these quantity related policies, however, are high. They tend to reduce the time available for graduate studies, for the accumulation of work experience, for the earning of income, and for the starting of a family. Consequently, a superior educational policy should be one whose focus is to promote the quality of the educational experience and not one that simply adds to its quantity. 3 An important question, however, remains unanswered. Is it possible to achieve this goal by increasing the learning intensity ratio, i.e., the ratio of academic curriculum content per unit of instructional time? If the length of time students spend in school is reduced, while at the same time the curriculum content remains the same, is that the optimal way to shorten the duration of schooling without affecting the overall quality of education? Presently, little is known about the relationship between learning intensity, an essential element in the quality of education, and the academic achievement of students, a measure of their human capital accumulation. In this paper, the relationship between increased learning intensity ratios and the student academic achievements that result are investigated.
Introduction : International comparisons have shown that Gymnasium (secondary school) graduates in Germany are comparatively older than their counterparts in comparable countries. 4 As a result, almost all of the German states implemented policies designed to reduce the time spent in secondary school by eliminating the thirteenth year. This was done, however, without commensurately reducing the scholastic requirements for graduation. The academic curriculum remained almost unaltered and, therefore, the learning intensity ratio for the twelve-year students was considerably increased. This change was announced in 2003 and was enacted for the first time in 2007 in the state of Saxony-Anhalt. Subsequently, similar changes were implemented in almost all other German states. This educational reform provides a natural experimental setting where comparisons in the scholastic achievement of graduates in this double cohort of students can be compared.
Introduction : Using primary data from the Saxony-Anhalt double cohort of 2007 Abitur graduates, yields the following results: The estimated effects of increased learning intensities on the scholastic achievements of students depend on the specific academic subjects considered. In addition, the effects differ by gender. Significantly negative effects were discovered in mathematics for both genders, however, it was much more pronounced for males. Scholastic performance in foreign language was also decreased due to the reform for females, but the effect for males was statistically insignificant. No differences were discovered in German literature.
Introduction : There exists only very few published studies where the effects of increased learning intensity are related to scholastic achievements. Pischke (2007) investigated the impact of shortening the instructional time by two short school years 1966-7 in West Germany on grade repetition, secondary schooling opportunities, earnings, and employment. He found no negative effects on earnings and employment but there was an increase in grade repetition and lesser academic track choice. It is, however, the only study considering policy-induced variation in schooling time without a commensurate alteration in the curriculum. As there existed no standardized testing system in Germany at the time, he could not estimate the effect directly on student performance. Consequently, the opportunity for deriving insights concerning the development of human capital is limited for that reason. Furthermore, translating these results into today's world may be difficult, as the composition of the student body has changed substantially. Today there is a trend towards more students seeking diplomas in the highest level of secondary education. Further evidence was provided by Skirbekk (2006) who looked at the effect of variation in the duration of schooling on human capital using test scores from TIMSS for different Swiss cantons. He discovered that differences in the length of the Swiss academic program across regions had no influence on the scholastic achievement in mathematics and aged 19 whereas, e.g., in the Netherlands graduation age is 17-18 years, 18 years in the US, and 17 years in Russia.
Introduction : science when school specific effects were taken into account. Marcotte (2007) , Lee and Barro (2001), and Woessmann (2003) examined the impact of considarable lower reductions in instructional time on student performance. Using Canadian data, Marcotte (2007) used the variation in school days caused by inclement winter weather to identify the impact of increased learning intensity on test scores. His findings are in line with the results presented herein. Students with less instructional time perform significantly worse than their peers most notably in mathematics. Lee and Barro (2001) investigated the effects of school resources on student performance as measured by internationally comparable test scores across countries. They found significant positive effects of the length of the school term on the mathematics and science scores, but significantly negative effects for reading. Woessmann (2003) discovered significantly positive, albeit relatively small, effects of instruction time on student performance in mathematics and science. This evidence suggests that the effect of increasing learning intensity on the accumulation of knowledge depends on the kind of subject.
Introduction : In the Province of Ontario, Canada, an educational reform similar to the German took place.
Introduction : In this instance, the length of schooling in high school was reduced by one year. The major difference compared to the German experience, however, consists in a more modified academic curriculum. In Ontario less courses in main subjects like mathematics and the English language were made available for the treatment group and, therefore, the impact of the reform on learning intensity is not determinable. Moreover, the thirteenth year was not a full-fledged academic grade like it was in Germany. Students in Ontario were able to graduate from high school after the twelfth year. Before the educational reform was enacted, students could complete their schooling by utilizing this additional year or not. Morin (2010) estimated the effect of abolishing the thirteenth year on the academic performance of high-ability students in their first year at the university. He found only small effects on student performance. However, Krashinsky (2006) found larger negative impacts on academic performance at the university analyzing the impact of the same educational reform on students with lower high school grade averages. In addition to the differences with respect to learning intensity their analysis varies from the one presented here because we control for more of the student's personal background information. Another advantage of our study is the fact that the measurements of scholastic achievement were made at the completion of schooling. All of the students were required to take the final exams and so there is no potential for a self-selection problem as with Morin (2010) and Krashinsky (2006) who measure the performance later and only for university students.
Introduction : The present study contributes to the existing literature in several respects. It analyzes a policyinduced large-scale variation in the length of secondary schooling with only minor changes in the academic curriculum, which resulted in a considerably increased level of learning intensity.
Introduction : Identical final written exams for both grades allow for the direct assessment of school performance. Primary data was collected from the double cohort of the 2007 graduating class. The estimation model controlled for a number of student performance influencing factors such as family background, student ability, and school fixed effects. Furthermore, a check was made of the reliability of the assumptions inherent in the natural experiment used to identify the educational reform effect.
Introduction : The paper is organized into seven sections. Section 2 provides background information regarding the educational reform that took place in Germany. A presentation of the natural experiment and the estimation approach is provided in Section 3. The data set used for the empirical analysis is explained in Section 4 together with some selected sample statistics. The empirical estimates of the educational reform on the scholastic achievement of students are provided in Section 5. Section 6 provides a discussion of the implications from these results. The final section concludes.
Conclusion : It is recognized that an adequate amount of instructional time is an essential ingredient in the development of human capital. A sufficient understanding concerning the relationship between these instructional hours and their impact on the human capital enhancement of students, however, is still in its infancy. The gain in human capital that can be attributed to the instructional hours received in schools depends on a diverse set of factors. The innate ability of students to absorb the available knowledge, the amount of effort they expend in this intellectual endeavor, the possibility to interact with stimulating teachers, an amiable social contact with peers, availability of up-to-date academic resources in the school, to name but a few, all play an important role. Presently, there is little evidence concerning the function of the academic curriculum as an important institutional factor in this process of human capital accumulation. The implementation of academic curriculum affects human capital accumulation by its impact on the learning intensity ratio. Consequently, it is useful to study the link between learning intensity and student scholastic achievement. The current lack of evidence in the literature is due to the difficulty in collecting suitable data. This paper attempts to fill this gap and to contribute in this area.
Conclusion : This study is an econometric examination of a very rare educational policy reform that took place in 2007 in Saxony-Anhalt, Germany. This reform shortened the duration of secondary schooling by one full year while maintaining the academic curriculum requirements for graduation nearly unaltered. This substantially increased the learning intensity ratio experienced by the students involved. The estimated effect of this increased learning intensity on student scholastic achievement depends on the particular subject areas studied and they differ by gender.
Conclusion : Significantly negative effects on student scholastic performance in mathematics was discovered that was much more severe for males than for their female counterparts. Scholastic achievement in the English language decreased for females, but the effect for males was statistically 

2874910
ABSTRACT : -previously to affect LTP induction in an ''inverted U'' doseham. Glucocorticoid receptor activation lowers the threshold for dependent fashion. Very low and high serum CORT titers NMDA-receptor-dependent homosynaptic long-term depression in (as might occur after adrenalectomy and stress, respectively) the hippocampus through activation of voltage-dependent calcium are associated with a suppression of LTP induction, whereas channels. J. Neurophysiol. 78: 1-9, 1997. The effects of the gluco-midrange titers are associated with robust LTP induction corticoid receptor agonist RU-28362 on homosynaptic long-term (Diamond et al. 1992; Kerr et al. 1994;).
ABSTRACT : depression (LTD) were examined in hippocampal slices obtained CORT activates two types of receptors, the mineralocortifrom adrenal-intact adult male rats. Field excitatory postsynaptic coid receptor (type I) and the glucocorticoid receptor (type potentials were evoked by stimulation of the Schaffer collateral/ II). These receptors differ in both their affinity for CORT commissural pathway and recorded in stratum radiatum of area CA1. Low-frequency stimulation (LFS) was delivered at LTD and their cellular effects. Mineralocorticoid receptors have threshold (2 bouts of 600 pulses, 1 Hz, at baseline stimulation a 10-fold higher affinity for CORT than glucocorticoid reintensity). LFS of the Schaffer collaterals did not produce signifi-ceptors (Jöels and de Kloet 1995). At the trough of the cant homosynaptic LTD in control slices. However, identical con-circadian rhythm (i.e., low CORT levels), the mineralocortiditioning in the presence of the glucocorticoid receptor agonist coid receptors are tonically occupied, whereas at the peak RU-28362 (10 mM) produced a robust LTD, which was blocked of the circadian rhythm or under periods of stress, there is 

2875387
ABSTRACT : Abstract. This paper describes two ways of improving Burt and Adelson's Laplacian pyramid, a technique developed for image compression. The Laplacian pyramid is a multi-resolution image representation that captures the loss of information occurring through repeated reduction of the spatial resolution. The generation of this data structure involves the use of two complementary functions: EXPAND, which increases the size of an image by a factor of 2, and REDUCE, which performs the reverse operation. The first modification is the adjunction of a pre-filter to the initial EXPAND function in order to guarantee an image extrapolation that is an exact interpolation of the coarser resolution level. The second refinement is a REDUCE operation modified to minimize information loss. The corresponding least squares Laplacian pyramid (LSLP) is generated by adding a post-filter to the initial REDUCE function. These new functions have an efficient implementation using recursive algorithms. Preliminary experiments indicate improved performance: for a Gaussian-like kernel (a = 3), the new EXPAND function exhibits a 2 to 2.5 dB attenuation of the first level of the Laplacian pyramid, while the complete scheme (LSLP) leads to a 4.7 to 8.5 dB improvement in the two images used to test the procedure. For comparable compression ratios, the subjective image quality for the LSLP appears to be significantly better. A theoretical relationship between the present approach and the family of quadrature mirror filter image pyramids is also derived.
ABSTRACT : Zusammenfassung. Diese Arbeit beschreibt zwei Methoden, die Laplacepyramide yon Burr und Adelson zu verbessern, welche zur Bildkompression dient. Die Laplacepyramide ist eine Bilddarstellung dutch Mehrfachaufl6sung, welche den Informationsverlust erfasst, der durch die wiederholte Reduktion der rS.umlichen Aufl6sung entsteht. Fiir die Erzeugung dieser Datenstruktur werden zwei komplement/ire Funktionen gebraucht : EXPAND, welche das Bild um einen Faktor zwei vergr6s-sert, und REDUCE, welche die inverse Operation durchfiihrt. Die erste Modifikation besteht im Hinzufiigen eines Vorfilters zur EXPAND Funktion, um eine Bildextrapolation zu erreichen, welche eine pr/izise Interpolation des gr6beren Aufl6sungsni-veaus ist~ Die zweite Verbesserung betrifft die REDUCE Funktion, welche modifiziert wird, um einen minimalen Informationsverlust zu erreichen. Die entsprechende Laplacepyramide kleinster Quadrate (LSLP) wird durch Hinzufiigen eines Nachfilters zur REDUCE Funktion erzeugt. Diese zwei neuen Funktionen erlauben eine effiziente Realisierung mithilfe rekursiver Algorithmen. Erste Versuche deuten auf eine deutliche Verbesserung hin: fiir einen Gauss-artigen Kern (a = 3) erreicht die neue EXPAND Funktion eine D/impfung von 2 bis 2.5 dB im ersten Niveau der Laplacepyramide, w/ihrend die vollst/indige Methode (LSLP) eine Verbesserung yon 4.7 bis 8.5 dB fiir zwei Testbilder erreicht. Fiir vergleichbare Bildkompressionsfaktoren ist die subjektive Bildqualit/it von LSLP wesentlich besser. Ferner wird ein theoretischer Zusammenhang zwischen dieser Methode und der Familie Bildpyramiden mit quadratischen Spiegelfiltern hergeleitet.
ABSTRACT : R6sum6. Ce papier dbcrit deux faqons d'am61iorer la pyramide Laplacienne de Burt et Adelson, une technique d6veloppee pour la compression d'images. La pyramide Laplacienne est une repr+sentation multi-rbsolution d'image qui code la perte d'information li+e ~i une r6duction r6p6t+e de la r6solution spatiale. La g6n6ration de cette structure s'effectue ~i l'aide de deux opbrations compl6mentaires: EXPAND, qui accro~t la taille de l'image par un facteur deux, et REDUCE qui effectue l'op~ration inverse. La premi6re modification est l'adjonction d'un pr6filtre ~i la fonction EXPAND afin de garantir que l'agrandissement d'une image ~i partir d'une representation plus grossi6re donne lieu ~ une interpolation exacte. La seconde am61ioration est la redefinition de la fonction REDUCE afin de minimiser la perte d'information. Ceci donne lieu/t une repr6sentation pyramidale aux moindres carr6s (LSLP) qui diff6re de la pr6c6dente par la simple adjonction d'un post-filtre. Ces nouvelles fonctions se preterit/t une mise en oeuvre tr6s efficace par filtrage r6cursif. Des experiences pr61iminaires indiquent une am61ioration des performances: pour un noyau quasi-gaussien (a = 3), la nouvelle fonction EXPAND att6nue le premier niveau de la pyramide Elsevier Science Publishers B.V.
ABSTRACT : 188
ABSTRACT : Laplacienne de 2 ',i 2.5 dB, tandis que I'algorithme complet (LSLP) donne lieu ~i une am61ioration de 4.7 ~t 8.5 dB sur les deux images utilis6es afin de tester la proc6dure. Pour des taux de compression comparables, les images cod6es avec LSLP sont de qualit+ subjective sup6rieure. Finalement, un lien th6orique est btabli entre l'approche pr6sente et la famille des repr6sentations pyramidales par filtres mirroirs en quadrature.
Introduction : Multi-resolution data representations are becoming increasingly popular in image processing applications. Pyramid data structures, in particular, play an important role in coding, and are ideally suited for progressive image transmission [13, 15] . In these data structures, the image is represented hierarchically with each level corresponding to a reduced-resolution approximation. An example of such a coding scheme is the Laplacian pyramid proposed by Burt and Adelson in which the difference between successive levels of a Gaussian pyramid is transmitted [3] . This approach compares favorably with earlier techniques, such as transform or predictive image coding, especially when large compression ratios are desired [7] . Recent developments in pyramid image compression also include subband coding techniques [ 18, 20] , orthogonal pyramid structures [1, 12] and wavelet transforms [9] , which are all based on the concept of quadrature mirror filters (QMF) [4] .
Introduction : The Laplacian pyramid coding technique described by Burt and Adelson relies on the use of two complementary functions: REDUCE and EXPAND. REDUCE computes a lower resolution level of the Gaussian pyramid by decreasing the resolution by a factor of two. EXPAND performs the reverse operation by mapping the coarser level onto a finer sampling grid. These two functions, as defined initially, were sub-optimal in two respects. First, the basic EXPAND function induces some image blurring, tending to increase the energy of the residual image. Second, the initial REDUCE function fails to minimize the loss of information (in the least squares sense) from one level to the next one. It will be shown here that these limitations can be corrected through the appropriate Signal Processing insertion of additional post-and pre-filtering modules. These operators have an infinite impulse response (IIR) and yet can be implemented very efficiently using simple forward and backward recursions, as discussed in Appendix A.
Introduction : The presentation is organized as follows. Following a series of definitions, a brief review of the Laplacian pyramid coding concept is given in Section 2. A modified EXPAND function that guarantees an exact image interpolation is described in Section 3. The least squares Laplacian pyramid is introduced in Section 4 and the corresponding REDUCE function is derived. The performance improvement of this new approach is illustrated both qualitatively and quantitatively with some experimental results in Section 5. Finally, the present approach is reinterpreted in terms of quadrature mirror filters in order to bring out the relationship with recent subband (or wavelet transform) coding techniques.
Conclusion : Two methods for improving the Laplacian pyramid proposed by Burt and Adelson for image coding have been described: (i) The EXPAND function has been redefined to ensure that the expansion of a coarser level onto a finer grid is an exact interpolation. (ii) An improved REDUCE function has been derived in order to minimize the loss of information occurring during resolution conversion.
Conclusion : It is easy to modify the initial scheme to incorporate these new functions. This is achieved by adding a pre-filter and a post-filter in the expansion and reduction modules, respectively. These filters can be coded very efficiently and the resulting increase of computations is moderate.
Conclusion : For lossless progressive data transmission, the performance improvement that can be achieved in this way is significant. The least squares scheme performs best according to the quantitative criteria used in this paper. Preliminary results suggest that this approach allows improved image coding according to the lossy scheme developed by Burt and Adelson. The least squares pyramid also stands as an interesting alternative to the widely used Gaussian pyramid and should be useful in a variety of multi-resolution image processing algorithms. It has also been shown that the present approach can be linked to the family of QMF image pyramids (e.g., orthogonal pyramids, wavelet transforms, subband coders). 
Conclusion : The implementation of these elementary units is based on the decomposition of H(z; z,% into a sum of simple causal and anti-causal first order systems, as given by the right-hand side of (A.3). The corresponding recursive filter equations are also more economical to combine the individual scaling factors in (A.1) and (A.5) or (A.7) into a single multiplication at the end of the process. The relevant filter parameters for implementing some of the operators described in Sections 3 and 4 using this strategy are given in Table A . This approach is also applicable in higher dimensions through the successive use of the same onedimensional filter along the various dimensions of the data. For digital images there is no need for floating point data storage other than the onedimensional array(s) required by the basic onedimensional filtering module.
Conclusion : We note that the second equation is borrowed from the sum decomposition and is required to initialize the backward recursion correctly.
Conclusion : All operations in (A.7) (respectively (A.5)) are real, and it is necessary to use one (respectively two) one-dimensional real array(s) for storing the filtered sequences with sufficient precision to avoid a recursive propagation of errors. It is relatively straightforward to write a general subroutine that implements (A. 1) from a succession of simple convolutions of the form (A.5) or (A.7); no additional intermediate storage is necessary for this task. It is

2876847
ABSTRACT : All humans, animals, and plants are holobionts. Holobionts comprise the host and a myriad of interacting microorganisms-the microbiota. The hologenome encompasses the genome of the host plus the composite of all microbial genomes (the microbiome). In health, there is a fine-tuned and resilient equilibrium within the members of the microbiota and between them and the host. This relative stability is maintained by a high level of microbial diversity, a delicate bio-geographic distribution of microorganisms, and a sophisticated and intricate molecular crosstalk among the multiple components of the holobiont. Pathobionts are temporarily benign microbes with the potential, under modified ecosystem conditions, to become key players in disease. Pathobionts may be endogenous, living for prolonged periods of time inside or on the host, or exogenous, invading the host during opportunistic situations. In both cases, the end result is the transformation of the beneficial microbiome into a health-perturbing pathobiome. We hypothesize that probably all diseases of holobionts, acute or chronic, infectious or non-infectious, and regional or systemic, are characterized by a perturbation of the healthy microbiome into a diseased pathobiome.

2877021
ABSTRACT : Noise is omnipresent in biomedical systems and signals. Conventional views assume that its presence is detrimental to systems' performance and accuracy. Hence, various analytic approaches and instrumentation have been designed to remove noise. On the contrary, recent contributions have shown that noise can play a beneficial role in biomedical systems. The results of this literature review indicate that noise is an essential part of biomedical systems and often plays a fundamental role in the performance of these systems. Furthermore, in preliminary work, noise has demonstrated therapeutic potential to alleviate the effects of various diseases.
ABSTRACT : Further research into the role of noise and its applications in medicine is likely to lead to novel approaches to the treatment of diseases and prevention of disability.
Introduction : Albert Einstein discovered noise accidentally in 1905, when he observed that atoms move according to the Brownian molecular motion [1] . Following his discovery, numerous descriptions of physical and biological systems have made incidental reference to noise, without recognizing its essential contribution. Noise is often regarded as an unwanted component or disturbance to a system, even though it has a tremendous impact on many aspects of science and technology [1] , including medicine and biology. A typical example for such a statement is a field of engineering called signal processing. On one hand, many signal processing algorithms have been designed to remove noise from a system, since greater noise levels are associated with degraded performance of algorithms.
Introduction : On the other hand, noise has been shown to enhance system performance in many areas of signal * Ervin Sejdić is with the Department of Electrical and Computer Engineering, Swanson School of Enginering, University of Pittsburgh, Pittsburgh, PA, 15261, USA. E-mail: esejdic@ieee.org. Ervin Sejdić is the corresponding author.
Introduction : † Lewis A. Lipsitz is with Harvard Medical School, Beth Israel Deaconess Medical Center and Hebrew Senior Life, processing including stochastic optimization techniques, genetic algorithms, dithering, just to name a few. Similarly, another concept called stochastic resonance (SR), first proposed in 1981 (e.g., [2] , [3] ), describes a positive impact of noise in nonlinear systems. SR refers to the fact that at an optimal level of input noise, signal detection is enhanced [4] , [5] . SR is observed in both man-made and naturally occurring nonlinear systems [6] . For example, paddlefish were shown to use SR to locate and capture prey, implicating this phenomenon in animal behavior [7] . Also, small noisy input can influence the firing patterns of squid axons [8] , enhance breathing stability in pre-term infants [9] , improve postural control in human aging, stroke or peripheral neuropathy [10] , [11] , and stabilize gait in elderly people with recurrent falls [12] .
Introduction : The intent of this manuscript is to inform researchers from multiple scientific disciplines that noise (i.e., stochastic processes) is a critical component of many biological and physiological systems that may be exploited in the future to develop interventions for the prevention and treatment of diseases. In other words, this manuscript is a crossover between a review paper and a position paper and as such is meant to initiate further discussions about the role of stochastic processes in modeling of physiological systems.

2877038
ABSTRACT : A common pitfall of many proposals on new informationcentric architectures for the Internet is the imbalance of upfront costs and immediate benefits. If properly designed and deployed, information-centric architectures can accommodate the current Internet usage which is at odds with the historical design of the Internet infrastructure. To address this concern, we focus on prospects of incremental adoption of this paradigm by introducing a peer-to-peer based transport protocol for content dissemination named Swift that exhibits properties required in an Information-Centric Network (ICN), yet can be deployed in the existing Internet infrastructure. Our design integrates components while highly prioritizing modularity and sketches a path for piecemeal adoption which we consider a critical enabler of any progress in the field.
INTRODUCTION : In this paper, we introduce Swift protocol. Swift was originally designed to be a replacement for the BitTorrent protocol and inherits some of the characteristics that have made BitTorrent successful, but was not intentionally designed according to the principles of information-centric networking (ICN) paradigm. It is, thus, until now a product of (unintentional) evolution towards ICN that we now seek to direct and accelerate, while retaining all the properties that make it work well on top of the existing network.
INTRODUCTION : We find the ICN concept to be increasingly reflected in both the way Internet is being used and in how Internetbased services are being implemented today. In many cases, we find that the problems we struggle with in the current * Work by Victor Grishchenko was carried out while at the Technical University of Delft incarnation of the Internet are those that ICN design proposals seek to address.
INTRODUCTION : For example, over 90% of today's Internet bandwidth [4] is effectively devoted to disseminating static multimedia content. In order to do so to an increasingly large and geographically diverse audience, various approaches are used. For example, for web-based content, Content Delivery Networks (CDNs) like Akamai use modified DNS servers that generate responses based on the topological/geographical location of the requester. These "tricks" are there to achieve on the current Internet the features that are at the core of ICN.
INTRODUCTION : During the past years, a chain of new network architectures based on the information-centric paradigm (also content-centric, name-oriented, named-data) have been proposed, including CCN [17] , DONA [20] , NetInf [11] , secure naming by Wong et al [23] , and content-centric router by Arianfar et al [6] to address the limitations of IP, namely the inability to decouple data from storage, inefficient data dissemination, lack of support for middleboxes, ubiquitous availability of data, and security.
INTRODUCTION : The historical conversation-centric end-to-end model, embodied in the TCP/IP stack, is based on message exchange between pairs of peers, typically servers and clients. On the other hand, ICN is a paradigm in which focus shifts away from the mechanics of moving bits between peers (endhosts). Instead, the focus in on the information itself, and the underlying network only a conduit for the information. In a sense, named-data network breaks with the end-to-end abstraction, as there are no ends and the entire network is considered a cloud, which both stores and serves data.
INTRODUCTION : Similarly, we can see in the evolution of peer-to-peer (P2P) file-sharing technologies how they have adopted ways of managing content that more and more look like ICN. While BitTorrent [10] has always used a SHA-1 hash of the content data to identify that unique content item, it used to be that you also needed a location identifier (the address of the tracker through which the peers hosting the content can be located). However, the current incarnation of BitTorrent instead uses a shared global Distributed Hash Table (DHT) to locate peers using the aforementioned content hash as the key.
INTRODUCTION : The historical Usenet discussion system [5] had all the key information-centric features: logical namespace and unique message identifiers, flood message propagation and caching. The git [3] revision control system represents version history of a project as a directed acyclic graph of revisions, where every revision is identified with SHA-1 hash of its contents; repositories push and pull content, thus forming a network of arbitrary topology.
INTRODUCTION : This tendency towards information-centricity implies a strong demand for a generic named-data substrate that is not reflected yet in the de jure network architecture. Given this dissonance, some have proposed a networking revolution to dethrone the Internet Protocol (IP) in favor of a cleanslate redesign of the networking infrastructure.
INTRODUCTION : While intellectually attractive, we do not consider such an approach realistic. Not only because it would require the expensive and disruptive wholesale replacement of the existing infrastructure, but more importantly because such a migration is unlikely to happen before the wholesale conversion of applications to information-centric analogues of the current application ecosystems, and that conversion is unlikely to happen until the required infrastructure is in place.
INTRODUCTION : Having made this observation, it seems clear that evolution, not revolution, is the best way towards ICN, and by recognizing and helping this along, we can both make ICN happen sooner and ensure that the ICN approach will be one tested in both lab and real-world settings, and hence "the fittest".
INTRODUCTION : We start with the necessary basic properties of any information centric architecture and determine which of them Swift already supports. Further, we determine which information centric primitives Swift does not provide and address them by leveraging existing technologies, such as DHTs to find peers and standard IP to route packets.
INTRODUCTION : In this paper, we argue that our modular design addresses the gap between Internet usage and the underlying network, without requiring clean-slate redesigning of the architecture. In particular, Swift protocol supports most properties proposed in information-centric architectures like CCN [17] , DONA [20] , and NetInf [11] . First, Swift uses names -flat identifiers -to request content instead of end-point addresses; in addition, it segments named objects in uniquely identified chunks. Second, Swift employs perpacket integrity check, enabling any peer in the network to cache and relay content and verify the integrity of each piece. Third, Swift avoids transmitting additional metadata and is suitable for live/mutable data, by employing Merkle hashes [21] .
INTRODUCTION : Moreover, Swift is a receiver-driven chunk-level transport protocol; the receiver may send concurrent requests for chunks to multiple peers in the network in order to enhance its content retrieval rate. To efficiently exploit available bandwidth, Swift employs a delay-based congestion control algorithm named LEDBAT [22] , and to address the issue of middleboxes Swift employs a NAT hole punching mechanism. For peer discovery, Swift can use centralized trackers or DHTs; in Section 7 we explain how Mainline DHT (MDHT) can be used to find peers offering the given data object in sub-second time periods.
INTRODUCTION : The paper proceeds as follows. In Section 2 we introduce ICN related work and summarize system description. Section 3 discusses design properties and the resulting separation of transport and internetworking layers. Section 4 describes our variation of the Merkle hashing scheme and its extensions. In Section 5 we introduce a vocabulary of messages that constitutes our protocol. Section 6 describes our UDP-based implementation. Section 7 discusses the implications for peer discovery and packet routing. Section 8 concludes.
CONCLUSION : In this paper, we presented a peer-to-peer based transport protocol for content dissemination named Swift and argued that the protocol exhibits ICN properties that help close the gap between the way Internet applications are used today and the underlying infrastructure supporting such applications. Further, we explored ways Swift may embed additional ICN properties in its behavior by leveraging existing technologies and infrastructure, such as decentralized peer discovery mechanisms and standard IP routing.

2877272
ABSTRACT : Background Arteriovenous malformations (AVMs) of the brain are commonly treated in multimodality fashion, with endovascular embolization followed by surgical extirpation being one of the most effective strategies. Modern endovascular suites enable rotational angiography, also known as cone-beam CT angiography (CBCT-A), using the full capability of modern C-arm digital angiography systems. This imaging modality offers a superior image quality to current options such as digital subtraction angiography, MRI, or CT angiography. Preoperative planning can be greatly aided by the resolution of angioarchitecture seen in CBCT-A images. Furthermore, these images can be used for intraoperative neuronavigation when integrated with widely used frameless stereotactic systems. The utility and outcome of the use of CBCT-A for preoperative planning and intraoperative localization of AVMs was evaluated. Methods A retrospective review was performed of 16 patients in which CBCT-A was performed, including radiological review and all clinical data. Results CBCT-A was successfully employed in all cases including those with (n=9) and without (n=7) rupture. Complete resection confirmed by postoperative angiography was achieved in all cases. Conclusions We present a novel application of CBCT-A in the treatment of AVMs, both for preoperative surgical planning and an intraoperative reference during neuronavigation.
INTRODUCTION : Arteriovenous malformations (AVMs) of the brain are commonly treated in multimodality fashion, with endovascular embolization followed by surgical extirpation being one of the most effective strategies. Embolization can reduce blood loss and minimize complications associated with microsurgery for AVMs. Frameless stereotactic image guidance can also be used to make AVM resection safer. Image guidance has become standard in cranial neurosurgery, increasing the accuracy of craniotomy and allowing for smaller scalp incisions and bone flaps. Although image guidance for AVM surgery has been previously described in the literature, the optimal timing and imaging modality have not been established.
INTRODUCTION : The capabilities of modern angiographic platforms have recently improved substantially. Two-dimensional (2D) digital subtraction angiography (DSA) can now be enhanced by 3D, functional, and axial-anatomic adjunctive technologies. One such technology, rotational angiography, also known as cone-beam CT angiography (CBCT-A), uses the full capability of modern C-arm digital angiography systems available in neuroendovascular suites. An angiographic imaging study immediately prior to surgical resection can provide important information to a cerebrovascular surgeon. In this study we review a series of cases in which CBCT-A was performed preoperatively, either with or without an endovascular intervention, to assist in the image-guided surgical resection of cerebral AVMs. The use of CBCT-A has been tested in laboratory/cadaveric studies by other groups; 1 we report its use in a series of patients at two different institutions.
CONCLUSION : We present a novel application of CBCT-A in the treatment of AVMs, both for preoperative surgical planning and an intraoperative reference during neuronavigation. The additional resolution of AVM angioarchitecture provided by this imaging as well as the efficient workflow inherent in the technology hold promise for further and better integration of the traditional dichotomy ('open' and 'endovascular') in cerebrovascular treatments. Funding This research received no specific grant from any funding agency in the public, commercial or not-for-profit sectors.

2877363
ABSTRACT : Linker length and composition were varied in libraries of single-chain Arc repressor, resulting in proteins with effective concentrations ranging over six orders of magnitude (10 M-10 M). Linkers of 11 residues or more were required for biological activity. Equilibrium stability varied substantially with linker length, reaching a maximum for glycine-rich linkers containing 19 residues. The effects of linker length on equilibrium stability arise from significant and sometimes opposing changes in folding and unfolding kinetics. By fixing the linker length at 19 residues and varying the ratio of Ala͞Gly or Ser͞Gly in a 16-residue-randomized region, the effects of linker f lexibility were examined. In these libraries, composition rather than sequence appears to determine stability. Maximum stability in the Ala͞Gly library was observed for a protein containing 11 alanines and five glycines in the randomized region of the linker. In the Ser͞Gly library, the most stable protein had seven serines and nine glycines in this region. Analysis of folding and unfolding rates suggests that alanine acts largely by accelerating folding, whereas serine acts predominantly to slow unfolding. These results demonstrate an important role for linker design in determining the stability and folding kinetics of single-chain proteins and suggest strategies for optimizing these parameters.
ABSTRACT : The construction of single-chain or hybrid proteins is a potentially powerful method for generating proteins with novel functions and improved properties (1-11). A critical element in such efforts is the design of the peptide linkers that serve to connect different protein domains or subunits. Designed linkers are usually glycine-based peptides with lengths calculated to span the minimum distance between the C terminus of one subunit or domain and the N terminus of the next. How important is linker design in determining the properties of single-chain proteins? Alterations in linker regions have been found to affect the stability, oligomeric state, proteolytic resistance, and solubility of single-chain proteins (12-23), but few systematic investigations of these relationships have been reported. Here, we test the effects of linker design on the stability, protein folding kinetics, and biological activity of single-chain Arc repressor. Wild-type Arc is a dimer with identical subunits, and Arc-L1-Arc is a single-chain variant with a 15-residue linker connecting the subunits (see Fig. 1 ). The L1 linker of Arc-L1-Arc holds the subunits at an effective concentration (C eff ) of 3 mM. By varying linker length and composition, we have isolated single-chain variants with effective subunit concentrations ranging from 10 M to 10 M, corresponding to changes in the free energy of unfolding (⌬G u ) from 3 to 11 kcal͞mol. These differences in stability arise from changes in the folding and unfolding rates, suggesting that linker design can affect protein stability by altering the free energies of both the native and denatured states.

2877521
ABSTRACT : Regression aims at estimating the conditional mean of output given input. However, regression is not informative enough if the conditional density is multimodal, heteroskedastic, and asymmetric. In such a case, estimating the conditional density itself is preferable, but conditional density estimation (CDE) is challenging in high-dimensional space. A naive approach to coping with high dimensionality is to first perform dimensionality reduction (DR) and then execute CDE. However, a two-step process does not perform well in practice because the error incurred in the first DR step can be magnified in the second CDE step. In this letter, we propose a novel single-shot procedure that performs CDE and DR simultaneously in an integrated way. Our key idea is to formulate DR as the problem of minimizing a squared-loss variant of conditional entropy, and this is solved using CDE. Thus, an additional CDE step is not needed after DR. We demonstrate the usefulness of the proposed method through extensive experiments on various data sets, including humanoid robot transition and computer art.
Introduction : Analyzing an input-output relationship from samples is one of the central challenges in machine learning. The most common approach is regression, which estimates the conditional mean of output y given input x. However, just analyzing the conditional mean is not informative enough, when the conditional density p(y|x) possesses multimodality, asymmetry, and heteroskedasticity (i.e., input-dependent variance) as a function of output y. In such cases, it would be more appropriate to estimate the conditional density itself (see Figure 2 ). The most naive approach to conditional density estimation (CDE) would be -neighbor kernel density estimation ( -KDE) , which performs standard KDE along y only with nearby samples in the input domain. However, -KDE does not work well in high-dimensional problems because the number of nearby samples is too few. To avoid the small sample problem, KDE may be applied twice to estimate p(x, y) and p(x) separately and the estimated densities may be plugged into the decomposed form p(y|x) = p(x, y)/p(x) to estimate the conditional density. However, taking the ratio of two estimated densities significantly magnifies the estimation error and thus is not reliable. To overcome this problem, an approach to directly estimating the density ratio p(x, y)/p(x) without separate estimation of densities p(x, y) and p(x) has been explored (Sugiyama et al., 2010) . This method, called least-squares CDE (LSCDE), was proved to possess the optimal nonparametric learning rate in the mini-max sense, and its solution can be efficiently and analytically computed. Nevertheless, estimating conditional densities in high-dimensional problems is still challenging.
Introduction : A natural idea to cope with the high dimensionality is to perform dimensionality reduction (DR) before CDE. Sufficient DR (Li, 1991; Cook & Ni, 2005 ) is a framework of supervised DR aimed at finding the subspace of input x that contains all information on output y, and a method based on conditional-covariance operators in reproducing kernel Hilbert spaces has been proposed (Fukumizu, Bach, & Jordan, 2009) . Although this method possesses superior theoretical properties, it is not easy to use in practice because no systematic model selection method is available for kernel parameters. To overcome this problem, an alternative sufficient DR method based on squared-loss mutual information (SMI) has been proposed recently (Suzuki & Sugiyama, 2013) . This method involves nonparametric estimation of SMI that is theoretically guaranteed to achieve the optimal estimation rate, and all tuning parameters can be systematically chosen in practice by cross-validation with respect to the SMI approximation error.
Introduction : Given such state-of-the-art DR methods, performing DR before LSCDE would be a promising approach to improving the accuracy of CDE in highdimensional problems. However, such a two-step approach is not preferable because DR in the first step is performed without regard to CDE in the second step, and thus small errors incurred in the DR step can be significantly magnified in the CDE step.
Introduction : In this letter, we propose a single-shot method that integrates DR and CDE. Our key idea is to formulate the sufficient DR problem in terms of the squared-loss conditional entropy (SCE), which includes the conditional density in its definition, and LSCDE is executed when DR is performed. Therefore, when DR is completed, the final conditional density estimator has already been obtained without an additional CDE step (see Figure 1 ). We demonstrate the usefulness of the proposed method, named least-squares conditional entropy (LSCE), through experiments on benchmark data sets, humanoid robot control simulations, and computer art. 
Conclusion : We proposed a new method for conditional-density estimation in highdimension problems. The key idea of the proposed method is to perform sufficient dimensionality reduction by minimizing the square-loss conditional entropy (SCE), which can be estimated by least-squares conditional-density estimation. Thus, dimensionality-reduction and conditional-density estimation are carried out simultaneously in an integrated manner.
Conclusion : We have shown that SCE and the squared-loss mutual information (SMI) are similar but different in that the output density is included in the denominator of the density ratio in SMI. This means that estimation of SMI is hard when the output density is fluctuated, while the proposed method using SCE does not suffer from this problem. The proposed method is also robust against outliers since minimization of the Pearson divergence automatically weighs down the effects of outlier points. Moreover, the proposed method is applicable to multivariate output data, which is not straightforward to handle in other dimensionality-reduction methods based on conditional probability density. The effectiveness of the proposed method was demonstrated through extensive experiments, including humanoid robot transition and computer art. Using ∂X −1 ∂ h k ∂W l,l = − 1 σ 2 n n i=1 ϕ k (z i , y i ) ((z (l) i − u (l) k )(x (l ) i −ũ (l ) k )).

2878408
ABSTRACT : Breast cancer is among the most common cancers worldwide. Diabetes is an important chronic health problem associated with insulin resistance, increased insulin level, changes in growth hormones and factors, and activation of mitogen-activating protein kinase (MAPK) pathways, leading to an increased breast cancer risk. is paper looked at the epidemiologic studies of the association between type 2 diabetes and risk of breast cancer and its effect on overall cancer-speci�c survival. e combined evidence overall supported a modest association between type 2 diabetes and the risk of breast cancer, which was found to be more prevalent among postmenopausal women. Effect of oral diabetics and insulin therapy on breast cancer risk was also evaluated. It was found that metformin and thiazolidinones tended to have a protective role. Metformin therapy trials for its use as an adjuvant for breast cancer treatment are still ongoing. Sulfonylurea and insulin therapy were found to be mildly associated with increased overall cancers. No evidence or studies evaluated the association of DPPIV inhibitors and GLP 1 agonists with breast cancer risk because of their recent introduction into the management of diabetes.
Introduction : Breast cancer is among the most common cancers worldwide and is the second leading cause of cancer death for women in the United States, aer lung cancer, with an estimated incidence of 226,870 cases and estimated deaths of 39, 510 cases in the year of 2012. e National Cancer Institute also estimated that 1 in 8 women in the United States has the chance of developing invasive breast cancer throughout her lifetime [1, 2] . Diabetes is also a very common chronic health problem where it is currently estimated that 10% of women in the United States over the age of 20 have type 2 diabetes. Prevalence of diabetes has steadily increased since 1990. e 2010 CDC study projected that by 2050, as many as one of three US adults could have diabetes if the current trend continues [3, 4] . Association between diabetes and breast cancer has been noted where 16% of older breast cancer patients were found to suffer from diabetes, and this might have important public health implications.

2878772
ABSTRACT : This guideline has been discussed by the SOSORT guideline committee prior to the SOSORT consensus meeting in Milan, January 2005 and published in its first version on the SOSORT homepage: http://www.sosort.org/meetings.php. After the meeting it again has been discussed by the members of the SOSORT guideline committee to establish the final 2005 version submitted to Scoliosis, the official Journal of the society, in December 2005.
ABSTRACT : Scoliosis is defined as a lateral curvature of the spine with torsion of the spine and chest as well as a disturbance of the sagittal profile [2] .
ABSTRACT : Idiopathic scoliosis is the most common of all forms of lateral deviation of the spine. By definition, it is a lateral curvature of the spine in an otherwise healthy child, for which a currently recognizable cause has not been found. Less common but better defined etiologies of the disorder include scoliosis of neuromuscular origin, congenital scoliosis, scoliosis in neurofibromatosis, and mesenchymal disorders like Marfan's syndrome [3] .
ABSTRACT : The prevalence of adolescent idiopathic scoliosis (AIS), when defined as a curvature greater than 10° according to Cobb, is 2-3%. The prevalence of curvatures greater than 20° is between 0.3 and 0.5%, while curvatures greater than 40° Cobb are found in less than 0.1% of the population. All etiologies of scoliosis other than AIS are encountered more rarely [4] .
ABSTRACT : The anatomical level of the deformity has received attention from clinicians as a basis for scoliosis classification. The level of the apex vertebra (i.e., thoracic, thoracolumbar, lumbar or double major) forms a simple basis for description. In 1983, King and colleagues [5] classified different curvature patterns by the extent of spinal fusion required; however, recent reports have suggested that

2879242
Introduction : The potential role of Human cytomegalovirus (hCM) infection in promoting neoplasia is an active area of scientific research. [1] Although still controversial, there is a growing body of evidence that links hCMV infection to a variety of malignancies, including those of the breast, prostate, colon, lung and brain (gliomas). [2] [3] [4] [5] [6] [7] hCMV induces alterations in regulatory proteins and non-coding RNA that are associated with a malignant phenotype. These changes promote tumour survival by effecting cellular proliferation, invasion, immune evasion, and production of angiogenic factors [8] Constant immune surveillance governs the destruction of the majority of cancer cells and precancerous conditions in the human body. However, the most pathogenic of malignant tumors acquire immune evasion strategies which render them less vulnerable to destruction by immune cells.
Introduction : The characteristic hallmarks of a malignant cell include:
Introduction : 1. sustaining proliferative signaling and evading growth suppressors, 2. resisting cell death and enabling replicative immortality, 3. inducing angiogenesis, activating invasion and metastasis. [9] In cancers which are not attributable to infectious agents, chronic inflammation may also play a critical role in the transition from a precancerous condition to invasive malignancy. Inflammation is the seventh hallmark of neoplasia (Table 1) . [10] During chronic inflammation, certain "promoters," such as hepatitis C virus and Epstein-Barr virus (EBV), may facilitate the transformation of a pre-malignant condition to neoplasia. [11, 12] Cancer "promoters" are agents that, by themselves, may not have a significant oncogenic impact on normal cells but can drive precancerous cells towards neoplasia. 
Conclusions : Significant advances have been made in understanding the roles of chronic inflammation, tumor microenvironment, cancer stem cells, tumor immunology, and infectious agents in the pathobiology of cancer. Several clinical and experimental findings suggest that hCMV may play a role in promoting certain cancers. In cells that are persistently infected with hCMV, the expression of viral proteins may prevent the immune system from identifying or removing these cells, thereby offsetting immune detection of transformed cells. The effects of hCMV in promoting tumor cell immune evasion may prove important in development of cancer immunotherapies, particularly if the hCMV-infected cells are resistant to the action of cytolytic peptides released by activated NK and cytotoxic T-cells. Also, if viral proteins that inhibit apoptosis are expressed by hCMV infected tumour cells, the cancer cells may be less susceptible conventional chemotherapeutic agents. Whether hCMV is ultimately established as an oncogenic virus will require additional research in the areas of virology, epidemiology and molecular oncology, and systematic refinement of the concept of "oncomodulation." Insights into the role of hCMV in oncogenesis may increase understanding of cancer biology and promote development of novel therapeutic strategies. 

2879621
INTRODUCTION : In a recent paper (Sabelis and Bakker, 1992) we calculated the optimal number of dorsal setae required to minimize somal contact with sticky structures, such as silken threads forming the chaotically structured web spun by two-spotted spider mites. This calculation was based on two vital assumptions: (1) full avoidance of somal contact is achieved when the imaginary central axis is fully covered by the projections of the dorsal setae on this axis; and (2) investment per seta is proportional to setal length to the power 3 (which holds only when the seta is coneshaped and the setal diameter-length ratio is constant). While the first assumption is a gross oversimplification, the second assumption lacks support by hard data. In this paper we will ( 1 ) extend the optimality model to include a more general class of diameter-length relations with respect to dorsal setae, (2) present measurements of diameter-length relations in various genera of the Phytoseiidae and (3) predict the optimal number of setae for comparison with the actual number of dorsal setae. This test of the model will then be used to determine wbether the number of dorsal setae in phytosciid mites can be considered as an adaptation or a phylogenetic constraint.

2879698
ABSTRACT : The demonstrated modified spectrophotometric method makes use of the 2,2-diphenyl-1-picrylhydrazyl (DPPH) radical and its specific absorbance properties. The absorbance decreases when the radical is reduced by antioxidants. In contrast to other investigations, the absorbance was measured at a wavelength of 550 nm. This wavelength enabled the measurements of the stable free DPPH radical without interference from microalgal pigments. This approach was applied to methanolic microalgae extracts for two different DPPH concentrations. The changes in absorbance measured vs. the concentration of the methanolic extract resulted in curves with a linear decrease ending in a saturation region. Linear regression analysis of the linear part of DPPH reduction versus extract concentration enabled the determination of the microalgae's methanolic extracts antioxidative potentials which was independent to the employed DPPH concentrations. The resulting slopes showed significant differences (6 -34 µmol DPPH g -1 extract concentration) between the single different species of microalgae (Anabaena sp., Isochrysis galbana, Phaeodactylum tricornutum, Porphyridium purpureum, Synechocystis sp. PCC6803) in their ability to reduce the DPPH radical. The independency of the signal 
Introduction : In oxygen involving metabolisms like photorespiration and photosynthesis, reactive oxygen species (ROS) are natural byproducts (He and Häder 2002, Apel and Hirt 2004) in phototrophic microorganisms. Typical ROS are e.g. superoxide, hydroxyl, peroxyl, and alkoxy radicals. Under nonstressed conditions, production and scavenging of ROS in these microorganisms is in equilibrium (Apel and Hirt 2004) .
Introduction : Different environmental stress factors like pollution, drought, temperature, excessive light intensities, and nutritional limitation are able to increase the production of ROS (Ehling-Schulz and Scherer 1999, Rijstenbil 2002 , Arora et al. 2002 . Oxidative stress is closely associated to these unstable but very reactive radicals (Fang et al. 2002) . Their highly reactive potential is discussed to be responsible for some human diseases e.g. cancer and cardiovascular diseases and is able to cause oxidative damages to proteins, DNA, and lipids (Jacobi and Burri, 1996) in both humans and microorganisms.
Introduction : Microalgae have to counteract these negative effects by diverse effective enzymatic and nonenzymatic mechanisms (Apel and Hirt, 2004) . Several enzymes like superoxide dismutase, catalase peroxidase are able to scavenge ROS (Blokhina et al. 2003) . Carotenoids and fatty acids are two examples for non-enzymatic classes of substances which are able to protect the organism from oxidative damage (Sies and Stahl, 1995) . Tocopherol, flavonoids, and alkaloids are other examples for substances belonging to this group of non-enzymatic substances.
Introduction : Two major but different mechanisms are known (Prior et al. 2005 ): Both mechanisms lead to a reduction of the radicals but differ in kinetics and propensity for side reactions (Prior et al. 2005 ). For hydrogen atom transfer, the antioxidants quench the free radicals by donating a hydrogen whereas for single electron transfer the antioxidants transfer one electron to the radical.
Introduction : For estimating the antioxidative potential of chemical components, different experimental approaches were used (Prior et al. 2005 ). Most of them require a spectrophotometric measurement and a certain reaction time in order to obtain reproducible results (Kulisic et al. 2004) .
Introduction : For example, the ß-carotene bleaching test (BCB) is based on the decolorization of ß-carotene by its reaction with radicals. This effect is measured at a wavelength of 470 nm after a reaction time of nearly 120 min. Other methods like the 2,2-diphenyl-1-picrylhydrazyl (DPPH) radical scavenging method or the thiobarbituric acids reactive species (TBARS) assay work similar to the BCB test.
Introduction : The TBARS assay uses the production of a pink pigment produced by the reaction of thiobarbituric acid (TBA) with malondialdehyd (MDA) and other secondary lipid peroxidation products (Kulisic et al. 2004) . Absorbance measurements at 532 nm serve as an indicator of the extent of lipid degradation.
Introduction : The use of DPPH for a radical scavenging measuring method is described e.g. by Yen and Duh (1994) , Yordanov and Christova (1997), Masuda et al. (1999) , Anderson and Padhye (2004) , and Iwashima et al. (2005) . DPPH is a stable free radical in a methanolic solution. In its oxidized form, the DPPH radical has an absorbance maximum centered at about 520 nm (Molyneux, 2004) . The DPPH method is described as a simple, rapid and convenient method independent of sample polarity for screening of many samples for radical scavenging activity (Koleva et al. 2001 ). These advantages make the DPPH method interesting for testing microalgae as a natural source to scavenge radicals and to find out promising candidates for a commercial sense.
Introduction : Usually, the DPPH absorbance is measured at a wavelength of 515 -520 nm (Bandoniene et al. 2002 , Pavlov et al. 2002 , Gazi et al. 2004 ). But here, demonstrated in Figure 1 , an applied modification of this DPPH-method is described enabling the measurement of the antioxidative potential of the microalgae's specific methanolic extracts. Spolaore et al. 2006 ). The demonstrated modified DPPH-method enables therefore the screening of microalgae as promising candidates in a commercial sense. Furthermore, the modified method can be used, e.g. if the aim of future investigations is the detection of the antioxidative potential of some microalgae influenced by different environmental parameters.

2879848
ABSTRACT : The vestibular system analyses angular and linear accelerations of the head that are important information for perceiving the location of one's own body in space. Vestibular stimulation and in particular galvanic vestibular stimulation (GVS) that allow a systematic modiWcation of vestibular signals has so far mainly been used to investigate vestibular inXuence on sensori-motor integration in eye movements and postural control. Comparatively, only a few behavioural and imaging studies have investigated how cognition of space and body may depend on vestibular processing. This study was designed to diVerentiate the inXuence of left versus right anodal GVS compared to sham stimulation on object-based versus egocentric mental transformations. While GVS was applied, subjects made leftright judgments about pictures of a plant or a human body presented at diVerent orientations in the roll plane. All subjects reported illusory sensations of body self-motion and/ or visual Weld motion during GVS. Response times in the mental transformation task were increased during right but not left anodal GVS for the more diYcult stimuli and the larger angles of rotation. Post-hoc analyses suggested that the interfering eVect of right anodal GVS was only present in subjects who reported having imagined turning themselves to solve the mental transformation task (egocentric transformation) as compared to those subjects having imagined turning the picture in space (object-based mental transformation). We suggest that this eVect relies on shared functional and cortical mechanisms in the posterior parietal cortex associated with both right anodal GVS and mental imagery.
Introduction : The vestibular system detects angular and linear accelerations of the head in space. Even though most previous studies on the human vestibular system focus on sensori-motor control of eye movements and posture, various studies in patients and healthy human subjects also suggest an important contribution of the vestibular system to cognitive aspects such as spatial and bodily cognition. Nevertheless, the underlying mechanisms are still largely unknown.
Introduction : Clinical evidence suggests that peripheral vestibular loss leads to deWcits in spatial cognition such as spatial navigation, learning or memory abilities Smith et al. 2005) . Thus, patients with unilateral peripheral vestibular loss suVer from deWcits in path integration during active goal-directed locomotion (Glasauer et al. 2002; Peruch et al. 2005 ) and navigation in virtual environments (Peruch et al. 1999 (Peruch et al. , 2005 suggesting that vestibular processing contributes to spatial cognition. Vestibular mechanisms have also been shown to be important for own body processing. For example a functional and anatomical relationship between spatial neglect subsequent to right hemispheric brain damage-leading to deWcits in spatial and bodily processing-and vestibular disturbances has previously been discussed (for an overview see Karnath and Dietrich 2006) . This link is supported by the fact that caloric vestibular stimulation and galvanic vestibular stimulation (GVS) may temporarily decrease symptoms of spatial neglect such as rightwards bias in visuo-spatial tasks (Cappa et al. 1987; Rode et al. 1992; Bottini et al. 2005) as well as symptoms of disturbed bodily awareness (Vallar 1998; Fink et al. 2003) . Vestibular dysfunctions have also been reported in neurological patients with disturbed own body perceptions due to damage in the temporo-parietal cortex (Devinsky et al. 1989; Blanke et al. 2004) . Direct electrical stimulation of this region may also induce out-ofbody experiences (PenWeld and Erickson 1941; Blanke et al. 2002) as well as vestibular illusions (Blanke et al. 2002; Kahane et al. 2003) at similar stimulation sites. Collectively these Wndings suggest an important vestibular contribution to spatial and bodily processing (for review see Lenggenhager et al. 2006; Lopez and Blanke 2007) .
Introduction : Similarly, several studies have reported vestibular contributions to spatial and bodily processing in healthy subjects. Thus, it has been shown that visuo-spatial judgments such as line-bisection, visual vertical judgment and body orientation judgment Mars et al. 2005) , spatial memory (Bächtold et al. 2001) , and mental transformation (Mast and Meissner 2004; Mast et al. 2006) can be inXuenced by vestibular stimulation. Mast et al. (2006) showed that caloric vestibular stimulation leads to impaired performance in mental transformation tasks, but not in a control task using mental imagery.
Introduction : Here we will focus on mental transformation since it is a spatial cognitive ability that may also rely on bodily processing. At least two diVerent kinds of mental transformation have initially been described, object-based mental transformation (Shepard and Metzler 1971) and egocentric mental transformation (Parsons 1987) . Only a few studies have directly investigated the inXuence of vestibular processing on either type of mental transformation (Mast et al. 2006; Mast and Meissner 2004) . Mast and colleagues (2006) showed that performance in mental transformation is generally decreased during caloric vestibular stimulation. More interestingly for the scope of the present study, they showed that in an egocentric mental transformation task performance can be modiWed in a direction-speciWc way by vestibular stimulation during physical body rotations (Mast and Meissner 2004) . In this study, subjects were more accurate when the direction of physical rotation and egocentric mental transformation were congruent, suggesting that egocentric mental transformation shares mechanisms with physical body rotation. Thus, both actual and mental body transformation seem to rely on vestibular cues. This is further corroborated by the Wnding that egocentric mental transformations (imagined sensation of body motion) may induce a direction-speciWc vestibulo-ocular reXex (Rodionov et al. 2004) and that performance in mental transformation of pictures of human bodies and body parts decreases in microgravity (Grabherr et al. 2007 ). Collectively, these data suggest that egocentric mental transformation depends partly on vestibular processing.
Introduction : The present study was designed to investigate the eVects of GVS on mental transformations. First, we were interested whether there is an overall decrease in performance in mental transformation tasks during GVS as compared to sham stimulation. Mast et al. (2006) reported impaired performance in a mental transformation task, but not in a non-spatial control task during caloric vestibular stimulation. GVS may interact with mental transformation due to overlapping and interfering neural mechanisms between GVS and mental transformation. The comparison to sham stimulation was chosen to control for attentional eVects due to skin/pain sensation. Second, we also investigated whether the direction of the illusory body motion induced by right and left GVS inXuences diVerently clockwise and counterclockwise mental transformations. Using binaural bipolar GVS it is possible to evoke illusory body motion to the right or the left by reversing electrode polarity (Fitzpatrick and Day 2004) . Therefore, based on the results of Mast and Meissner (2004) , we hypothesized that mental transformation performance might improve when the direction of the illusory body motion and mental transformation are congruent, but deteriorate when incongruent. Third, we investigated whether object-based and egocentric mental transformations are diVerently inXuenced by GVS. Previous literature suggests that subjects tend to use an object-based mental transformation (imagined rotation of the picture in space) when pictures of non-human objects are presented, but use an egocentric perspective-based mental transformation (imagined turning of oneself in space) for pictures of human bodies (Zacks and Tversky 2005) . Based on these results and observations that egocentric perspective-based mental transformation seems to interact with vestibular processing (Rodinov et al. 2004; Mast and Meissner 2004) , we hypothesized that the eVect of GVS would be stronger for pictures representing a human body than a non-human object. Finally, we were interested whether left and right GVS would inXuence mental transformation diVerently. Fink et al. (2003) found diVerent cerebral activation patterns for left versus right GVS. During right anodal stimulation they found bilateral activations in superior temporal, posterior insular and inferior parietal regions as well as right lateral occipito-parietal activations, whereas activations during left anodal GVS were conWned to the right hemisphere only (superior temporal gyrus, posterior insular cortex, anterior inferior parietal cortex). Therefore, based on the above-mentioned assumption of common and interfering neural mechanisms we hypothesised that left and right GVS could diVerently inXuence cortical mental transformation processes.
Conclusion : Although the interaction between GVS and mental transformation was not as strong as assumed based on previous literature, this study reveals novel results concerning the contribution of the vestibular system to high level spatial and bodily processing. The results suggest that GVS may impair demanding mental transformation tasks. This is mainly true for right GVS and for egocentric mental transformation, suggesting a shared neural processing in the posterior parietal cortex.
Conclusion : Based on these results several implications for further studies can be derived: Studies on the eVects of GVS and bodily and spatial processing should consider sham stimulation as it is routinely used in transcranial magnetic stimulation studies. Task complexity should generally be high enough in order to observe potential GVS eVects, and these eVects should further be investigated by systematically manipulating task diYculty. Since we found diVerences between left and right GVS, future studies should also investigate GVS over both mastoids separately. Moreover, for mental transformation tasks it would be interesting to assess and manipulate mental transformation strategies, since they seem to rely on diVerent neural processes. Finally, to test our hypothesis of a shared brain mechanism between right GVS and mental transformation it would be important to combine neuroimaging with GVS as done previously to demonstrate this common neural substrate between GVS and line bisection .

213466066
ABSTRACT : Improvement in Aerodrome Warning (AW) nowcasts need better prediction for supporting the safety and security of air traffic from extreme weather. AW consists of weather conditions, wind direction and wind speed, and visibility with observing time and validity time of forecast. Weather forecast verification is important for all stakeholders in the airport, so they can prepare and have plans to mitigate undesirable activity disturbance. AW and Automated Weather Observing System (AWOS) data are from Soekarno-Hatta Meteorological Station (07L) and Tanjungpinang Meteorological Station for January to April2019. Statistic test using hits, false alarm, misses, the correct negative is to find the score of POD, Bias, FAR, TS, and HSS which is to measure the magnitude of AW. Overall, AW has good accuracy to predict the extreme weather in the aerodrome.
Introduction : Aviation meteorology is the most important aspect of supporting the safety and security of air traffic from extreme weather [1] . Weather conditions can cause or contribute to the aviation accidents included wind, visibility or ceiling, high-density altitude, turbulence, carburetor icing, updrafts or downdrafts, precipitation, icing, thunderstorms, wind shear, thermal lift, temperature (T) extremes, and lightning [2] . A weather forecaster is an actor who guarantees the efficiency and effectiveness of airport operational without affected by weather, so they have to observe all the weather parameters such as air temperature, winds, weather condition, and visibility. Furthermore, these parameters are analyzed in an isobar chart, streamline chart, and upper air chart to get an accurate weather forecast.
Introduction : Based on Figure 1 , accidents by flight phase as a percentage of all accidents from 1998 to 2017 have dominantly occurred with approach (up to 20%) and landing (up to 50%). Next, it was followed by parking and taxi as a non-fatal hull loss, but it was also essential. Building and cargo in aerodrome may be received damage from the weather such as floods or strong surface winds. To minimize the negative risks, World Meteorological Organization (WMO) has arranged the rules with the use of Aerodrome Warning in Technical Regulations [4] , Volume II, Part I, 7.3. In Indonesia, The Agency for MeteorologyClimatology and Geophysics (BMKG) also compiled the detailed of Aerodrome Warning in PERKA BMKG No. 13 Tahun 2015 [5] . Aerodrome Warning is concise information about meteorological conditions that can affect aircraft and airport service facilities on land such as runway. Aerodrome Warning (AW) consists of weather conditions, wind direction and wind speed, and visibility with observing time and validity time of forecast. Weather conditions as though rainfall, tropical cyclones, thunderstorm, squall, hail, fog, volcanic ash, tsunami, smoke, and toxic chemistry gases should be reported if it occurred or will occur in AW format. Delay avoiding, cargo activities will be fluent and aircraft parking will exists in safety cone and remains sterile if AW is disseminated well [6] .
Introduction : WIII AD WRNG 02 VALID 170420/170530 HVY TSRA WIND 28015KT MAX 25KT OBSAT 170400 NC= (1)
Introduction : The code form above explains about Aerodrome Warning in Soekarno-Hatta Meteorological Station (WIII) number 2 with time validity on date 17 from 04:20 UTC until 05:30 UTC will occur heavy rain with thunderstorm with the average wind direction is from 280⁰ (South-West) and wind speed is 15 knot, and maximum wind speed up to 25 knot. The observation of AW was 04:00 UTC on date 17, with no change of phenomena intensity. Improvement in aerodrome warnings nowcasts need better predictions, thus a verification becomes its measurement. Weather forecast verification provides benefits, such as knowing the mistake which causes false prediction [7] . In cases, all stakeholders in the airport can prepare and have plans to mitigate undesirable activity disturbance.
Introduction : Aerodrome warning archives from January to April 2019 are needed as the basic materials. A computer, an especially calculator, are used to calculate all formula in statistic verification. Automatic Weather Observation System (AWOS) data from January to April 2019 are collected in one folder including rainfall events, thunderstorm events, the peak of wind speed, and minimum visibility. Aerodrome warning and AWOS data are from Soekarno-Hatta Meteorological Station (07L) and Tanjungpinang Meteorological Station. 

147015306
ABSTRACT : Individuals can simulate and pre-experience the future events. The ability to image the plausible future is termed episodic future thinking. Particularly, it is important for us to construct the detailed scenario about future event. If we couldn't simulate it in detail, we may fail to prepare what is necessary in future or deal with the unexpected event in an appropriate manner. In present study, we investigate how the level of detail specified in episodic future thinking. We supposed two possibilities specifying level of detail in future thinking: one is the retrieval process from past episodic memory and the other is the recombination process of retrieved details into coherent representation. To investigate which process is important for specifying the level of detail, we draw on the association between level of detail and temporal distance. Level of detail of imaged future event was reflected in activation pattern of concept of future temporal distances, and retrieval of detail from episodic memory was reflected in concept of past temporal distance. The results show that details were retrieved from episodic memory when participants construct the detailed image of future events. We suggest that levels of detail in future events are specified in recombination process.

116985999
ABSTRACT : We reanalyzed the data from the Infrared Telescope in Space (IRTS) based on up-to-date observations of zodiacal light, integrated star light and diffuse Galactic light. We confirmed the existence of residual isotropic emission, which is slightly fainter, but at nearly the same level as previously reported. At wavelengths longer than 2 µm, our result is fairly consistent with recent observations with Japanese infrared astronomy satellite, AKARI. We performed all of our analyses using two different models of zodiacal light (Kelsall and Wright models). In both cases, we detect residual isotropic emission that is significantly brighter than the integrated light of galaxies (though slightly fainter in the case of the Wright model). Thus, we confirm the existence of excess near-infrared emission, independent of the zodiacal light model used. The spectral shape of the excess isotropic emission is similar to that of the recently observed spectrum of excess fluctuations, which suggests the excess brightness and fluctuations may arise from the same source.
Introduction : Observations of extragalactic background light (EBL) have been obtained over a wide range of wavelengths to examine the energy density of the universe. In particular, near-infrared EBL has been thought to provide important clues for our understanding of the early universe and the evolution of galaxies. Significant near-infrared isotropic emission, that cannot be explained with known foreground emission, was detected with the COsmic Background Explorer (COBE) (Cambrésy et al. 2001; Gorjian et al. 2000; Wright and Reese 2000; Levenson et al. 2007 ) and the InfraRed Telescope in Space (IRTS) (Matsumoto et al. (2005) , hereafter referred to as Paper I). Recent AKARI observations (Tsumura et al. 2013d ) also showed a result consistent with COBE and IRTS at wavelengths longer than 2 µm.
Introduction : Due to the recent discovery of large excess fluctuations in the near-infrared sky (Kashlinsky et al. 2005 (Kashlinsky et al. , 2007a Matsumoto et al. 2011; Zemcov et al. 2014) , the excess background emission in the near-infrared has attracted more interest.
Introduction : The result of paper I generated broad interest, given that the IRTS detection limit for point sources was much fainter ( ∼ 11 mag) than for COBE, and the low resolution spectroscopy aspect of IRTS was unique. However, as Mattila (2006) pointed out, paper I did not take the contribution of diffuse Galactic light (DGL) into account. Furthermore, uncertainty in the model of zodiacal light (ZL) (Kelsall et al. 1998 ) has been a concern, given that ZL is the dominant foreground emission, and that the spectral shape of the residual isotropic emission is similar to that of ZL (Dwek et al. 2005) .
Introduction : In response to these criticisms, here we have reanalyzed the IRTS data using up-to-date observational results on ZL, integrated star light (ISL), and DGL. Furthermore, we performed the same analysis with a different ZL model (the so-called Wright model, Wright (1998) ), and examined how a different choice of ZL model affects our results.
Introduction : In this paper, we briefly present the IRTS observations and the acquisition of raw data in Section 2. In Section 3, we estimate the contribution of foreground emission from ZL, ISL and DGL based on the latest observations. In Section 4, we search for residual isotropic emission using two ZL models based on the correlation of model ZL brightness with the overall sky brightness, after subtracting the ISL and DGL. In section 5, we discuss the astrophysical implications of the detected excess brightness.

116987163
ABSTRACT : Young massive stars, with their spectacular masers and HII regions, dominate our Galaxy, and are a cornerstone for understanding Galactic structure. I will highlight the role of Parkes in contributing to these studies -past, present and future.
INTRODUCTION : On the first day of our symposium. the contributions were entertaining and knowledgable reflections on the early years at Parkes -chiefly recalling an era from which there are few pioneers left with personal experience. Subsequent days focus on active work still continuing at Parkes. I am pleased to be the bridge from the past to the present, especially to the next session of the symposium, dealing with current studies of young massive stars and their masers.
INTRODUCTION : My main theme will be to show how the Parkes studies of masers and related objects contribute to revolutionizing the picture of our Galaxy, its content and its structure.

116988830
ABSTRACT : In this paper, we are concerned with the existence and multiplicity of no-node solutions of the Lazer-McKenna suspension bridge models by using the fixed point theorem in a cone.
Introduction : In [1] , the Lazer-McKenna suspension bridge models are proposed as following 

118860172
ABSTRACT : We generalize the Rigid-Field Hydrodynamic equations to accommodate arbitrary magnetic field topologies, resulting in a new Arbitrary Rigid-Field Hydrodynamic (ARFHD) formalism. We undertake a critical point calculation of the steady-state ARFHD equations with a CAK-type radiative acceleration and determine the effects of a dipole magnetic field on the usual CAK mass-loss rate and velocity structure. Enforcing the proper optically-thin limit for the radiative line-acceleration is found to decrease both the mass-loss and wind acceleration, while rotation boosts both properties. We define optically-thin-correction and rotation parameters to quantify these effects on the global mass-loss rate and develop scaling laws for the surface massflux as a function of surface colatitude. These scaling laws are found to agree with previous laws derived from magnetohydrodynamic simulations of magnetospheres. The dipole magnetosphere velocity structure is found to differ from a global beta-velocity law, which contradicts a central assumption of the previously-developed XADM model of X-ray emission from magnetospheres.
INTRODUCTION : In the last decade, spectropolarimetric surveys of OB stars have revealed that about 5-10% of these massive stars have large-scale, organized magnetic fields (MiMeS: Wade et al. 2014; BOB: Morel et al. 2015) . Such detectable magnetic fields (B 100 G) have a significant effect on the stellar wind, both channelling and trapping plasma within a stellar magnetosphere. This accumulated plasma produces extrastellar emission in optical (e.g. Howarth et al. 2007 , Bohlender & Monin 2011 , Grunhut et al. 2012 and references therein), infrared (Eikenberry et al. 2014) , radio (Linsky et al. 1992; Chandra et al. 2015) , and X-ray (Nazé et al. 2014 (Nazé et al. , 2015 . Furthermore, this emission exhibits a rotational modulation as the plasma is forced by the magnetic field to co-rotate with the star.
INTRODUCTION : Similar advances in magnetosphere theory have also followed, starting with the pioneering magnetohydrodynamics (MHD) simulations of ud-Doula & Owocki (2002) . They developed a "wind magnetic confinement parameter" to characterize the interplay between the stellar magnetic field and flow:
INTRODUCTION : email: bard@astro.wisc.edu withṀB=0 and v∞,B=0 being the stellar mass-loss rate and terminal velocity if the star had no magnetic field. The confinement parameter η * has become the canonical value adopted in scaling relations to explain the size (ud-Doula & Owocki 2002) , the mass-loss (ud-Doula et al. 2008) , the spin-down (ud-Doula et al. 2009 ), and, with the critical rotation fraction ω, the classification (Petit et al. 2013 ) of magnetospheres. However, η * itself depends on nonmagnetic values, ignoring any effects of the magnetic field. How does the magnetic field change the mass-loss rate and velocity? Can we use these new values to make a better confinement parameter?
INTRODUCTION : Traditionally,Ṁ and v∞ have been determined by analyzing the equation of motion for a line-driven wind (Castor et al. 1975; hereafter CAK) and solving for the so-called "critical point". Over the years, various modifications to the base CAK model (finite-disk effect: Friend & Abbott 1986 , Pauldrach et al. 1986 ; depth-dependent force multiplier parameters: Kudritzki 2002 ) have led to more realistic predictions of the mass-loss and terminal velocities. Other methods have been developed to improve on these estimates, such as a Monte Carlo method (Vink et al. 2000; Noebauer & Sim 2015) and a scattering source function technique (Sundqvist & Owocki 2015) . For now, we use the CAK line-driving force in order to take the first steps towards understanding the effect of a dipole field on a stellar wind.
INTRODUCTION : In this paper, we present and study the Arbitrary Rigid-Field Hydrodynamics (ARFHD) equations, an extension of Rigid-Field Hydrodynamics (RFHD) (Townsend et al. 2007) to account for non-dipole magnetic geometries (though we will consider only dipolar topologies in this analysis). RFHD was originally developed as an extension of the Rigidly Rotating Magnetosphere (RRM) model (Townsend & Owocki 2005) for centrifugal magnetospheres, whose large magnetic fields make MHD simulations very impractical. In this ansatz, the magnetic fields are assumed to be completely rigid (η * → ∞), channeling the stellar wind along quasi-onedimensional flux tubes. This allows each field line to be studied and simulated independently from one another, though this does miss important multi-dimensional effects present in the MHD simulations. In essence, the MHD studies approach the subject of massive-star magnetospheres from the regime of low magnetic confinement; ARFHD approaches this subject from the opposite regime of strong magnetic confinement. By blending both studies, we can set limits on the behavior of magnetospheres.
INTRODUCTION : In Section 2, we present the reformulated ARFHD equations and define all the terms, including external sources of acceleration and cooling. Following this, we develop the critical point equations for an arbitrary magnetic configuration in Section 3 and an algorithm for determing the critical point location in Section 4. Section 5 details the implementation and application of an aligned magnetic dipole radiationdriven wind model which includes the effect of stellar rotation. We present analytic scalings of the surface mass-flux in Section 6 and model results for the critical point location (Section 7), velocity structure (Section 8), and, finally, the global mass-loss rate (Section 9).

118860835
ABSTRACT : The equilibrium binding energy is an important factor in the design of materials and devices. However, it presents great computational challenges for materials built up from nanostructures. Here we investigate the binding-energy scaling law from first-principles calculations. We show that the equilibrium binding energy per atom between identical nanostructures can scale up or down with nanostructure size. From the energy scaling law, we predict finite large-size limits of binding energy per atom. We find that there are two competing factors in the determination of the binding energy: Nonadditivities of van der Waals coefficients and center-to-center distance between nanostructures. To uncode the detail, the nonadditivity of the static multipole polarizability is investigated. We find that the higher-order multipole polarizability displays ultra-strong intrinsic nonadditivity, no matter if the dipole polarizability is additive or not.
I. INTRODUCTION : There is strong interest in nanomaterials, motivated by the development of nanotechnoloy and by their novel properties arising from quantum confinement. In particular, the discovery of various atomic-level materials has received overwhelming attention for their remarkable properties and wide-ranging applications [1] . A common feature of these materials is the strong adhesive van der Waals (vdW) force due to the instantaneous charge fluctuations. To understand the nature of the vdW force, a variety of experiments ranging from the smallest atomistic to the largest macroscopic scales have been performed recently [2] [3] [4] [5] [6] . However, details of many surprising phenomena due to the vdW interaction have not been well understood at the nanoscale [7] . Here we will ask and answer another such question.
I. INTRODUCTION : The equilibrium binding energy between identical nanostructures is an important property involving microscopically the short-range contribution arising from the density overlap and the long-range vdW interaction. However, due to the large size of nanostructures, it presents great computational challenges. As such, an energy scaling law showing the variation of equilibrium binding energy per atom with system size is highly desired. We here apply an efficient first-principles method, the vdW-DF-cx [8] density functional, to investigate the energy scaling law, aiming to provide novel insights into nanostructures. Figure 1 shows the energy scaling law for a variety of nanomaterials, while Table II shows the energy scaling law for a variety of nanostructures obtained by fitting to our numerical calculations. We find that the binding-energy scaling law is largely due to the competing size effects of the vdW coefficients and sum of the vdW radii of nanostructures determining the intermolecular distance. Our finding is different from previous works [7, 9, 10] , in which the vdW coefficients and intermolecular distance are treated independently, allowing one to study the dependence of the vdW interaction upon the power of distance.
V. CONCLUSION : The binding energy determines the stability of nanostructures and is therefore very important in the study of nanostructures. However, it has presented computational challenges. In this work, we have studied the binding energy law of nanostructures based on a first-principles method. We find that there is a binding-energy scaling law between identical nanostructures. From the law, we can predict the binding energy at any structure size. We illustrate this finding with fullerenes, PAHs, nanotubes, and nanowires. Apart from fullerenes, we chose AA stacking in our study. From the energy scaling law, we predict finite large-size limits, which are expected. To understand the energy-scaling law, we have studied the vdW coefficients using the accurate hollow-sphere model within the SFA. We find that the energy scaling law is determined by two competing factors: Nonadditivities of the vdW coefficients and the center-to-center distance. This leads us to conclude that the energy-scaling law in part originates from the nonadditivity of the static multipole polarizability of nanostructures.

118861869
ABSTRACT : An eccentric nuclear disk consists of stars moving on apsidally-aligned orbits around a central black hole. The secular gravitational torques that dynamically stabilize these disks can also produce tidal disruption events (TDEs) at very high rates in Newtonian gravity. General relativity, however, is known to quench secular torques via rapid apsidal precession.
ABSTRACT : Here we show that for a disk to black hole mass ratio of M disk /M • 10 −3 , the system is in the full loss cone regime. The magnitude of the torque per orbital period acting on a stellar orbit means that general relativistic precession does not have a major effect on the dynamics. Thus we find that TDE rates from eccentric nuclear disks are not affected by general relativistic precession. Furthermore, we show that orbital elements between successive TDEs from eccentric nuclear disks are correlated, potentially resulting in unique observational signatures.
INTRODUCTION : A tidal disruption event (TDE) occurs when a star is violently ripped apart by a black hole's tidal forces (Hills 1975) . When a star is tidally disrupted, roughly half of the stellar debris remains bound to the black hole while the other half of the debris escapes. The gravitationally bound debris forms an accretion disk which feeds the black hole, producing a flare (Rees 1988) . The current detection rate of flares from TDEs is about two per year (van Velzen 2018) and this is expected to increase with new surveys such as the Large Synoptic Survey Telescope (LSST) (van Velzen et al. 2011) .
INTRODUCTION : TDE flares can provide insight into the mysteries of many areas of astrophysics. They illuminate central black holes in otherwise quiescent galaxies (Maksym et al. 2013; MacLeod et al. 2014) . We can use their observations to test theories of accretion physics and relativistic jets (Zauderer et al. 2011; Bloom et al. 2011; van Velzen et al. 2016; Alexander 2017) . Tidal disruptions of white dwarfs should even produce gravitational waves detectable by the Light Interferometer Space Antenna (LISA) (MacLeod et al. 2014) . Additionally, we can test our understanding of gravitational stellar dynamics near supermassive black holes by comparing theoretical TDE rates with observations. heather.wernke@colorado.edu

118862329
ABSTRACT : We determine the commutation relations satisfied by the quantized electromagnetic field in the presence of macroscopic dielectrics and conductors, with arbitrary dispersive and dissipative properties. We consider in detail the case of two plane-parallel material slabs, separated by an empty gap, and we show that at all points in the empty region between the slabs, including their surfaces, the electromagnetic fields always satisfy free-field canonical equal-time commutation relations. This result is a consequence of general analyticity and fall-off properties at large frequencies satisfied by the reflection coefficients of all real materials. It is also shown that this result does not obtain in the case of conductors, if the latter are modelled as perfect mirrors. In such a case, the free-field form of the commutation relations is recovered only at large distances from the mirrors, in agreement with the findings of previous authors. Failure of perfect-mirror boundary conditions to reproduce the correct form of the commutation relations near the surfaces of the conductors, suggests that caution should be used when these idealized boundary conditions are used in investigations of proximity phenomena originating from the quantized electromagnetic field, like the Casimir effect.
I. INTRODUCTION : The interaction of radiation with matter has always been a fascinating subject of investigation, and in fact it is at the roots of quantum mechanics, with Planck's work on black body radiation. Even though, after the development of Quantum Electrodynamics (QED) in the middle years of last century, all fundamental principles involved in this interaction are undoubtedly well understood at the microscopic level, recent experimental advances have prompted much interest in theoretical studies of the quantized electromagnetic (e.m.) field in close proximity to macroscopic bodies. A thorough understanding of this problem is indeed needed for a correct interpretation of numerous important proximity phenomena of e.m. origin, that include cavity QED [1] , the Casimir effect [2] , radiative heat transfer [3] , quantum friction [4] , the Casimir-Polder interaction of Bose-Einstein condensates with a substrate [5] , etc. Apart from the intrinsic interest of these phenomena, it has been shown recently that the quantum fluctuations of the e.m. field surrounding macroscopic bodies, that are at the origin of the Casimir effect, could have exciting application in nanotechnology [6] .
I. INTRODUCTION : The common feature of the above e.m. phenomena, is that they all involve several macroscopic bodies and possibly one or more microscopic objects (atoms, ions etc.) placed in a vacuum and separated by distances (typical separations range from a few tens of nanometers * Bimonte@na.infn.it%; to several microns) that, while small from a macroscopic point of view, are still large compared to the interatomic distance in condensed bodies. In such circumstances, the microscopic point of view is not of great help, because the long range character of the e.m. field implies that macroscopically large number of atoms are inevitably involved in the interaction. A much more effective approach would be to describe the influence of the macroscopic bodies on the quantized e.m. field in the vacuum just outside their boundaries, in terms of macroscopic features of the bodies like the electric and/or magnetic permittivities. On physical grounds, one expects that such an approach should be feasible, in certain circumstances at least, because the wavelengths of the e.m. fields participating in these phenomena are expected to be of the order of the bodies separations, and are therefore large on the atomic scale. This being the case, use of macroscopic response functions of the bodies should be legitimate. An inevitable complication that one faces though, when dealing with macroscopic response functions of real bodies, is that they always display dispersion and absorption. As is it well known, the former feature is mathematically reflected in the fact that response functions depend on the frequency ω (we shall neglect spatial dispersion, and therefore we shall not consider the possible dependence of the response functions on the wave-vector k), while the presence of dissipation entails that the response functions have a non-vanishing imaginary part. The existence of absorption, in particular, greatly complicates explicit quantization of the macroscopic e.m. field. Unfortunately, such a difficulty cannot be disposed of by simply neglecting dissipation, because dispersive, real-valued response functions inevitably violate causality, and must therefore be rejected.
I. INTRODUCTION : Fortunately, though, there exists a way out that avoids the above mentioned difficulties. This is so because a full quantization of the e.m. field is usually not needed, as the quantities of interest are typically statistical averages of quadratic expressions involving the macroscopic e.m. field. For systems that are in thermodynamic equilibrium, such averages can be expressed in terms of (the imaginary part of) suitable macroscopic response functions, as a result of general fluctuation-dissipation theorems derived in the framework of linear-response theory [7] . This general approach was probably pioneered by Rytov [8] in his investigations of e.m. fluctuations in the presence of macroscopic bodies in thermal equilibrium, and it was later used by Lifshitz [9] in his famous theory of dispersion forces between macroscopic condensed bodies. In one form or another, the fluctuation-dissipation theorem is used in all existing approaches to problems involving the quantized e.m. field in the vicinity of or inside macroscopic bodies. In the seventies of last century, Agarwal used it as the basis of a systematic investigation of QED in the presence of dielectrics and conductors [10] . For a review of the most recent work we address the reader to Refs. [11, 12] (see also Refs. therein). It is important to note that this approach is not restricted to systems in global thermodynamic equilibrium, as it is still valid in systems that are only in local thermodynamic equilibrium. This feature permits to include within the scope of the theory other important phenomena, like radiative heat transfer between closely separated bodies (for a recent review see [3] ), and quantum friction [4] . Recently, the theory has also been applied to the investigation of Casimir-Polder [5, 13] and Casimir [14] forces out of thermal equilibrium.
I. INTRODUCTION : In this paper, we reexamine the basic quantum-fieldtheoretical problem of the commutation relations satisfied by the quantized e.m. field in the presence of dielectrics and/or conductors, in the framework of the general macroscopic theory described above. Our interest in this problem arose from a paper by Milonni [15] on the Casimir effect, in which it was found that near a perfectly reflecting slab, the transverse vector potential and the electric field satisfy a set of equal-time canonical commutation relations of a different form form those holding for free fields. This result is quite worrisome, in view of the very fundamental character of commutation relations, because it contradicts one's expectations based on microscopic theory, and therefore it deserves detailed investigation. We remark that unexpected commutation relations between the annihilation and creation operators for the e.m. field inside a cavity were also found more recently in Ref. [16] . A partial resolution of the paradox was offered in Ref. [17] , in which the problem of the e.m. commutation relations was investigated within a simplified form of QED, in one space dimension. By relying on a simple quantum theory of the one-dimensional lossy beam splitter, along the lines of Ref. [18] (see also [19] ), it was shown that the anomalies found in [16] in the commutators of the annihilation and creation operators were associated with a particular choice of the the cavity e.m. modes. The authors of Refs. [17] also showed that the canonical one-dimensional commutation relations involving the vector potential and the electric field do not display any anomalous behavior. No detailed explanation was however provided for the modified form of the equaltime commutators derived in Ref. [15] , apart from the remark that the boundary conditions (b.c.) satisfied by the e.m. field in the case of ideal mirrors are incompatible with the transverse delta function form of the full canonical commutator in three dimensions. These authors further conjecture that standard equal-time commutators would probably be restored after incorporating, in the full three-dimensional setting, the physical requirements of finite reflectivity and absorption losses by the mirrors.
I. INTRODUCTION : Addressing this problem in detail is not only interesting as a matter of principle, but it is also important for a better understanding of the numerous proximity phenomena arising from quantum fluctuations of the e.m. field described earlier. In many theoretical investigations of these phenomena, one deals with conductors that are frequently modelled as ideal mirrors. A famous example of this is provided by original Casimir's derivation [20] of the effect that goes under his name. It is then important to know to what extent conclusions drawn from the ideal-metal model can be trusted. Indeed Casimir physics offers examples where predictions drawn from the idealmetal model are in contradiction with those derived by more realistic modelling of the plates. One such example is still much debated as we write, and it is the problem of determining the influence of temperature on the magnitude of the Casimir force between two metallic plates in vacuum. It turns out the the ideal-metal model predicts a thermal force that, for sufficiently large separations between the plates, attains a magnitude which is twice the one calculated on the basis of realistic dielectric models of a conductor, displaying a finite, though large, dc conductivity (for a review of this puzzle, see for example Ref. [21] and References therein).
I. INTRODUCTION : In order to shed light on this question, in this paper we work out a detailed analysis of the full threedimensional commutators for the e.m. field, in the presence of dielectric and/or conducting walls with arbitrary dispersion and dissipative features. The analysis turns out to be considerably more involved than the simple one-dimensional model studied in [17] . Our main result is that the canonical commutation relations satisfied by free e.m. fields are always valid at all points between two macroscopic dielectric or conducting slabs, including their surfaces, in full agreement with expectations based on the microscopic theory for a system of charged nonrelativistic particles interacting with the e.m. field. This result is consequence of analyticity and fall-off properties at large frequencies of reflection coefficients of all real materials, as it was correctly conjectured in Ref. [17] . We also show that such a result is not recovered, however, in the case of conductors, if they are modelled as perfect mirrors. In this case we find that near the conductors the equal-time commutation relations of the vector potential with the electric field have a different form from the freefield case. Only at points that are sufficiently far from the conductors, the free-fields commutators are recovered. Our results generalize those obtained by Milonni, in the one slab setting, and show that the modified form of the commutation relation entailed by perfect-mirror b.c. are indeed an artifact of these idealized b.c., not shared by real materials.
I. INTRODUCTION : The paper is organized as follows. In Section II we recall the basic commutation relations satisfied, within the microscopic theory, by the e.m. field in vacuum and in the presence of charged particles. In Section III we briefly review some general results of linear response theory, as applied to macroscopic quantum electrodynamics, and derive formulae for the expectation values of the field commutators outside a system of macroscopic bodies, in terms of suitable classical Green's functions. In Section IV we estimate the Green's functions for a system of one or two dielectric and/or conducting slabs in vacuum, and in Sec V we use them to calculate the commutation relations satisfied by the e.m. field outside the slabs. In Sec VI we consider the case of ideal, perfectly reflecting slabs, while Section VII contains our conclusions. Finally, three Appendices close the paper.

118863510
ABSTRACT : The Barabási-Bianconi (BB) fitness model can be solved by a mapping between the original network growth model to an idealized bosonic gas. The well-known transition to Bose-Einstein condensation in the latter then corresponds to the emergence of "super-hubs" in the network model. Motivated by the preservation of the scale-free property, thermodynamic stability and self-duality, we generalize the original extensive mapping of the BB fitness model by using the nonextensive Kaniadakis κ-distribution. Through numerical simulation and mean-field calculations we show that deviations from extensivity do not compromise qualitative features of the phase transition. Analysis of the critical temperature yields a monotonically decreasing dependence on the nonextensive parameter κ.
Introduction : Over the last twenty years, research about complex networks has yielded many insights into a large number of real-world systems in various contexts, with systems as diverse as the World Wide Web, social media, power grids, transportation networks and gene regulation networks being prime examples, cf. [1, 2, 3] for reviews on the field.
Introduction : The network paradigm has proven very useful in quantifying the topology of interactions in these systems. In recent years the interest of the scientific community has shifted from analysis of purely static structures towards attempts at gaining insights into dynamically evolving or optimized networks [2] . In many applications statistical physics has provided a powerful toolbox, sometimes discovering surprising parallels between networked systems and other physical systems. One example of such a parallel is the Barabási-Bianconi (BB) model [4, 5] , which describes a process of network evolution guided by a combination of preferential attachment and intrinsic fitness properties of nodes [2] .
Introduction : In [5] , a mapping between the growing network and a bosonic gas undergoing a Bose-Einstein condensation was proposed, realized by means of extensive statistical mechanics, and solved via a mean field approach. No interactions between particles or energy level transitions were contemplated. Interesting behaviour is found when introducing a fictitious temperature parameter T regulating the network dynamics. Even for T → 0 the bosonic gas counterpart exhibits a ground state in which only half of the particles reach the minimum energy level, the others being scattered in fixed positions throughout the whole energy spectrum [5, 6] . This anomalous thermodynamic behavior is incompatible with the Boltzmann weight used for the original mapping. In fact, a purely physical perspective would suggest that in equilibrium all the bosons of the Bose-Einstein condensate populate the minimum energy level in the ground state. This anomaly has motivated research into applications of deformed non-Gaussian statistics [7, 8] and it is of interest to generalize the mapping between bosonic gases and growing networks by recurring to a nonextensive deformation of the original equilibrium Boltzmann-Gibbs distribution. Even though Information Theory seems to suggest a link between non-ergodic behavior and nonextensive statistics in nature, the emergence of non-Gaussian statistics for complex systems, as, e.g., growing complex networks, has not yet been fully understood [9] .
Introduction : To the best of our knowledge one recent study [10] of the BB model using Tsallis' q-statistics [7] is the only previous investigation of extensions of the BB model to nonextensive statistics. Properties of the q-statistics make it very difficult to find exact results and the insights gained by [10] are limited to numerical simulation. Further, it has been argued [11] that non-extensive statistics should have the following characteristics: (i) preservation of scale-free property, (ii) self-duality, and (iii) thermodynamic stability. Even though the Tsallis q-exponential gives rise to power-laws in many real world modelling applications [7] it does not satisfy self-duality. This gives an added interest to the Kanidakis κ-distribution which meets all three requirements. The κ-deformed statistical mechanics was originally proposed in the context of non-linear kinetics in particle systems and is deeply linked to the structure of special relativity [11] . In the last decade, κ-deformed statistics have been successfully applied to model the distribution of stellar rotational velocities of dwarf stars [12] , cosmic ray fluxes [11] , the formation of Quark-Gluon plasmas [13] , and the income distribution of the USA, UK, Germany and Italy [8] . Moreover, the use of κ-deformed statistics has led to some insight [14] in addressing the inadequacy of the Bose-Einstein distribution in predicting the fluid-superfluid transition temperature in 4 He . The latter gives an additional motivation for our application of κ-deformed statistics to models of network formation.
Introduction : In this article we show that the Kaniadakis κ-distribution can be used to generalize the BB bosonic mapping.
Introduction : In contrast to previous numerical results based on q-statistics [10] , analytical mean-field results can be retrieved with the κ-deformed distribution. Our findings show that the use of the Kaniadakis κ-distribution does not alter the qualitative presence of condensation. We show both analytically and numerically that the main influence of the nonextensive parameter is a systematic shift of the critical temperature with κ.
Introduction : Our paper is organized as follows. In Section I we briefly review some of the properties of the BB fitness model and its original extensive mapping to a Bose-Einstein condensate. In Section II we summarize some basic results of the Kaniadakis κ-distribution and generalize them to the nonextensive mapping. Finally, in Section III numerical results for the critical temperatures for different values of the nonextensive parameter are presented and compared to analytical findings. In the same section, we will also analyze the degree distribution of our generalized model, by using both analytical and numerical methods.
Conclusions : In this paper we have generalized the Barabási-Bianconi fitness model by the means of the non-Gaussian Kaniadakis κ-distribution, which was originally proposed in the framework of nonextensive statistical mechanics. Our analytical results show that the resulting generalized fitness model presents a phase transition from a "fit-get-rich" phase to a "gel" phase, formally equivalent to a Bose-Einstein condensation. Analytical calculations supported by numerical estimates show that the critical temperature T c of the Bose-Einstein condensation on networks decreases when the nonextensive parameter κ is increased from 0 to 1. A numerical analysis of the degree distribution, complemented by an analytically obtained lower bound, reveals the presence of power-law behaviour in the phase of high-temperatures and a linear energy level density for g( ) → 0. In contrast, in the condensate phase stretched exponentials, compatible with the recent finding of a hierarchy of hubs, are retrieved.

118863857
ABSTRACT : Single-exposure spectra in large spectral surveys are valuable for time domain studies such as stellar variability, but there is no available method to eliminate cosmic rays for single-exposure, multi-fiber spectral images. In this paper, we describe a new method to detect and remove cosmic rays in multi-fiber spectroscopic single exposures. Through the use of two-dimensional profile fitting and a noise model that considers the position-dependent errors, we successfully detect as many as 80% of the cosmic rays and correct the cosmic ray polluted pixels to an average accuracy of 97.8%. Multiple tests and comparisons with both simulated data and real LAMOST data show that the method works properly in detection rate, false detection rate, and validity of cosmic ray correction.
Introduction : Cosmic rays (CRs, hereafter) are high-energy particles that generate randomly distributed, large signals on charge-coupled devices (CCDs), which could affect the measured fluxes of astronomical objects if not detected or removed properly. Generally, CRs are removed by combining three or more exposures of the same field (Windhorst 1994; Freudling 1995; Fruchter 1997; Gruen 2014; Desai 2016) , as they are unlikely to hit the same pixel in more than one exposure. However, multiple exposures are not always available. Furthermore, there are certain situations in which CR detection in single exposures is desired, such as in time domain studies.
Introduction : Various methods have been developed for identifying and replacing CRs in CCD data of single exposures, including median filtering (e.g., Dickinsons IRAF tasks QZAP, XZAP, and XNZAP), applying a threshold on the contrast (e.g., IRAF task COSMIC-RAYS ), trainable classification (Murtagh 1992; Salzberg 1995; Bertin 1996) , convolution with adapted point-spread functions (PSFs; Rhoads 2000) , Laplacian edge detection (van Dokkum 2001) , analysis of the flux histogram (Pych 2004 ) and a fuzzy logic-based method (Shamir 2005) . All of the median filtering or PSF methods remove small CRs from well-sampled data effectively, but problems arise when CRs affect more than half the size of the filter or when the PSF is smaller than the filter (van Dokkum 2001) . All of the methods listed above are designed for photometric data except those of van Dokkum (2001) and Pych (2004) , which work for long-slit spectroscopic data.
Conclusion : We present a method for detecting and removing 2D profile fitting to each segment. A new cosmic ray list is generated by comparing the fitting residual with a noise model depending on both the intrinsic shot noise and the relative position in the profile. We finally produce a more accurate cosmic ray mask table and more reasonable substitution values for CR polluted pixels. The method is tested by both simulations and real data; the results show that our method has a high detection rate, low false detection rate, and proper replacement of the CR polluted pixels.
Conclusion : Since this method fits the 2D profiles of the fiber spectro-scopic data, which are different from the photometric PSF, it cannot be applied to photometric data. However, it can be used in slit spectroscopic data after minor modifications. The code and samples are available at http://lamostss.bao.ac.cn/~bai/crr. -Performance of our CR correction. F r is the spectral flux extracted at the position of CR influences (any detected or fake or undetected CR within the extraction aperture) from the CR corrected image, and F r is the corresponding flux from the CR-free spectrum. The left column shows the comparison of our result and the clean spectra, with the number density indicated by the grey level. The right column shows the histogram of F r /F c . Rows from top to bottom show the replacement performance of falsely detected, correctly detected and undetected CRs, respectively. The biggest deviation (2.2%) happens in the false detection, since the program tries to replace the falsely detected CRs with lower "correct" values. Fig. 7 .-Example of extracted spectra. In the top panel, the dotted line is the spectrum extracted from the image without CR correction, the solid black line is the CR-free spectrum and the solid gray line is our CR corrected result. Spectra have been shifted in the flux direction for clarity. In the middle panel, only the CR-free and the CR corrected spectra are plotted to show more detail. In the bottom panel, the relative difference between the CR-free and the CR corrected spectrum is shown. Figure 6 , the scatter is larger because the uncertainty in the real data is larger than simulations.

118864086
ABSTRACT : Exceptional points are found in the spectrum of a prototypical thermoacoustic system as the parameters of the flame transfer function are varied. At these points, two eigenvalues and the associated eigenfunctions coalesce. The system's sensitivity to changes in the parameters becomes infinite. Two eigenvalue branches collide at the exceptional point as the interaction index is increased. One branch originates from a purely acoustic mode, whereas the other branch originates from an intrinsic thermoacoustic mode. The existence of exceptional points in thermoacoustic systems has implications for physical understanding, computing, modeling and control.
Introduction : At exceptional points (EPs), at least two eigenvalues and the associated eigenfunctions coalesce, and the eigenvalue sensitivity with respect to changes in the parameters becomes infinite [1, 2] . Interesting physical phenomena associated with EPs appear across various disciplines from quantum mechanics through optics and acoustics [2, 3, 4] . To the best of the authors' knowledge, the role of exceptional points has not yet been explored in thermoacoustic systems, although points in the parameter space with infinite sensitivity were discussed in a recent review article [5] . In this letter, we show that these points in the thermoacoustic spectrum are exceptional, and that they can be found in a generic thermoacoustic system when two real parameters are varied.

118864185
ABSTRACT : I consider whether we can significantly improve the Cardelli et al. (1989) family of extinction laws using new data and techniques. There are six different aspects that need to be treated: The use of monochromatic quantities, the three different wavelength regimes (NIR, optical and UV), the sample, and the photometric calibration. Excluding the behavior in the NIR and UV, I discuss the other four aspects and propose a new family of extinction laws derived from VLT/FLAMES and HST/WFC3 data.

118864492
ABSTRACT : Since the detection of non-thermal radio emission from the bow shock of the massive runaway star BD +43
ABSTRACT : • 3654 simple models have predicted high-energy emission, at X and gamma-rays, from these Galactic sources. Observational searches for this emission so far give no conclusive evidence but a few candidates at gamma rays. In this work we aim at developing a more sophisticated model for the nonthermal emission from massive runaway star bow shocks. The main goal is to establish whether these systems are efficient non-thermal emitters, even if they are not strong enough to be yet detected. For modeling the collision between the stellar wind and the interstellar medium we use 2D hydrodynamic simulations. We then adopt the flow profile of the wind and the ambient medium obtained with the simulation as the plasma state for solving the transport of energetic particles injected in the system, and the non-thermal emission they produce. For this purpose we solve a 3D (2 spatial + energy) advection-diffusion equation in the test-particle approximation. We find that a massive runaway star with a powerful wind converts 0.16-0.4% of the power injected in electrons into non-thermal emission, mostly produced by inverse Compton scattering of dust-emitted photons by relativistic electrons, and secondly by synchrotron radiation. This represents a fraction of ∼ 10 −5 − 10 −4 of the wind kinetic power. Given the better sensibility of current instruments at radio wavelengths theses systems are more prone to be detected at radio through the synchrotron emission they produce rather than at gamma energies.
INTRODUCTION : Runaway massive stars are stars with high spatial velocities (V > 30 km s −1 ) that have been expelled from their formation sites (e.g., Hoogerwerf et al. 2000; Tetzlaff et al. 2011) . Massive stars have strong winds that interact with the interstellar medium (ISM) as the stars move supersonicaly through it. In this interaction a bow shock is formed, in some cases detectable in the infrared (IR) (e.g., van Buren & McCray 1988; Kobulnicky et al. 2010) . This last emission is reprocessed stellar light by the dust swept by the bow shock. There are of the order of ∼ 700 stellar bow shocks cataloged so far (Peri et al. 2012 (Peri et al. , 2015 Kobulnicky et al. 2016) .
INTRODUCTION : The bow shock of the massive runaway star BD +43
INTRODUCTION : • 3654 was detected at radio wavelengths, and the emission might be synchrotron radiation (Benaglia et al. 2010) . This suggests that a population of high-energy electrons is present in the source, interacting locally with the magnetic field. In the collision between the ISM and the stellar wind a system of two shocks is formed: a forward shock and a reverse shock. This last shock is adiabatic and fast, with velocities of the order of ∼ 10 3 km s −1 . Hence it is straight forward to think that this reverse shock might accelerate particles up to highenergies through diffusive shock acceleration (DSA) .
INTRODUCTION : If the electrons that produce the radio non-thermal emission were accelerated in the reverse shock of BD arXiv:1807.05895v1 [astro-ph.HE] 16 Jul 2018
INTRODUCTION TO THE MODEL : As mentioned above, the bow shock of a massive runaway star is formed by the collision of the stellar powerful wind with the incoming ISM, in the star's reference frame. The wind and ISM pressure balance at the contact discontinuity. The characteristic scale of the system is usually taken as the standoff distance R 0 , given by the Figure 1 . Scheme of a runaway massive star bow shock. Five regions can be distinguished: free flowing stellar wind, the shocked wind, hot shocked ISM, cooled ISM, and the ISM itself. Due to thermal conduction two layers of shocked ISM are formed. A hot and low density layer adjacent to the contact discontinuity and an outer one formed of cooled shocked ambient medium (Comeron & Kaper 1998; Meyer et al. 2014 ).
INTRODUCTION TO THE MODEL : balance of the wind and ambient medium ram pressures:
INTRODUCTION TO THE MODEL : whereṀ w and V w are the wind mass loss rate and velocity, respectively; ρ ISM is the ISM density and v is the star's velocity. In the instantaneous cooling approximation R 0 would directly give the distance from the star to the apsis of the bow shock, however in a real system this distance might vary, due to thermal conduction and cooling, for example (e.g., Raga et al. 1997; Comeron & Kaper 1998; Meyer et al. 2014) .
INTRODUCTION TO THE MODEL : In the literature a number of works exists on the collision of two fluids and specifically for modeling the bow shocks of massive runaways (e.g. Wilkin 1996; Canto et al. 1996; Raga et al. 1997; Comeron & Kaper 1998; Wilkin 2000; van Marle et al. 2011; Meyer et al. 2014 Meyer et al. , 2016 Meyer et al. , 2017 . A precise description of the phenomenology requires a dynamical treatment implementing numerical simulations. An appropriate treatment of the hydrodynamics of stellar winds should include both optically thin cooling and thermal conduction (e.g., Raga et al. 1997; Comeron & Kaper 1998) .
INTRODUCTION TO THE MODEL : After the formation of the bow shock the system would reach globally a steady state. A general sketch of a bow shock is shown in Figure 1 . The system is very prone to suffer many instabilities: Rayleigh-Taylor between the dense cooled layer and the hotter less dense one, an instability arising in shocked layers bounded by thermal pressure on one side and ram pressure on the other (e.g., Ryu & Vishniac 1987; Mac Low & Norman 1993; Comeron & Kaper 1998) and Kelvin-Helmholtz due to the velocity shear between the material layers (e.g., Dgani et al. 1996) . A complete analysis of these instabilities is made in Comeron & Kaper (1998) .
INTRODUCTION TO THE MODEL : In this work we use the PLUTO code (Mignone et al. 2007 ) to solve the 2D hydrodynamic equations following the set-up by Meyer et al. (2014) (see also, Meyer et al. 2016 Meyer et al. , 2017 . At this stage we do not consider the magnetic field in the simulations. As the system reaches a steady state we use that state as a scenario to solve in it the transport of energetic particles, assumed to be accelerated via DSA in the wind shock. We search for the reverse shock position and inject there relativistic electrons and protons; using our own code we solve the diffusion-advection equation for the particles in the 2D domain.
INTRODUCTION TO THE MODEL : The energetic particles would interact with the magnetic field producing synchrotron emission (only for electrons, proton synchrotron is very inefficient in this case); with the density producing relativistic Bremsstrahlung and p − p inelastic collisions -for electrons and protons, respectively-; and with the radiation fields: the stellar photon field and the stellar-reprocessed dust emission. Only electrons interact efficiently with the radiation fields, via IC scattering.
INTRODUCTION TO THE MODEL : Other works that solve the hydrodynamic and magnetohydrodynamic equations together with the transport of high energy particles exist. For example, in de la Cita et al. (2016) they use a similar approach as the one we use, but here we do solve the spatial diffusion of the particles, which is key in the system we are studying. In Pakmor et al. (2016) they solve the hydrodynamics of galactic winds and cosmic-ray diffusion, but they consider this last component as a fluid, without solving the energy dependence of the particles, needed to compute the non-thermal emission; in contrast to this system, the pressure of the energetic particles is negligible in our case. Brose et al. (2016) make a self-consistent treatment of the plasma dynamics, acceleration and transport of cosmic rays in supernova remnants. However their 1D treatment is not appropriate in our problem.
INTRODUCTION TO THE MODEL : In the following Sections we describe with more detail the hydrodynamic model and the modeling of the transport of relativistic particles.

118864667
ABSTRACT : We have compiled photometry at 3.4, 4.6, 12 and 22 µm from the all-sky survey performed by the Wide-field Infrared Survey Explorer (WISE) for all known members of the Taurus complex of dark clouds. Using these data and photometry from the Spitzer Space Telescope, we have identified members with infrared excess emission from circumstellar disks and have estimated the evolutionary stages of the detected disks, which include 31 new full disks and 16 new candidate transitional, evolved, evolved transitional, and debris disks. We have also used the WISE All-Sky Source Catalog to search for new disk-bearing members of Taurus based on their red infrared colors. Through optical and near-infrared spectroscopy, we have confirmed 26 new members with spectral types of M1 -M7. The census of disk-bearing stars in Taurus should now be largely complete for spectral types earlier than ∼M8 (M 0.03 M ).
Introduction : Complete samples of circumstellar disks in star-forming regions and accurate classifications of those disks represent a foundation for studies of star and planet formation. Both the identification of circumstellar disks and their classification are most easily performed using mid-infrared (IR) continuum emission from warm circumstellar dust. Because the atmosphere is bright and strongly absorbing at mid-IR wavelengths, space-based telescopes have provided the most sensitive data of this kind. The all-sky mid-IR images from the Infrared Astronomical Satellite (IRAS; Neugebauer et al. 1984) enabled the first detections of circumstellar disks in star-forming regions. The most thorough census of disks was performed in Taurus because it is nearby (d = 140 pc; Wichmann et al. 1998; Loinard et al. 2005; Torres et al. 2007 Torres et al. , 2009 ) and has a low enough stellar density that its members could be resolved by IRAS (Kenyon & Hartmann 1995) . Subsequent mid-IR telescopes, such as the Infrared Space Observatory (Kessler et al. 1996) and the Spitzer Space Telescope (Werner et al. 2004) , have offered progressively better sensitivity and resolution, enabling detections of disks at fainter levels and in more crowded fields. Due to its modest field of view (5 ), Spitzer primarily observed more compact star-forming regions (Lada et al. 2006; Sicilia-Aguilar et al. 2006; Dahm & Hillenbrand 2007; Hernández et al. 2007; Luhman et al. 2008; Gutermuth et al. 2009 ), although it was able to map a significant fraction of a few widely distributed populations such as Taurus (Luhman et al. 2010; Rebull et al. 2010 ).
Introduction : The latest mid-IR satellite, the Wide-field Infrared Survey Explorer (WISE; Wright et al. 2010) , has lower spatial resolution than Spitzer but it covered the entire sky. As a result, WISE provides mid-IR photometry for the portions of large star-forming regions and associations that were not imaged by Spitzer. For instance, WISE data have been used to search for new disk-bearing stars in Taurus (Rebull et al. 2011) and to classify disks among the known members of Upper Sco (Luhman & Mamajek 2012) . Because of the importance of Taurus in studies of disks and because proper use of WISE data requires great care, we have performed our own search for new members with disks in Taurus, and have classified all the disks that we have detected around the known members. In this paper we begin by compiling photometry from 3 to 24 µm from both WISE and Spitzer for all known members of Taurus (Section 2). With these data, we then measure mid-IR excesses (Section 3) and classify the evolutionary stages of the detected disks (Section 4). Finally, we use WISE photometry in conjunction with proper motions, color-magnitude diagrams, and follow up spectroscopy to identify new members of Taurus (Section 5).
Conclusions : We have performed a survey of circumstellar disks in the Taurus star-forming region using WISE. Our results are summarized as follows:
Conclusions : 1. We have examined all images from the cryogenic phase of WISE for all known members of Taurus to check for false detections, blends with nearby objects, and extended emission. We have presented the resulting catalog of vetted photometry in the WISE bands at 3.4, 4.6, 12, and 22 µm (W 1-W 4). All resolved, unblended members are detected by WISE in at least one band.
Conclusions : 2. By using colors constructed from K s and six Spitzer and WISE bands, we have identified Taurus members showing excess emission from circumstellar disks and have estimated the evolutionary stages of the detected disks, consisting of full, transitional, evolved, evolved transitional, and debris disks. Our classifications are generally consistent with those found by Luhman et al. (2010) and Furlan et al. (2011) . We have found 31 new full disks and 16 new candidate disks in the more advanced evolutionary stages.
Conclusions : 3. Using photometry from WISE, Spitzer, 2MASS, and USNO, proper motions from UCAC4 and other catalogs, and our optical and near-IR spectroscopy, we have found 26 new members of Taurus with spectral types of M1 -M7. The census of disk-bearing stars in Taurus should now be largely complete for W 1 < 13 ( M8; M 0.03 M ).
Conclusions : T.E. and K.L. were supported by grant NNX12AI58G from the NASA Astrophysics Data Analysis Program. E.E.M. acknowledges support from NSF grants AST-1008908 and AST-1313029. This publication makes use of data products from the Wide-field Infrared Survey Explorer, which is a joint project of the University of California, Los Angeles, and the Jet Propulsion Laboratory/California Institute of Technology, and NEOWISE, which is a project of the Jet Propulsion Laboratory/California Institute of Technology. WISE and NEOWISE are funded by the National Aeronautics and Space Administration (NASA). The Spitzer Space Telescope and the IPAC Infrared Science Archive (IRSA) are operated by JPL and Caltech under contract with NASA. 2MASS is a joint project of the University of Massachusetts and the Infrared Processing and Analysis Center (IPAC) at Caltech, funded by NASA and the NSF. The IRTF is operated by the University of Hawaii under cooperative agreement NNX-08AE38A with NASA. The HET is a joint project of the University of Texas at Austin, the Pennsylvania State University, Stanford University, LudwigMaximillians-Universität München, and Georg-August-Universität Göttingen and is named in honor of its principal benefactors, William P. Hobby and Robert E. Eberly. The Marcario Low-Resolution Spectrograph at HET is named for Mike Marcario of High Lonesome Optics, who fabricated several optics for the instrument but died before its completion; it is a joint project of the HET partnership and the Instituto de Astronomía de la Universidad Nacional Autónoma de México. The Digitized Sky Survey was produced at the Space Telescope Science Institute under U.S. Government grant NAG W-2166. The images of these surveys are based on photographic data obtained using the Oschin Schmidt Telescope on Palomar Mountain and the UK Schmidt Telescope. The plates were processed into the present compressed digital form with the permission of these institutions. The Center for Exoplanets and Habitable Worlds is supported by the Pennsylvania State University, the Eberly College of Science, and the Pennsylvania Space Grant Consortium.
Conclusions : A. Comments on B/A stars from Mooley et. al (2013) From among the five new proposed B/A-type Taurus candidates from Mooley et al. (2013) , we retain only HD 31305 as a probable Taurus member (see 5.4) and reject the other four stars (HD 28929, τ Tau, 72 Tau, and HD 26212) . Below, we describe our reasoning for excluding the latter stars from our membership sample.
Conclusions : HD 28929 is a B8V star (Kenyon et al. 1994 ) with a common proper motion companion, HD 28929B (2MASS J04343987+2857347, UCAC4 595-013167, TYC 1841-1391-1). The BVJHK s photometry for HD 28929B is similar to that of a F6V star (Ofek 2008) , and so if it has the same reddening as HD 28929A (E(B-V) 0.05), then it is likely a mid-F star. If the revised Hipparcos parallax for HD 28929A ( = 6.72 ± 0.34 mas) is adopted, then the companion has M V 4.2, which is more consistent with a main sequence star rather than a pre-main sequence star. Zorec & Royer (2012) estimates an isochronal age of HD 28929 of ∼600 Myr. Using the estimated H-R diagram position for HD 28929 A of T ef f = 12850 K and log(L/L ) 2.30, we estimate an age of ∼120 Myr using the Bertelli et al. (2009) evolutionary tracks. Although the tangential motion of HD 28929 differs from that of the nearest Taurus subgroups at a level of only ∼2-3 km/s, and it appears to be co-distant with Taurus, the HD 28929 system is isolated and has no other known Taurus members within a degree. Based on all of these considerations, and the lack of indicators of extreme youth for either component, we conclude that the HD 28929 system is likely to be a ∼120 Myr-old interloper.
Conclusions : τ Tau (HD 29763) is a B3V triple system (Lesh 1968) . We disagree with the assessment of Mooley et al. (2013) that the star appears to be a kinematic match to the Tau V subgroup, which is in its vicinity. Combining the systemic velocity from Petrie & Ebbighausen (1961) (12.3 km/s) with the revised Hipparcos astrometry, we estimate its space motion to be (U, V, W) -10, -8, -12 km/s, which is not near that of the Taurus subgroups (see Table 8 of Luhman et al. 2009 ). The star's tangential motion differs by ∼4-6 km/s from the mean motion of the nearest Tau subgroups (IV and V), with its proper motion in right ascension going the wrong direction, and the systemic radial velocity differs by ∼4 km/s from that of Taurus. τ Tau A is not near the ZAMS (log(T ef f ) 4.20 ± 0.01 dex, log(L/L bol ) 2.9 ± 0.1 dex); indeed, we estimate an isochronal age of ∼60 Myr using the Bertelli et al. (2009) tracks. τ Tau B is a A1V star (Tolbert 1964) , for which we estimate M V 1.6. As a ∼2.1 M main sequence star, it would take a star of τ Tau B's mass ∼8 Myr to contract to reach the main sequence, which sets a strong lower limit on the age of the τ Tau system, and rules out the possibility that τ Tau A might be in the pre-main sequence phase. We conclude that the τ Tau system is a ∼60 Myr-old interloper.
Conclusions : 72 Tau (HD 28149) is a B7V star (Lesh 1968 ) near the Tau V subgroup. Its tangential motion differs by ∼6 km/s from that of the Tau V group (Luhman et al. 2009 ). The radial velocity for 72 Tau (7.3 ± 2.6 km/s) is significantly different from that of Tau V (15.7 km/s). Its space velocity (U, V, W = -5.9, -5.3, -7.4 km/s; Anderson & Francis 2012) differs from that of the Tau V subgroup by 11 km/s (Luhman et al. 2009 ). Zorec & Royer (2012) estimates an isochronal age of 179 ± 87 Myr, although our estimated H-R diagram position for 72 Tau (log(T ef f ) 4.16, log(L/L ) 2.32) places the star closer to the ZAMS (<15 Myr?). Despite its youth, we find that 72 Tau is a poor kinematic match for Taurus membership, and consider it an interloper. HD 26212 is a A5V star (Grenier et al. 1999; Mooley et al. 2013) at a distance of 100 ± 7 pc (van Leeuwen 2007), placing it >5σ closer than the mean distance to Taurus (140 pc). HD 26212 is not in the vicinity of any of the Taurus subgroups (>2 deg from Tau VIII), and its velocity (U, V, W = -18, -5, -12 km/s; Anderson & Francis 2012) is not a good match to the Taurus subgroups (Luhman et al. 2009 ). We estimate that the star is lightly reddened (E(B-V) = 0.04 ± 0.02) with an H-R diagram position of log(T ef f ) = 3.907 ± 0.007 dex, log(L/L ) = 1.01 ± 0.07 dex. Comparing this position to pre-main-sequence isochrones and members of Upper Sco in Figure 14 of Pecaut et al. (2012) , it appears that HD 26212 is a ZAMS star and is below the Upper Sco (∼11 Myr) A-star sequence. Hence, it is almost certainly >10 Myr, and not a pre-main-sequence star. Given the discordance of its position, distance, velocity, and age compared to Taurus, and lack of secondary youth indicators (e.g., IR excess), we consider HD 26212 an interloper. Note. -Entries of "· · ·" and "out" indicate measurements that are absent because of non-detection and a position outside the field of view of the camera, respectively. Note. - Table 5 is available in a machine-readable format, a portion is shown here for guidance regarding its form and content.
Conclusions : a Coordinate-based identifications from the WISE All-Sky Source Catalog.
Conclusions : b Coordinate-based identifications from the 2MASS Point Source Catalog when available.
Conclusions : c Probable non-members based on: false = spurious WISE detection in all bands or bands that seem to show excesses; mismatch = different sources dominate in W 1/W 2 and W 3/W 4; galaxy = resolved galaxy; extended = extended in 2MASS, DSS or SDSS images, indicating that it may be a galaxy; blend = unreliable photometry because of blending with other sources.
Conclusions : d Probably non-members based on the listed spectroscopic classification Table 6 . IR Excess Sources Observed with Spectroscopy Table 7 . IR Excess Sources with Undetermined Membership in Taurus µm from Spitzer are shown when available. Otherwise, measurements at similar wavelengths from WISE are used (W 2 and W 4). We have indicated known protostars (class I and 0, triangles), candidate transitional disks (crosses), candidate evolved disks (stars), and candidate debris disks or evolved transitional disks (circles). The reddest protostars are beyond the limits of these diagrams. In the middle and lower diagrams, we have marked the lower boundaries that we have adopted for full disks (dotted lines). . We have defined regions that separately encompass most of the members with and without excesses from disks (solid lines). Middle: Among WISE sources that are not known Taurus members (points), we have identified those that have colors indicative of disks based on the boundaries defined on the left (red points). Right: Some of the IR excess sources from the middle diagrams are probable non-members (Table 5) .
Conclusions : We show the positions of the remaining candidates that have been confirmed as members through spectroscopy (Table 6 , points) and that have undetermined membership status (Table 7 , circles). Most of the latter are probably galaxies based on their faint magnitudes and very red colors. The spectra have been corrected for extinction, which is quantified in parentheses by the magnitude difference of the reddening between 0.6 and 0.9 µm (E(0.6-0.9)). The spectra have a resolution of 7Å and are normalized at 7500Å. Fig. 7 .-Near-IR spectra of new members of Taurus. The spectra with measured spectral types have been corrected for extinction. These data have a resolution of R = 100 and are normalized at 1.68 µm.

118866026
ABSTRACT : Abstract: The semi-constrained NMSSM (scNMSSM) extends the MSSM by a singlet field, and requires unification of the soft SUSY breaking terms in the squark and slepton sectors, while it allows that in the Higgs sector to be different. We try to interpret the muon g-2 in the scNMSSM, under the constraints of 125 GeV Higgs data, B physics, searches for low and high mass resonances, searches for SUSY particles at the LHC, dark matter relic density by WMAP/Planck, and direct searches for dark matter by LUX, XENON1T, and PandaX-II. We find that under the above constraints, the scNMSSM can still (i) satisfy muon g-2 at 1σ level, with a light muon sneutrino and light chargino; (ii) predict a highly-singlet-dominated 95 GeV Higgs, with a diphoton rate as hinted at by CMS data, because of a light higgsino-like chargino and moderate λ; (iii) get low fine tuning from the GUT scale with small µ eff , M0, M 1/2 , andA0, with a lighter stop mass which can be as low as about 500 GeV, which can be further checked in future studies with search results from the 13 TeV LHC; (iv) have the lightest neutralino be singlino-dominated or higgsino-dominated, while the bino and wino are heavier because of high gluino bounds at the LHC and universal gaugino conditions at the GUT scale; (v) satisfy all the above constraints, although it is not easy for the lightest neutralino, as the only dark matter candidate, to get enough relic density. Several ways to increase relic density are discussed.
Introduction : In July 2012, the Higgs boson was discovered at the LHC [1, 2] , and searching for physics beyond the Standard Model (SM) has now become the main objective in high energy physics. Supersymmetry (SUSY) is one of the most popular theories for new physics. As the simplest SUSY model, the minimal supergravity model (mSUGRA) has attracted a lot of attention from both theorists and experimentalists. However, it cannot predict a 125 GeV SM-like Higgs when considering all the constraints, including muon g-2 at 2σ level, and dark matter [3, 4] . When we give up uniform parameters at the grand unification (GUT) scale, the MSSM can satisfy all the constraints well, but there is a problem with fine-tuning [4] .
Introduction : After the Higgs boson was discovered, it became necessary to ask whether there is a second Higgs-like particle. Searches at LEP, the Tevatron, and the LHC have excluded a lighter SM-like Higgs, while a lighter second Higgs with rates lower than the SM-like one could still be possible. Recently, the CMS collaboration presented their searches for low-mass new resonances decaying to two photons. For both the 8 TeV and 13 TeV dataset, a small excess around 95 GeV was hinted at, with approximately 2.8σ local (1.3σ global) significance for a hypothetical mass of 95.3 GeV in combined analysis [5] . This result has been interpreted or discussed in several papers [6] . The MSSM cannot predict such a lighter second Higgs together with a 125 GeV SM-like Higgs under other constraints like the muon g-2 and dark matter [4] .
Introduction : The next-to-minimal supersymmetric Standard Model (NMSSM) has more freedom to predict a SMlike 125 GeV Higgs, under all the constraints and with low fine-tuning [4] . At the same time, it can also predict a lighter second Higgs with rates lower than the SM-like one [7, 8] . Since simple models are usually more favoured, the fully constrained NMSSM (cNMSSM) [9] [10] [11] and the semi-constrained NMSSM (scNMSSM) are also being studied [12, 13] . For the full cNMSSM, with all soft SUSY breaking terms unified at the GUT scale, including M Hu = M H d = M S = M 0 , there should be only four continuous parameters, the same as mSUGRA. While in many studies of the cNMSSM [9] [10] [11] , there is an additional parameter λ, for a singlet scalar M S does not in fact need to be unified. Such an issue was also pointed out in Ref. [14] . In the 5-parameter and 4-parameter cNMSSM, the SM-like Higgs cannot get to 125 GeV under all the constraints including muon g-2 [9, 11] . The scNMSSM is also called the non-universal Higgs mass (NUHM) version of the NMSSM, for it allows the soft SUSY breaking terms in the Higgs sector to be different. In Ref. [9] , the parameter λ is always less than 0.1, so the results in Higgs sector may not be much different from the NUHM version of MSSM, e.g., the 125 GeV SM-like Higgs is always the lightest Higgs. In Refs. [12, 13] , the muon g-2 constraint is set aside. In this paper, we consider all the constraints, including muon g-2, and also require a lighter Higgs with rates constrained by LEP, Tevatron, and LHC searches. For the dark matter relic density, we only apply the upper bound [8] , considering that there may be other sources of dark matter [15] . We focus on the muon g-2, its relation to model parameters, SUSY particle masses, and other constraints like the dark matter relic density. This paper is organized as follows. First, we briefly introduce the NMSSM and scNMSSM in Section 2. In Section 3, we discuss the constraints on the model, present our numerical results and have some discussion. Finally, we draw our conclusions in Section 4.
Conclusions : In this work, we have checked the status of the scN-MSSM under current constraints, such as 125 GeV Higgs data, searches for low and high mass resonances, searches for SUSY particles at the LHC, B physics, muon g-2, dark matter relic density by WMAP/Planck, and direct searches for dark matter by LUX 2017, PandaX-II 2017, and XENON1T 2018. First, we scanned the parameter space of the scNMSSM in 9 dimensions with the MCMC method. For each valid sample, we calculated its various physical quantities and required them to satisfy corresponding constraints. For the surviving samples, we analyzed fine tuning from the GUT scale, SUSY particle masses, the light scalar and its diphoton signal, dark matter relic density and direct detection, muon g-2, and their favoured parameter space. Finally, we come to the following conclusions regarding the scNMSSM: 1) For low fine tuning samples, small µ eff , M 0 , M 1/2 , A 0 , are more favored, and the lighter stop mass can be as low as about 500 GeV, which can be further checked in future works with search results at the 13 TeV LHC.
Conclusions : 2) For light higgsino-like charginos and moderate λ, the highly-singlet-dominated light scalar can have a considerable diphoton rate, satisfying the latest results of the search for low-mass resonances by CMS.
Conclusions : 3) For high gluino bounds at the LHC and the condition of universal gauginos at the GUT scale, the lightest neutralino can only be singlino-dominated or higgsino- 4) For light muon sneutrino and light higgsino-like charginos, we can get large muon g-2, while the contribution of neutralinos cannot be large because bino-like and wino-like neutralinos are heavy.
Conclusions : 5) The model can satisfy all the above constraints, although it is not easy for the lightest neutralino, as the only dark matter candidate, to get enough relic density.
Conclusions : Considering the disadvantage of the scNMSSM, one can try three main kinds of ways to raise the relic density: 1) Considering other source of relic density, e.g., the effects of modifications of the expansion rate and of the entropy content in the early universe.
Conclusions : 2) Changing the LSP to another sparticle, such as bino-like neutralinos in the non-universal gaugino cases, or sneutrinos in the right-handed neutrinos extended case.
Conclusions : 3) Reducing λ and κ in the h/Z funnel scenario, although this way may lose a light Higgs.

118869358
ABSTRACT : We investigate the approximate quantum state sharing protocol based on random unitary channels, which is secure against any exterior or interior attackers in principle. Although the protocol leaks small information for a security parameter ε, the scheme still preserves its information-theoretic secrecy, and reduces some pre-shared classical secret keys for a private quantum channel between a sender and two receivers. The approximate private quantum channels constructed via random unitary channels play a crucial role in the proposed quantum state sharing protocol.
I. INTRODUCTION : Quantum physics allows us a perfect randomness, so most of all quantum information-theoretic primitives try to offer an unconditional security under the randomness. For examples, quantum key distribution protocols such as BB84 [1] and B92 [2] highly depend on a random measurements for given classified non-orthogonal quantum states.
I. INTRODUCTION : Instead of the random measurement on non-orthogonal states, we can consider a direct randomization of quantum states through a quantum channel. This randomizing procedures are efficiently accomplished via the private quantum channels (PQC) or quantum one-time pads [3] . In the paper we are interest to some schemes for approximate encryptions (no perfect) and we make an attempt to reducing some classical communication resources. We would like to call the randomizing procedures or maps as random unitary channels (RUC) in terms of quantum channels. There are several methods for the approximate randomizing quantum states, for examples, [4, 5, 8] : We here adapt the procedure of Hayden et al. [4] .
I. INTRODUCTION : Many applications of RUC in quantum protocols (See e.g., [4, 6, 7] .) are started from the approximate version of PQC. Here we will propose new approximate quantum state sharing (AQSS) scheme, which uses two approximate PQCs (APQC) and reduces the classical pre-shared secrets about one-half as compared with a perfect protocol. Actually our protocol could be including the (well-known) quantum secret sharing protocols [9, 10] , because a quantum state itself is able to operate special quantum tasks, though those are impossible in the classical power. Imagine that if there is a quantum computer only activated under a bipartite quantum state (or quantum key), then our AQSS protocol will give a efficient and secure solution for the quantum key. These approximate quantum state sharing protocols may offer us more opportunities as compared with the quantum secret sharing.
I. INTRODUCTION : Let's take account of the pre-shared secrets for the approximate quantum state sharing protocols under RUC-based PQC roughly. Assume that a sender Charlie prepares a quantum state ϕ AB (two-qudit) and transmits the state through two independent RUCs, then two distant agents Alice and Bob will receive some output state of including high entropy. For the state ϕ AB the perfect randomization protocol will require exactly the amount of 4 log d-unitary matrices (Pauli matrices). On the other hand, the construction of Hayden et al. [4] for our AQSS scheme implies that only 2 log d + o(log d)-unitaries sufficient. In other words, the perfect quantum state sharing protocol needs to 2l bits of pre-shared secret information, while the AQSS protocol demands about l bits of information. Note that the works in [5, 8] will give a similar result for l bits bound.
I. INTRODUCTION : We will prove the information-theoretic security of the AQSS scheme in two kinds of eavesdropping: an interior and exterior attackers. The proof of having higher entropy condition for the exterior attacks is not easy fact, so we split the input state ϕ AB to separable and entangled cases. As a result, the von Neumann entropy in both cases can be chosen sufficiently larger, and a leakage information will be arbitrarily small. Finally the authors show that our bipartite AQSS scheme naturally can be generalized to an one-sender and multiparty-receivers schemes.
I. INTRODUCTION : In section II we introduce the definition of random unitary channels, and briefly mention about special property known as the destruction of quantum states on a product random unitary channel. We present our AQSS protocol based on two approximate PQCs in section III, and investigate the security of AQSS of considering two attacks: an exterior and interior strategies. we finally conclude our results in section IV. 
IV. CONCLUSIONS : We studied that the approximate quantum state sharing schemes are efficient from the classical information cost of view and those are robust to the two kinds of attacks. The proposed AQSS protocol basically depends on an approximate private quantum channel, which is constructed via two independent random unitary channels. Although the protocol leaks small information corresponding to the security parameter ε, the scheme preserves its informationtheoretic security, and so the AQSS and MAQSS schemes can be interpreted as some high-efficiency state sharing protocols for any bipartite and multipartite quantum states.

84882469
ABSTRACT : In this research, three different hormones and five different hormone dosages were applied on cuttings were taken from Anatolian sage plants (Salvia fruticosa Mill.) before flowering period. NAA, IBA (0, 60, 120, 180, 240 ppm) and IAA hormones (0, 100, 200, 300, 400 ppm) were prepared by dissolving in distilled water. Stem cuttings were kept in hormone solution for 24 hours and they were planted in perlit medium under greenhouse conditions. After a month, the number of rooted stem cutting, the number of root per stem cuttings, root length and root weight were determined on stem cuttings. Rooting was observed in all of the cuttings for both samples to which hormone was applied and to which hormone was not applied. According to the result of the variance analysis, the effects of the hormones and hormone doses on the examined characters were found significant as statistically. According to the results obtained, IAA application increased root number considerably. While high hormone dose applications caused the notable increase in root weight and root number in all of three hormones, low hormone applications did not affect root length.
Introduction : Anatolia is the main centre of Salvia species in Asia, and its 89 species, half of which are endemic, were determined in Anatolia's natural vegetation. Salvia fruticosa Mill., being one of the commercial species with its essential oil that is over %1, spreads in Mediterranean mainly in Turkey and Greece [1] - [4] .
Introduction : In Turkey, the leaves of Anatolian sage (Salvia fruticosa Mill.) are used instead of medical sage (Salvia officinals L.). Its leaves resemble medical sage's leaves in terms of chemical structure and treatment effects. Its essential oil has gastric, gastral, diuretic and saccharic effects. Externally, it can be used as wound healer and antiseptic [5] and [6] . In addition to these features, it is informed that it is among the plants of sage having the best antioxidant activity [7] . Along with internal consumption, sage (Salvia fruticosa Mill.) is gathered from natural vegetation from the west of Turkey and is exported being dry [8] . Related to the increasing export, efforts for the plant's production have increased in recent years.
Introduction : Sage production can be made by the separation of secondary stems growing from seed, cutting and roots. Being an effective method in producing many plants, vegetative production with cuttings is seen as an attractive method for plant production industry [9] . Cuttings to which hormone is applied generally root much more rapidly than those to which hormones is not applied and construct a stronger root system, thus moulding in the lower parts of cuttings diminishes with accelerating of rooting. In their study they conducted with different species of Lamiaceae family, [10] detected the highest root height and root number values in Salvia fruticosa species in 3000 ppm IBA application for a period of 5 minutes.
Introduction : In this study, it was aimed at determining the effects of different hormones and hormone doses in S. fruticosa cuttings upon the growth of seedling root.

84883393
Introduction : The results of an investigation of the stratigraphy and diatom sequence in the Windermere deposits have already been published (Pennington 1943) . The present paper deals with the results of pollen analysis of selected profiles on a transect across the lake, and with the macroscopic plant remains found in the deposits, especially those of late-glacial age. Evidence obtained from these latter investigations has made it possible to date the lake deposits with more accuracy than formerly, and by the light it throws on the late-glacial and post-glacial vegetational sequence in the Windermere basin has made it possible to fit this region into the general picture of the Quaternary history of the British Isles. Further examination of the late-glacial plant-bearing layer described in the previous paper has confirmed the view that it represents a late-glacial climatic amelioration followed by a colder period and then by the final retreat of the ice. A similar layer has been recognized right across Ireland, and its discovery in England was expected. It seems possible that it may be correlated with a similar succession in Denmark and other parts of north-west Europe, and afford a means whereby the British succession could be linked with the Continental succession and De Geer's geochronology; the evidence in favour of such a correlation is put forward in the course of this paper.

84887049
ABSTRACT : Th is paper provides an overview of the life and scientifi c work of Dr. Christo Deltshev, the doyen of Bulgarian araneology. It also analyses his more important research contributions and provides a list of his scientifi c publications and the species he described.
ABSTRACT : Biography, Christo Deltshev, spiders, bibliography, Bulgaria It is both diffi cult and easy to write about the doyen of Bulgarian araneology Christo Deltshev. It is diffi cult because his work, although in one area -the study of spiders, is rather varied. And yet it is easy because we, the authors of this biography, know our colleague and friend Christo Deltshev and his development as a zoologist very well. We have witnessed how before our very eyes the enthusiastic lover of spiders, caves and mountains gradually turned into an erudite araneologist enjoying the respect of his colleagues from many countries. Th e seventieth anniversary of Christo Deltshev is reason enough to look back on his achievements in science and life and outline his

123621513
ABSTRACT : Nor mally, cyl in der pres sure was used as a cri te rion of com bus tion oc cur rence, while in some con di tions, it may be un re li able when iden ti fy ing lean mix ture combus tion. This is par tic u larly im
ABSTRACT : Nowdays, de mands for emis sion re duc tion of the ve hi cle be came very ur gent which lead to stricter emis sion reg u la tion. The cold start emis sion at -7 °C en vi ron men tal tem per a ture was re quired in the Eu rope and the USA emis sion reg u la tion [1] . The re search shows that 50%~80% HC and CO emis sions was pro duced dur ing the cold start [2, 3] . Since the 1990s, many re searches about HC emis sions dur ing the cold start were car ried out of China. Re cently, the sim i lar re search was car ried on in China. Pro fes sor Huang [4, 5] etc. stud ied the HC emissions and their in flu enc ing fac tors dur ing cold start. Yong [3, 6] etc., com bined with study of the light off char ac ter is tic of the three-way-cat a lytic con verter, stud ied the emis sions and af fect ing fac tors dur ing cold start and warm-up.
ABSTRACT : Those stud ies show that the ex cess air co ef fi cient was the key pa ram e ter for the first fir ing cy cle (FFC) dur ing cold start. The mix ture con cen tra tion be ing too rich or too lean will increase the HC emis sions of the FFC dur ing cold start sharply. There fore, the main ob ject was to op ti mize the ex cess air co ef fi cient in the FFC dur ing cold start. HC emis sions were one of the judg ments for the op ti mum mix ture con cen tra tion. While, study of HC emis sions were not 

